[
    {
        "id": 1,
        "title": "Sector Rotation by Factor Model and Fundamental Analysis",
        "corpus": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on. Authors: achieve the best HTML results from your LaTeX submissions by selecting from this list of supported packages. This study presents an analytical approach to sector rotation, leveraging both factor models and fundamental metrics. We initiate with a systematic classification of sectors, followed by an empirical investigation into their returns. Through factor analysis, the paper underscores the significance of momentum and short-term reversion in dictating sectoral shifts. A subsequent in-depth fundamental analysis evaluates metrics such as PE, PB, EV-to-EBITDA, Dividend Yield, among others. Our primary contribution lies in developing a predictive framework based on these fundamental indicators. The constructed models, post rigorous training, exhibit noteworthy predictive capabilities. The findings furnish a nuanced understanding of sector rotation strategies, with implications for asset management and portfolio construction in the financial domain. Keywords: US Industrial Sectors, Factor Analysis, Fundamental Analysis, Trading Strategy. Sector is composed by a basket of stocks that representing companies in certain business class, which has unique features according to the business. Under certain conditions, such as economic cycles, sectors may behave accordingly due to the different characteristics of businesses. In this report, we are exploring how to capture returns by finding the hidden features behind different sectors and determining the leading sectors in some particular market conditions or social environments. Generally, this report covers a brief exploration of market and fundamental factors, explaining the meaning of each factors and how they are related to some sectors.Then we applied a neural network model to do a classification and prediction using the fundamental factors as inputs. At the end of the report, we also covers how sectors behaved under global events. There are many different ways to divide sectors. For the purpose of common acceptance and convenience for future data acquirement, we used the MSCI Global Industry Classification Standard, which includes 11 level one sectors, 24 level two industry groups, 69 level three industries, and 158 sub-industries. We use the 11 level one sectors as our main target. They are Energy, Materials, Industrials, Consumer Discretionary, Consumer Staples, Health Care, Financials, Information Technology, Communication Services, Utilities, and Real Estate. In order to track the performance of each sector, we use the S&\\&&P500 GICS Indices which are constructed exactly as the MSCI classification.Before working on any strategies further, we need to determine if there are actually possible profits. In our case, we need to check how big the differences between sectors\u2019 returns are. For each observation time period, Define: Return Difference =(\u2211top 3subscripttop 3\\sum_{\\text{top 3}}\u2211 start_POSTSUBSCRIPT top 3 end_POSTSUBSCRIPT Sector Return - \u2211bottom 3subscriptbottom 3\\sum_{\\text{bottom 3}}\u2211 start_POSTSUBSCRIPT bottom 3 end_POSTSUBSCRIPT Sector Return )/3 Based on a monthly frequency, we calculate the return difference and get the following plot. Also, by calculation, the quarterly return difference has a mean of 0.1306, median of 0.1185, standard deviation of 0.0523. We can reach to a result that half of the quarterly return difference is more than 11.85%percent\\%%. It is easy to see that there does exist potential investment opportunity by capturing the return differences between sectors. Momentum premium was first recognized by UCLA scholars Narasimhan Jegadeesh and Sheridan Titman in 1993. The momentum premium is established on the observation that assets that have performed well in the past have the trend to persist good performance in the future. Though the momentum effect is considered to be a market anomaly, it has been recognized widely among many asset classes. We will explore the momentum effect based on the sector indices introduced above. First of all, we need to construct the momentum factor. Typically, the momentum factor is constructed by the past 6 or 12 months cumulative return and excludes the most recent month\u2019s return, considering that there are also short-term reversion effects based on the mean-reversion effects. However, without a clear idea of how the sector indices carry the momentum effect, we need to explore through time intervals to find the best possible momentum factor. Then we constructed 12 different momentum factors using the past 1 to 12 month\u2019s return and excluding the most recent 0.1 portion trading days of each time period to avoid short-term reversion. For each of the factors with the period of n months where Rdsubscript\ud835\udc45\ud835\udc51R_{d}italic_R start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT is the daily return. For the 12 factors we got, we normalized them cross sections. Then we rank the factor exposures for each sector and take long positions of sectors with the highest two factor exposures, take short position of sectors with the lowest two factor exposures. Then we trade our portfolio under a monthly frequency. Here are the results from 2002 to 2022 February: Since there are several market crashes where the momentum factor led to negative returns, we also take a look at the most recent five years from 2017 to 2022 February: From this table, we can tell that by using the MOM__\\__7M factor, we can reach a maximum annual return rate of 21.19%percent\\%% and a maximum Sharpe ratio of 0.62. It is also interesting that we find the momentum factor with a short time period, for example, MOM__\\__1M and MOM__\\__2M, have a very small even negative return rate. However, it exactly conforms to the short term reversion effect that the typical momentum factor would exclude. Short term reversion factor follows the simple principle that asset\u2019s price will have the trend to stay on an average level. Since we can see from the previous results of the momentum factor that there does exist short term reversion effect, we can try different reversion factors and find out what would be the best short term reversion observation period. Similarly, we can define several reversion factors with different time periods. And we take the negative number of the past n days cumulative return as the factor exposures. For the purpose of exploring the optimal time period, we take 5-day time interval and create 12 reversion factors from 5 trade days to 55 trade days. By using the same method, we compute the rank of each sector\u2019s factor exposure, and long the top two sectors, short the last two sectors on a monthly observation frequency. Between 2002 and 2022 February, the results are: From this table, we can tell that for the time between 2002 to recent time, the short term reversion effect is optimal for taking the past 30 days cumulative return. It has an optimal annual return rate of 8.77%percent\\%% on average and leads to a sharp ratio of 0.8735. Fundamental Analysis are always a good aspect to look at for investing. We collected quarterly data for all 11 indices from Bloomberg, including their P/E ratio, EV/EBIT, Profit Margin, etc. Our fundamental analysis would start from discovering features for each of the fundamental ratio,then we are trying to predict the sector performance by constructing using some of the features we found. The P/E is one of the most widely used tools to determine a stock\u2019s relative valuation. The purpose of analyzing the ratio is to show whether certain sector is worth to be invested because P/E ratio can reflect the investment risk in this sector. The figure below shows distribution of P/E ratios in different sectors. By comparing cross-sectional data, it is obvious that P/E of Real Estate Sector and Consumer Discretionary Sector are higher than others. The reason is that earning growth in the future is expected to grow fast in the two sectors or these sectors have some special advantages that guarantee long-term profitability with low risk. On the other hand, Financials Sector\u2019s ratio is relatively low compared with other sectors, which may result from its high volatility so investors are reluctant to pay for it. We also notice that the ratio in Energy Sector surged in 2015, which is related to some changes in the sector. The end of the oil age and emergence of alternative energy have reduced the earnings of the original sector. As a result, its relative price becomes higher than before. The change in EV/EBIT and EV/EBITDA is also due to this reason. The P/B ratio provides a valuable reality check for investors who are seeking growth at a reasonable price. For those sectors with more assets, their book value and market value are close, so P/B ratio is more useful when we analyze Real Estate sector and Financials sector. The figure below shows distribution of P/B ratio in different sectors. As the picture shows, Consumer Discretionary sector and IT sector have higher P/B ratio while Financials sector and Energy sector have relatively low ratios. What\u2019s more, Real Estate sector with high P/E ratio has relatively lower P/B ratio. EV/Sales can help investors better understand cost relative to unit sales and whether the company is overvalued or undervalued. If EV/Sales is relatively high, the company or sector is less attractive to investors. The figure below shows distribution of EV/Sales in different sectors. The result shows that Real Estate sector\u2019s ratio is higher than others\u2019, which means that index in this sector is overvalued. On the other hand, ratio in Energy sector is low, which can attract more investors. EV/EBIT and EV/EBITDA are independent of the capital structure of the company, whereas multiples like P/E ratio are impacted by financing decisions. Because of this reason, the two are the most commonly relied-upon multiples in relative valuation. However, one obvious distinction is that EV/EBIT considers depreciation and amortization. In some capital-intensive industries which have significant differences in D&\\&&A, EV/EBIT may make it a more accurate measure of value. But in our analysis, there is no such difference in the comparison of these two ratios under different sectors. The results can show that Real Estate sector has a higher ratio. The three ratio EV/Sales, EV/EBIT and EV/EBITDA can give a consensus conclusion that Real Estate sector is overvalued in the market. Dividend Yield is used to measure the amount of cash flow investors are getting back for each dollar. It is essentially the return on investment for a stock without any capital gains. The figure below shows distribution of Dividend yield in different sectors. The ratio in Communication Services sector is higher before 2018 while Energy sector\u2019s ratio is higher after that time. This is because communication services sector took place a reorganization of S&\\&&P500 index in 2018. It now includes at least eighteen companies from IT and Consumer Discretionary sectors. Due to this reshuffling, dividend yield of this sector is impacted. Gross margin equals net sales less the cost of goods sold (COGS). Net sales are equivalent to the total revenue from sales, and COGS is the direct cost associated with producing goods. By calculating gross margin, we could measure one company\u2019s retain revenue after subtracting the production cost. The higher the gross margin, the more capital a company retains, which it can then use to pay other costs or satisfy debt obligations. Generally, companies with good gross margins would have a relatively sustainable competitive advantage. By analyzing gross margin data across sectors, we may observe some sectors that have more stable development in the long run. For our 11 sectors\u2019 gross margin data, the line chart above shows significant differences between the sectors. Overall, each industry index is relatively flat on its own, and have gaps between each others. Utilities, Communication Services and Information Technology(IT) have been among the top spears for last 10 years, occupying the first, second and third positions respectively, all above 40%percent\\%%. On the contrary, the energy sector has been an under-performer for the past decade, ranking at the bottom, with gross margins consistently below 20%percent\\%%. Gross margins in the rest industries are concentrated in the 25%percent\\%%-35%percent\\%% range and have not fluctuate much. At the same time, by observing the comparison of fluctuations between industries, it is not difficult to see that the gross margin fluctuations of the energy industry and the utilities industry maybe relatively high in the past decade, and their peaks correspond to each other. During 2016, the utilities industry grew significantly, while energy declined comparatively. The trend was even more pronounced in 2020, with utilities reaching its highest level and the energy industry fell to the bottom. Generally, the gross margin feature maybe a significant indicator for Utilities, Communication Services and IT sectors. And our conjecture about the correlation between utilities and energy sectors will need further observation and verification. Operating margin equals operating income divided by revenue, it is a profitability ratio measuring revenue after covering operating and non-operating expenses of a business. And profit margin measures the profit ratio after paying for variable costs of production. It is calculated by the formula: Both operating margin and profit margin are used to gauge the degree of the company\u2019s activity makes money. Higher ratios are generally better, illustrating the company is efficient in its operations and is good at turning sales into profits. In our analysis, there is not a very big difference in the comparison of these two ratios under different sectors, which is determined by their definition. For these two ratios, Real Estate sector, IT sector and Financial sector have the top three high ratios.And Energy sector has the relatively lowest ratio. Also, both operating margin and profit margin for almost all sectors have similar trends in the last decade curves. This is attributed to the definition difference between the two features, and that\u2019s why the operating margin was slightly higher than the profit margin. Another thing that is worth to mentioning is that for Energy Sector, not just operating margin and profit margin, but also the gross margin, it always has the relatively lowest ratios and similar curve fluctuation, with sharp declines in 2016 and 2020. The two time nodes may consistent with some big revolution in the energy industry, which we will analyze later. Return on equity (ROE) and return on assets (ROA) are two of the most important measures for evaluating how effectively a company\u2019s management team is doing its job of managing the capital entrusted to it. ROE equals to generally net income divided by equity, while Return on Assets (ROA) is net income divided by average assets. So the primary differentiator between ROE and ROA is financial leverage or debt. ROE measures profitability and ROA is an efficiency measure of how well a company is using its assets. Investors may prefer to observe ROE, since equity represents the owner\u2019s interest in the business. Compared to other sources of fund, equity capital tends to be the most expensive source of funding and carries the largest risk premium of all financing options. Therefore, in our analysis, ROE may be a better feature that it could reflect the trend of market investment. As shown in the picture, IT sector has the highest ROA, the Consumer Staples sector and Consumer Discretionary sector also have a relatively higher ratio. In contrast, Financial sector has a lower ROA. The past ten years, or even twenty years, has been an era of rapid development of information technology. And compared with traditional industry and commerce, information technology is more flexible in the time and form of investment assets, that\u2019s the reason why IT will have the highest ratio. Also for the the Consumer Staples sector and Consumer Discretionary sector,they are all industries with fast innovation and short production cycle. Generally, these three will have constantly higher ratio for the long run. Therefore, for these three industries, if the ROA indicator fluctuates significantly, it may have an impact on the investment trend. For ROE ratio, similarly, IT, Consumer Staples stay high, and Consumer Discretionary sectors is also at a slightly higher level, except that the IT sector lost its prominence in ROA ratio. By comparing cross-sectional data, the Consumer Discretionary Sector and Industrials Sector have similar patterns in the last decade for both ROA and ROE ratios. They both have a low peak in 2020. It is conceivable that this is affected by the general environment of the epidemic. And as we mentioned before, the ROE and ROA curves of the energy sector still have a similar pattern, falling sharply in 2016 and 2020. In 2016, it was affected by changes in energy policy since 2015, reducing oil production while encouraging the development of clean and new energy. For 2020, we attribute this decline to the outbreak of the COVID-19 pandemic. Having these fundamental data, next step is to find out what quantitative relationships they have to futures sector returns. For fundamental factors, they are usually exposed in the company report with annual, semi-annual, or quarterly frequency. Our fundamental factors for each sector are reported quarterly, leading to a problem that the sample size for each individual sector is very small. To have a better performance of the prediction model, we need to combine all the sectors together and make a uniformed and comparable large sample. We neutralized each factor cross-sectional for the factor to have a mean of 0 and standard deviation of 1. If Xi,tsubscript\ud835\udc4b\ud835\udc56\ud835\udc61X_{i,t}italic_X start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT denotes one specific factor exposure for i\ud835\udc56iitalic_i-th sector at time t\ud835\udc61titalic_t, in this case would be at t\ud835\udc61titalic_t-th quarter, then for each individual t\ud835\udc61titalic_t we have the neutralized exposure to be: Then we used the next quarter\u2019s cross-sectional normalized return as the corresponding return. First, we want to have a general view of the relations. The scatter plots between neutralized factors and future returns are as following: From the scatter plots, the relations between all factors and their future returns cannot be well interpreted by simple linear models. However, it is very common in the financial field that the sample will have a very low signal-noise ratio. As we observed before, the relations between each factor and its future return cannot be interpreted very well by linear models. Also, we have no idea what model would exactly best fit the data. Therefore, converting prediction of future returns to a classification problem and fitting the training sample with a neural network model which has comparably good performance with non-linear relations would be a great start point. [height=10] \\inputlayer[count=10, bias=false, title=Input layer, text=x\\hiddenlayersubscript\ud835\udc65\\hiddenlayerx_{\\hiddenlayer}italic_x start_POSTSUBSCRIPT end_POSTSUBSCRIPT[count=5, bias=false, title=Hidden layer 1, text=h\\linklayers(1)subscriptsuperscript\u210e1\\linklayersh^{(1)}_{\\linklayers}italic_h start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT end_POSTSUBSCRIPT\\hiddenlayer[count=5, bias=false, title=Hidden layer 2, text=h\\linklayers(2)subscriptsuperscript\u210e2\\linklayersh^{(2)}_{\\linklayers}italic_h start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT end_POSTSUBSCRIPT\\outputlayer[count=2, title=Output layer, text=y^\\linklayerssubscript^\ud835\udc66\\linklayers\\hat{y}_{\\linklayers}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT end_POSTSUBSCRIPT Neural network takes a vector as the input, and goes to each of the neuron in the first hidden layer and gains new activation vectors which act as the input for next hidden layer. After the last hidden layer, neural network model would pass out the probability for each of the prediction class and we choose the one with the highest probability as the prediction. This process is called front propagation. After comparing the prediction to the actual results, we adjust the weights of the nodes by using back propagation for each training pair in the training samples. Also, we use the rectified linear unit function as the activation function for hidden layers and sigmoid function as the activation function for final output. Since we only have a sample of size 200, choosing quasi-Newton methods as the solver has better performance for small sample training. Then we need to construct the training, validation, and test sets. Since the fundamental factors are already neutralized (normalized) within each sector, we divide the sample data to 60%percent\\%%, 20%percent\\%%, 20%percent\\%% by convention. Without shuffling, we will have the historical data divided where test set contains the most recent data. For the corresponding output value, we assign 1 to samples with positive future return and 0 with negative returns. The complexity of neural network directly related to the number and sizes of hidden layers. For the purpose of avoiding overfitting or under-fitting, we need to find proper hyper parameters for neural network model. We start from a simple model with two layers. Let N\ud835\udc41Nitalic_N denote the number of nodes in each hidden layer, alpha is the hyper parameter for L2 regularization penalty function. With larger N\ud835\udc41Nitalic_N, the model is more complex. If alpha increases, the penalty for large weights increases, which makes the model tend to be more simple. Considering our sample size is small, intuitively we need to focus more on the overfitting problem. For a range of alpha and N\ud835\udc41Nitalic_N, we train the model using the training set data, and get the score for prediction on validation set. The score represents the probability of making a right prediction. Here are the results: To better understand how the hyperparameters influence model performance, we visualize the data by using N\ud835\udc41Nitalic_N and alpha as the bottom coordinates, and use the corresponding probability as the height. From the figure, we can tell that the model have several local optimal pairs. And the scores at the optimal points with relative large N values are also combined with small alpha values. For example, the combination of 14 nodes and alpha equals 0.01 has a local optimal score of 0.6. Since we are training with a small sample, using such a complex model with a high score is highly likely overfitting. Therefore, we start from the simple model by looking at models with 5 nodes model and check how the score varies with alpha. For model with 5 nodes, we see there are two local peaks with alpha equal to 1 and 0.25, then we pick the middle value 0.5 as the value of alpha considering the trade-off between variance and bias. Constructed and trained the model, next we would test the model by feeding a new data set to the model. On the test set, the score of the model is 0.64, which means the model predicts 64%percent\\%% of the results correctly. More detailed results are showed in the following table: On the test set, we have a 0.59 winning rates on the positive predictions and 0.72 on the negative predictions, which gives an overall winning rate of 0.64. By using the predictions from the trained model, we used the data from validation set to get trade signals. Instead of having signals of 1 or 0 as the model\u2019s output, we choose the probability of the prediction output being 1, which is given by the activation sigmoid function. Then we will have a time series of the probability for each sector, and rank the probability from highest to lowest where the highest probability will have a rank 1. For each cross-sectional ranking, we equally-weighted long sectors with rank 1 to 3 and short sectors with rank 9 to 11 to construct a dollar-neutral portfolio. On the test set, which is from September in 2020 to September in 2021, we have a Sharpe ratio of 2.21. The cumulative return plot is following: There are still issues that need to be considered carefully in the future. First is the factor neutralization. In previous model, we neutralized the factor exposure cross-sectionally, where the exposures reflect the relative level of factor exposure for one sector compared to other sectors at a given time. However, different sectors may have inner trends of higher exposures than others for some factors, especially for fundamental factor. What\u2019s more, we only have quarterly fundamental data available from 2017 and it is hard to implement time series normalization for each sector. Therefore, how to modify the factor exposures to make them comparable is a difficult problem. Secondly, as the sample size is small, the model might not be applicable on a wider range of time since we only trained and tested on the most recent five years. One possible way to improve this model is to use daily factors such as volume, close price as input, and convert fundamental factors to daily frequency by the corresponding quarter. Then we would have a sample size of approximately 1250 for each sector and over 13000 samples for training. However, the model might depends more on the daily factors rather than fundamental factors since their exposures would be the same value for each quarter. 1. Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency Narasimhan Jegadeesh; Sheridan Titman The Journal of Finance, Vol. 48, No. 1. (Mar., 1993), pp. 65-91. 2.The Global Industry Classification Standard, MSCI (1999)"
    },
    {
        "id": 2,
        "title": "Prompt emission of relativistic protons up to GeV energies from M6.4-class solar flare on July 17, 2023",
        "corpus": "We show evidence of particle acceleration at GEV energies associated directly with protons from the prompt emission of a long-duration M6-class solar flare on July 17, 2023, rather than from protons acceleration by shocks from its associated Coronal Mass Ejection (CME), which erupted with a speed of 1342 km/s. Solar Energetic Particles (SEP) accelerated by the blast have reached Earth, up to an almost S3 (strong) category of a radiation storm on the NOAA scale. Also, we show a temporal correlation between the fast rising of GOES-16 proton and muon excess at ground level in the count rate of the New-Tupi muon detector at the central SAA region. A Monte Carlo spectral analysis based on muon excess at New-Tupi is consistent with the acceleration of electrons and protons (ions) up to relativistic energies (GeV energy range) in the impulsive phase of the flare. In addition, we present another two marginal particle excesses (with low confidence) at ground-level detectors in correlation with the solar flare prompt emission. Since 1950 the observation of solar energetic particles from the solar flares and coronal mass ejections (CMEs) have been done with ground-level experiments, such as the neutron monitors (NMs) (Meyer et al., 1956; Simpson, 2000; Moraal et al., 2000) as well as the solar neutron telescope network (Hu & Semones, 2022; Vald\u00e9s-Galicia et al., 2009), all around the world. These observations have yielded a lot of new information. For instance, the existence of a prompt and gradual emission of solar energetic particles (SEP) in flares and CMEs, respectively, the correlations of the cosmic ray intensity with CMEs and other solar disturbances crossing the Earth, etc. (Chupp et al., 1987; Moraal et al., 2000) Also, the solar modulation of galactic cosmic rays is inversely correlated with solar activity, inferred through the number of sunspots, which can be the key to understanding more about space weather (Cade III & Chan-Park, 2015). Nowadays, particles accelerated to near the Sun can be detected by space-borne instruments such as the High-Energy Proton and Alpha Detector (HEPAD) on the Geostationary Operations Environmental Satellite (GOES) and the Advanced Composition Explorer (ACE) spacecraft at Lagrange L1 point, through the Electron Proton Alpha Monitor (EPAM) and the Solar Isotope Spectrometer (SIS), among others. Not all of the solar energetic particles can be measured at ground level. Even those SEPs from solar events with a good geoeffectiveness can be dissipated by the IMF, or deflected or captured by the Earth\u2019s magnetic field or until absorbed by atmosphere. On the other hand, ground-level enhancements (GLEs), typically in the MeV-GeV energy range, are sudden increases in cosmic ray intensities registered in most cases by NMs. GLEs are quite rare events, and fewer than 100 GLEs have been observed by NMs in the last 70 years. In most cases, the NMs that observed GLEs are located at regions with small geomagnetic rigidity cutoff, that is, at high latitudes (Shea & Smart, 2012). The GLEs follow the solar radiation storms, solar energetic particles (mostly protons) observed by GOES. They occur when a large-scale magnetic eruption, a coronal mass ejection and associated solar flare, accelerates charged particles in the solar atmosphere to high energies. However, in the present case, despite a radiation storm reaching above the S2-class on the NOAA scale on July 18, 2023, it did not generate a GLE, only a prompt emission of relativistic protons (ions) above GeV energies, during the phase eruptive, and observed by ground-level detectors strategically located, within the SAA central region (New-Tupi muon detector) and by the (Yangbagin muon telescope) at the Yangbajing Cosmic Ray Observatory (Tibet 4440 m a.s.l) (Zhang et al., 2010). Also, we looked for any signal in the counting rate at the Neutron Monitor\u2019s (NM) network around the world from Neutron Monitor Data Base (NMDB) https://www.nmdb.eu/nest/, with negative results. However, we found a low confidence signal only at Kerguelen NM, at geographical coordinates (49.3S, 70.3E), altitude of 33 m a.s.l, and an effective vertical cutoff rigidity of 1.14 GV. We present details of these observations. The New-Tupi muon detector is completely unmoderated (without no surrounding lead or other material). The muon detection energy threshold is about 200 MeV (see Appendix A). That contrasts with other muon detectors that have, in most cases, a surrounding lead material with a thickness of up to 5 cm. The shielding effect of the Earth\u2019s magnetic field on cosmic ray particles is quantified by the magnetic rigidity cutoff from a specific location (Smart & Shea, 2009). The smaller the rigidity cutoff, the lower the energy cosmic ray particles penetrate the magnetosphere. On the other hand, a restricted area between latitudes 20 and 40 of the southern hemisphere, over South America and the Atlantic Ocean poses a geomagnetic field with an anomalously lower intensity (around 22,000 nT). The region is known as the South Atlantic Anomaly (SAA) (Pav\u00f3n-Carrasco & De Santis, 2016). According to Swarm\u2019s satellite observations (Finlay et al., 2020), the SAA appears splitting into two, a smaller area over the Atlantic Ocean in southwest Africa and a larger area over eastern South America. Fig. 1 (top panel) summarizes the situation. We would like to point out that the location of the New-Tupi telescope coincides with the central part of the SAA indicated by the arrow on the left of Fig. 1 (top panel). The main effect of the SAA is on the satellites since the \u201970s. We know the frequent failures when they pass through the SAA region. A large amount of charged particles precipitation in this region damages and perturbates the satellites\u2019 electronics. Also, according to the results from the PAMELA detector at satellite\u2019s altitudes (Casolino et al., 2009), the effect of geomagnetic cutoff on low-energy particles is present in high latitudes close to the poles and also in the SAA region, composed mostly of low energy cosmic protons (E <<< 200 MeV ). In other words, the Pamella satellite has shown that the SAA introduces a sub-cutoff in the magnetic rigidity, below the Stormer\u2019s magnetic rigidity cutoff. We show that the SAA also affects secondary cosmic rays detected at ground level. As the horizontal magnetic component on Earth\u2019s surface is smaller on the SAA, the magnetic lateral dispersion of the secondary particles forming an air shower is smaller too. The effect increases the number of particles reaching a detector. In other words, this behavior mimics a magnetic rigidity sub-cutoff below the Stormer\u2019s rigidity cutoff. We show that effect through a Monte Carlo simulation based on CORSIKA-Fluka code (Heck et al., 2012; Battistoni et al., 2008), where 1.0\u00d71061.0superscript1061.0\\times 10^{6}1.0 \u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT proton air-showers are simulated, taking into account the magnetic coordinates (latitude, longitude) and height of several places where detectors are installed (mostly neutron monitors). Fig. 1 bottom left panel shows the lateral particle distribution in air-showers of cosmic rays, as detected from several ground-level detectors. In all cases, there is a fast rise of particles with the shower lateral development until reach um maximum value that happens for different values of R, called hereafter as Rmaxsubscript\ud835\udc45\ud835\udc5a\ud835\udc4e\ud835\udc65R_{max}italic_R start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT We can see that the number of shower particles at Rmaxsubscript\ud835\udc45\ud835\udc5a\ud835\udc4e\ud835\udc65R_{max}italic_R start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT in the SAA central region (SAA-CR) rigidity 9.6 GV is higher than at Rome and Athens, both with the rigidity of 6.3 GV, and 8.5 GV, respectively, i.e., minors than the SAA-CR. Already Fig. 1 bottom right panel, shows a correlation between Rmaxsubscript\ud835\udc45\ud835\udc5a\ud835\udc4e\ud835\udc65R_{max}italic_R start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT versus the geomagnetic Stormer rigidity cutoff of six different places (black circles), including the SAA-CR (blue square). The solid red line is a linear fit, and the two dotted red lines delimit the region with significance of \u00b11\u03c3plus-or-minus1\ud835\udf0e\\pm 1\\sigma\u00b1 1 italic_\u03c3. Only two places are out from the \u00b11.0\u03c3plus-or-minus1.0\ud835\udf0e\\pm 1.0\\sigma\u00b1 1.0 italic_\u03c3 significance region, the Thule (Groenlandia) in the lowest rigidity region and SAA-CR in the highest rigidity region. The high Stormer\u2019s rigidity of SAA-CR does not correspond to the high value of Rmaxsubscript\ud835\udc45\ud835\udc5a\ud835\udc4e\ud835\udc65R_{max}italic_R start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT expected by the correlation. From an interpolation, it is possible to see that the small value of Rmaxsubscript\ud835\udc45\ud835\udc5a\ud835\udc4e\ud835\udc65R_{max}italic_R start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT at SAA-CR correspond to the rigidity of only 3.1\u22121.7+3.0subscriptsuperscript3.13.01.73.1^{+3.0}_{-1.7}3.1 start_POSTSUPERSCRIPT + 3.0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT - 1.7 end_POSTSUBSCRIPT GV, within a confidence of \u00b11.0\u03c3plus-or-minus1.0\ud835\udf0e\\pm 1.0\\sigma\u00b1 1.0 italic_\u03c3. This behavior of having a location close to the Equator, with a nominal lower magnetic rigidity cutoff, favors the observation of phenomena such as the SEPs. On July 17, 2023, at \u223csimilar-to\\sim\u223c18h UT, the active region AR 13363 had an explosion, reaching an M6-class solar flare followed by a resplendent coronal mass ejection. Fig. 2 left panel shows the image from the Solar Dynamo observatory of the blaze of fire responsible for the X-ray flux reaching M6-class flare. Already the right panel shows the LASCO-C2 coronograph image of its associated CME on July 18, 2023, at 00:42 UT. NOAA prediction models confirmed that a CME originated in the powerful M6-class flare from sunspot AR3363 would pass through the magnetosphere on July 20, triggering at least a G1-class (minor) geomagnetic storm. However, no magnetic storms were observed. Fig. 3 shows the GOES-18 X-ray flux (upper panel) and the GOES-16 proton flux (bottom panel). The X-ray flux peaks at 18:00 UT, while the proton flux has two peaks. The first (in orange) is due to the acceleration of protons during the impulsive fast-rising phase of the flare peaking at 18:09 UT. The delay between the X-ray and proton flux peaks is because the proton velocity is slightly less than c, and the proton path is longer. The second peak are the protons accelerated by CME shocks, peaking at 18:14 UT, and it\u2019s the so-called gradual phase and is characterized by its long duration, up to several days. Fig. 4 shows the temporal coincidence between the GOES proton flux in the impulsive phase and New-Tupi muon excess. Particles (mostly proton) are accelerated in this phase exclusively by the flare, during the fast-rising until to reach the first peak (orange sector) in Fig. 4. However, in the so-called gradual phase, protons accelerated by CME\u2019s shocks, the proton flux does not reach the GeV energy range because there are no excess muons at ground level. We perform a Monte Carlo simulation of air showers initiated by SEP (protons) using the CORSIKA code (Heck et al., 2012), together with the FLUKA interaction model (Battistoni et al., 2008), that works well at GeV and sub-GeV energies, including secondary particle decay. The surviving particles are tracked through the atmosphere until they reach ground level (sea level). Most particles are muons with a small contribution of electrons and nucleons. The aim is to obtain the yield function, S\u03bc(EP)subscript\ud835\udc46\ud835\udf07subscript\ud835\udc38\ud835\udc43S_{\\mu}(E_{P})italic_S start_POSTSUBSCRIPT italic_\u03bc end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ), that is, the number of muons at sea level per primary proton, for an estimate of the upper limit of the integral proton flux in the GeV energy range, associated with the impulsive phase of M-6-class flare with onset on July 17, 2023, at \u223csimilar-to\\sim\u223c 18 UT. Fig. 6 (black squares) shows the Monte Carlo output under the New-Tupi geomagnetic conditions and vertical proton incidence, and fitting as where A\u03bc=(6.8\u00b11.4)\u00d710\u22123subscript\ud835\udc34\ud835\udf07plus-or-minus6.81.4superscript103A_{\\mu}=(6.8\\pm 1.4)\\times 10^{-3}italic_A start_POSTSUBSCRIPT italic_\u03bc end_POSTSUBSCRIPT = ( 6.8 \u00b1 1.4 ) \u00d7 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT, \u03bd=1.18\u00b10.24\ud835\udf08plus-or-minus1.180.24\\nu=1.18\\pm 0.24italic_\u03bd = 1.18 \u00b1 0.24, and E0=10.2\u00b12.1subscript\ud835\udc380plus-or-minus10.22.1E_{0}=10.2\\pm 2.1italic_E start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 10.2 \u00b1 2.1 GeV. Fig. 6 shows the fits (red dot line). In addition, we assume here that the energy spectrum of solar protons in the GeV energy range, which is in the high-energy tail of the SEP spectrum, can be fitted by a single power-law function. There are two unknown quantities in the above power-law function: the coefficient APsubscript\ud835\udc34\ud835\udc43A_{P}italic_A start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT and the spectral index \u03b2\ud835\udefd\\betaitalic_\u03b2. A convolution between the yield function S\u03bc(EP)subscript\ud835\udc46\ud835\udf07subscript\ud835\udc38\ud835\udc43S_{\\mu}(E_{P})italic_S start_POSTSUBSCRIPT italic_\u03bc end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ) and the proton spectrum JP(EP)subscript\ud835\udc3d\ud835\udc43subscript\ud835\udc38\ud835\udc43J_{P}(E_{P})italic_J start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_E start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ) gives the response function (Augusto et al., 2016b), which is the number of muons in the excess signal at New-Tupi detector generated by the SEP during the period T. We express this convolution as where F(\u03b8)\u223cexp(\u03b8/C)similar-to\ud835\udc39\ud835\udf03\ud835\udf03\ud835\udc36F(\\theta)\\sim\\exp(\\theta/C)italic_F ( italic_\u03b8 ) \u223c roman_exp ( italic_\u03b8 / italic_C ) is the pitch angle distribution (Shea & Smart, 1982; Miroshnichenko et al., 2005). In the central region of SAA (New-Tupi), the transverse geomagnetic component is only B\u27c2=18.3subscript\ud835\udc35perpendicular-to18.3B_{\\perp}=18.3italic_B start_POSTSUBSCRIPT \u27c2 end_POSTSUBSCRIPT = 18.3 mT, that is, almost 80% smaller than the transverse component, at the same latitude, but outside from the SAA region, favoring the focusing factor of the geomagnetic parallel geomagnetic on the incident solar protons (small pitch angles). For the present event, we found exp(\u03b8/C)\u223c1similar-to\ud835\udf03\ud835\udc361\\exp(\\theta/C)\\sim 1roman_exp ( italic_\u03b8 / italic_C ) \u223c 1. The muon excess associated with protons emitted during the impulsive phase (see Fig. 4), and considering an effective angular aperture of 60 degrees around the zenith of the New-Tupi detector, the counting rate excess is Furthermore, we also obtain the integrated time primary fluence as For the present case, the GOES-proton fluence in the high-energy region (Ep\u2265\\geq\u226550) MeV is The terms on the left side of Eq. 1 and Eq. 5 are known. Thus, we can consider all possible values of \u03b2\ud835\udefd\\betaitalic_\u03b2 and APsubscript\ud835\udc34\ud835\udc43A_{P}italic_A start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT compatible with the observed muon counting rate excess, J\u03bcsubscript\ud835\udc3d\ud835\udf07J_{\\mu}italic_J start_POSTSUBSCRIPT italic_\u03bc end_POSTSUBSCRIPT, and the integrated GOES-proton fluence F\ud835\udc39Fitalic_F. Fig. 7 summarizes the situation. Giving: Ap=(1.20\u00b10.96)\u00d710\u22123/(cm2ssrGeV)\ud835\udc34\ud835\udc5dplus-or-minus1.200.96superscript103\ud835\udc50superscript\ud835\udc5a2\ud835\udc60\ud835\udc60\ud835\udc5f\ud835\udc3a\ud835\udc52\ud835\udc49Ap=(1.20\\pm 0.96)\\times 10^{-3}/(cm^{2}s\\;sr\\;GeV)italic_A italic_p = ( 1.20 \u00b1 0.96 ) \u00d7 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT / ( italic_c italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_s italic_s italic_r italic_G italic_e italic_V ) and \u03b2=1.89\u00b11.10\ud835\udefdplus-or-minus1.891.10\\beta=1.89\\pm 1.10italic_\u03b2 = 1.89 \u00b1 1.10. To make a comparison with satellite GOES data, we obtain the integral proton flux in the GeV energy range as Fig. 8 shows the results of the integral proton flux obtained from the New-Tupi muon excess observed in coincidence with the radiation. The red circles represent the GOES-16 data, and the black squares represent the expected proton flux in the GeV energy range obtained from Monte Carlo, muon excess at the New-Tupi detector, and the GOES-proton fluence. The origin of this transient event was the solar eruption, an M6-class flare (prompt emission), accelerating protons (ions) up to relativistic energies, GeV energy range. We have reported evidence of SEPs accelerated up to GeV energies during the eruptive phase of the M6-class solar flare on July 17, 2023. The result comes from a timing analysis of a muon excess from the New-Tupi detector at the SAA central region. Muons at New-Tupi are produced by protons (ions) interaction in the upper atmosphere reaching the Earth with a magnetic rigidity above 3 GV (\u223csimilar-to\\sim\u223c 3 GeV for protons). In most cases, SEP (mostly protons) detected by the GOES-16 spacecraft shows two steps. An impulsive phase, where the acceleration of protons (ions) is by the prompt emission of flare, followed by a gradual phase of long duration, where the acceleration of protons (ions) is by the associated CME shock waves. We want to point out that the muon excess produced by SEPs with an effective rigidity above the cutoff (\u223csimilar-to\\sim\u223c 3 GV) at the New-Tupi muon detector is in temporal coincidence only with the GOES proton flux from the impulsive phase (see Fig. 4) . Consequently, in the gradual phase, the protons accelerated by CME\u2019s shocks do not reach the GeV energy range because a muon excess is absent at ground level. A marginal muon excess also appears on the Yan ba Jing S-21(pointing 21 degrees south) muon telescope (in Tibet). Also, a marginal particle excess is seen only in the French Kerguel NM (close to the South polar region). In both cases, the excesses are in (temporal) coincidence with the GOES proton flux (impulsive phase). However, it is hard to verify whether these excesses are genuine due to low confidence or simply fluctuations in the detectors\u2019 count rate. From a Monte Carlo analysis, we show that the SAA central region is favourable to the observation of transient solar events, especially SEP, because the magnetosphere has a \u201ddip\u00b4\u00b4 in this region, weakening the geomagnetic field strength and allowing the entrance of charged particles at large deeps in a region not far from the geographic Equator, giving a rigidity sub-cutoff around 3.1 GV in a place where the conventional Stormer geomagnetic rigidity cutoff is around 10 GV. This work is supported by the Rio de Janeiro Research Foundation (FAPERJ) under Grant E-26/010.101128/2018. We thank to NMDB Database (www.nmdb.eu), founded under the European Union FP7 Program (Contract No. 213007) by provide NMs data and the Space Weather Prediction Center from NOAA for its open data policy. The New-Tupi telescope is built with four identical particle detectors, forming two telescopes, as shown in Fig. 8 from Augusto et al. (2016a). Each detector consisting of an Eljen EJ-208 plastic scintillator slab of 150 cm x 75 cm x 5 cm and a Hamamatsu R877 photomultiplier of 127 millimeters in diameter, packaged in a pyramidal box. The PMT high voltage divider, amplifier, and high voltage power supplier are in the ORTEC ScintiPackTM Photomultiplier Base 296. From February 6, 2023, we have implemented a data acquisition system using a VERTILON high-speed pulse counting system (MCPC618-8 Channel). allowing for direct connection with the PMTs without the need for external preamplifiers, with a 250 MHz count rate per channel. Now the detector is working only in scaler mode or single particle technique (Aglietta et al., 1996), where the single hit rates of all four PMTs, are recorded once a second. However, so far, only two detectors are working. The coincidences among these detectors of each telescope will be implanted. Also, the barometric coefficients for cosmic muon fluxes at the Earth\u2019s surface can be obtained using the CORSICA code in Kovylyaeva et al. (2013). For New-Tupi detector conditions and at sea level, the barometric coefficient is about -0.14% per mb, about eight to nine times less than the typical barometric coefficient in NMs."
    },
    {
        "id": 3,
        "title": "Generative Inverse Design of Metamaterials with Functional Responses by Interpretable Learning",
        "corpus": "Metamaterials with functional responses, such as wave-based responses or deformation-induced property variation under external stimuli, can exhibit varying properties or functionalities under different conditions. Herein, we aim at rapid inverse design of these metamaterials to meet target qualitative functional behaviors. This inverse problem is challenging due to its intractability and the existence of non-unique solutions. Past works mainly focus on deep-learning-based methods that are data-demanding, require time-consuming training and hyperparameter tuning, and are non-interpretable. To overcome these limitations, we propose the Random-forest-based Interpretable Generative Inverse Design (RIGID), a single-shot inverse design method to achieve the fast generation of metamaterial designs with on-demand functional behaviors. Unlike most existing methods, by exploiting the interpretability of the random forest, we eliminate the need to train an inverse model mapping responses to designs. Based on the likelihood of target satisfaction derived from the trained forward model, one can sample design solutions using Markov chain Monte Carlo methods. The RIGID method therefore functions as a generative model that captures the conditional distribution of satisfying solutions given a design target. We demonstrate the effectiveness and efficiency of RIGID on both acoustic and optical metamaterial design problems where only small datasets (less than 250 training samples) are available. Synthetic design problems are created to further illustrate and validate the mechanism of likelihood estimation in RIGID. This work offers a new perspective on solving on-demand inverse design problems, showcasing the potential for incorporating interpretable machine learning into generative design and eliminating its large data requirement. Metamaterials with functional responses are engineered materials that exhibit varying properties or behaviors under different conditions. One example is metamaterials whose electromagnetic, acoustic, or elastic wave propagation behaviors change with wavelengths or frequencies [1]. Another example is metamaterials that exhibit changing properties or functionalities due to deformation in response to external stimuli like temperature [2] and magnetic fields [3]. Tailoring functional responses of these metamaterials is of interest to applications such as sound and vibration control, analog computing, medical imaging, sensing, communication, and soft robotics. In many use cases, rather than precisely controlling the complete functional responses, we only care about qualitative behaviors under certain conditions. For example, acoustic metamaterials were usually designed to have bandgaps at specified frequencies to achieve functionalities like wave-guiding [4, 5], focusing [6, 7], and vibration mitigation [8, 9, 10]. However, it is unnecessary and computationally expensive to design for the whole dispersion relation [11, 12, 13, 14, 15]. Similarly, we may design optical metamaterials to qualitatively manipulate optical properties (e.g., high or low absorption/reflection/transmission) under certain wavelengths, without requiring the entire spectral response to match an exact target [16, 17]. Identifying metamaterial designs from a given target forms an inverse design problem. Unlike many forward problems where one can obtain solutions (e.g., spectral responses or material properties under external stimuli) by modeling the physics or conducting experiments, inverse design problems are usually intractable. Traditionally, these problems are solved by iterative optimization (i.e., minimizing the difference between the actual quantity of interest and the target) [11, 12, 14]. This, however, requires repeatedly updating the design solution and solving forward problems. When the design target changes, one needs to rerun the entire optimization process. Thus, inverse design by iterative optimization becomes impractical if solving the forward problem (by simulations or experiments) is time-consuming or if the design target needs to change frequently. To accelerate the optimization approach, prior works replaced simulations or experiments with machine learning models [18, 19]. However, the efficiency and quality of final solutions are highly dependent on both the machine learning model and the optimization algorithm. On the other hand, a single run of optimization usually only returns one final solution, although multiple designs might satisfy a given target (i.e., the non-uniqueness of solutions). For example, multiple acoustic metamaterial designs may have bandgaps within the same target frequency range. This non-uniqueness nature of inverse design problems was also shown for optical metasurfaces [20, 21, 22]. The optimization approach eliminates the opportunity to explore diverse alternative solutions. To avoid iterative optimization and enable fast on-demand inverse design, prior research attempted to realize single-shot (iteration-free) inverse design using machine learning. There are three mainstream models (their schematic diagrams are shown in Appendix, Fig. 7). The most straightforward approach is to learn a direct inverse mapping from the response to design variables. Neural networks are the most commonly used machine learning model for this purpose, due to their high flexibility in approximating arbitrary nonlinear input-output relationships [23, 13]. Despite the simplicity of the direct inverse mapping, its underlying assumption of the response-design mapping being one-to-one does not hold in many cases due to the non-uniqueness of solutions, as mentioned earlier. Such non-uniqueness will cause conflicting training instances where the same input (response) is associated with distinct outputs (designs), which will destabilize the convergence during neural network training [20, 24]. To avoid this issue, past work proposed the Tandem Neural Network (T-NN) that cascades an inverse-design network with a forward-modeling network [20, 25, 26, 27, 28]. Its training is split into two steps: (1) pretraining the forward-modeling network to approximate the design-response mapping and (2) training the cascaded network by freezing the weights of the pretrained forward-modeling network. There is no loss function that forces designs at the intermediate layer to match data (which contains conflicting instances), hence the training convergence issue is avoided. Nonetheless, the original T-NNs still learn a one-to-one response-design mapping and cannot account for the non-uniqueness of design solutions. To fundamentally solve this problem, one needs to learn a one-to-many mapping. Bastek et al. [28] integrated stochastic sampling into the inverse-design network to allow the generation of multiple feasible solutions. A large body of recent works achieved the goal of learning one-to-many mapping by using conditional generative models, typically conditional generative adversarial networks (cGANs) [29, 30, 31, 32, 22], conditional variational autoencoders (cVAEs) [21], and conditional diffusion models [33]. These models can generate multiple designs given a target response by learning the distribution of designs conditioned on the response. Different generative models have distinct ways of learning conditional distributions. In general, this is realized by training neural networks to transform responses and random noise (or latent variables) into designs, so that the trained network can generate a non-deterministic design solution from a given target response and randomly sampled noise, which is equivalent to sampling from a conditional distribution. Although conditional generative models have demonstrated success in solving inverse design problems, they still have issues, such as high data demand, exhaustive hyperparameter tuning, slow training, and low interpretability, especially compared to traditional machine learning models like decision trees and random forests. On the other hand, Elzouka et al. [34] proposed to use the decision tree as a more interpretable model to solve both the forward prediction and inverse design problem. After training a decision tree for forward prediction, one can identify explicit design rules (i.e., feasible regions in the design space) by tracing from target leaf nodes to the root node. This approach also captures the one-to-many mapping nature of inverse design problems since it gives feasible design variable ranges rather than a single solution. However, there remain some limitations. Firstly, for solutions identified by the design rules, the method does not differentiate their likelihood of target satisfaction. Yet in reality, solutions always have different likelihoods due to the uncertainty of model estimation. Secondly, the method has to train two models: a random forest was trained first to ensure model accuracy and robustness, and then a large decision tree was trained to emulate the performance of the random forest and provide design rules. This is due to the challenge of deriving explicit design rules from an ensemble model like the random forest. Finally, the method was demonstrated on a problem with more than 104superscript10410^{4}10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT training data, while the effectiveness on smaller datasets (i.e., data with orders of magnitude smaller sample sizes) was not studied. This work aims to address the aforementioned problems by proposing a method called Random-forest-based Interpretable Generative Inverse Design (RIGID). Figure 1 shows an overview of this method. Specifically, we first train a forward prediction random forest. Then given a design target, we can probe the trained random forest to infer the likelihood of any design satisfying the target. To generate new designs tailored to the target, we can sample from the design space according to the likelihood. Compared to the most widely studied neural-network-based methods, RIGID has a much lower cost in training and hyperparameter tuning, and works more robustly on small-size datasets (as random forests are less prone to overfitting). Similar to deep generative models, it can generate a desired number of solutions, allowing one to explore alternative solutions that might have desired properties or functionalities beyond the ones considered as the target. The explicit likelihood estimation also offers an interpretable characterization of a design\u2019s target satisfaction probability and allows an exploitation-exploration trade-off when selecting generated designs. We validate the RIGID method on two metamaterial design examples \u2014 an acoustic metamaterial design example, where the target is to generate metamaterials with specific bandgaps, and an optical metasurface design example, where the target is to generate metasurfaces with high absorbance at specified wavelengths. Our contributions are three-fold. First, we propose a single-shot inverse design method that is fast, generative, interpretable, and small-data-compatible. Secondly, we demonstrate the effectiveness of the proposed method on acoustic and optical metamaterial design examples, and propose both qualitative and quantitative ways of assessing our method. Finally, we create two synthetic test cases for fast examination and validation of model performance. These test cases can be used for future benchmarking studies of related methods. The functional response of metamaterials can be modeled as y=f(\ud835\udc31,s)\ud835\udc66\ud835\udc53\ud835\udc31\ud835\udc60y=f(\\mathbf{x},s)italic_y = italic_f ( bold_x , italic_s ), where \ud835\udc31\ud835\udc31\\mathbf{x}bold_x denotes metamaterial design variables (e.g., materials and geometry parameters), s\ud835\udc60sitalic_s is an auxiliary variable representing the independent variable (or the \u201cx\ud835\udc65xitalic_x-axis\u201d) of the response (e.g., the frequency/wavelength or the external stimuli such as temperature), and y\ud835\udc66yitalic_y indicates the value of the response associated with our design target. In this paper, we assume y\u2208{0,1}\ud835\udc6601y\\in\\{0,1\\}italic_y \u2208 { 0 , 1 } since we only focus on qualitative behaviors at specified frequencies (e.g., for an acoustic metamaterial or an optical metamaterial design, whether a bandgap exists or whether the energy absorbance is higher than a threshold within a range of frequencies). We leave the more challenging problem of tailoring quantitative behaviors as future work. We use a random forest to approximate the function f\ud835\udc53fitalic_f. A random forest is an ensemble learning method that combines the predictions of multiple decision trees to improve accuracy and reduce overfitting [35]. The trained random forest serves as a forward prediction model that predicts the outcome y\ud835\udc66yitalic_y given design variables \ud835\udc31\ud835\udc31\\mathbf{x}bold_x and the auxiliary variable s\ud835\udc60sitalic_s. Compared to the widely used neural networks, the random forest as a forward prediction model offers (1) significantly faster training, (2) less hyperparameters to tune, (3) less susceptible to overfitting on small data, and (4) interpretability (i.e., the decision-making of each tree in the random forest is transparent). More importantly, this interpretability also allows us to realize inverse design without training a separate inverse model. Figure 2 shows how, by probing the trained random forest, one can estimate a likelihood distribution for target satisfaction of solutions over the entire design space and sample (generate) new designs based on this likelihood distribution. Since we target qualitative (binary) behaviors at specified s\ud835\udc60sitalic_s (e.g., a bandgap in 3-4 MHz frequency or high absorption at a wavelength of 400-500 nm), we first identify the leaf nodes (on each decision tree in the random forest) that are relevant to the s\ud835\udc60sitalic_s in the target (Fig. 2, Step 1). We do this by tracing down each tree, checking only the nodes that use s\ud835\udc60sitalic_s as the splitting feature, and pruning the branches that are irrelevant to the s\ud835\udc60sitalic_s in the target. For example, as shown in Fig. 2, there are two tree nodes using s\ud835\udc60sitalic_s as the splitting feature, with splitting criteria at s\u22645\ud835\udc605s\\leq 5italic_s \u2264 5 and s\u22647\ud835\udc607s\\leq 7italic_s \u2264 7. Given the target frequency range of 3\u2264s\u226443\ud835\udc6043\\leq s\\leq 43 \u2264 italic_s \u2264 4, we can remove the right branches of both nodes as these branches are only relevant to s>5\ud835\udc605s>5italic_s > 5 and s>7\ud835\udc607s>7italic_s > 7, respectively, which conflicts with the target range of 3\u2264s\u226443\ud835\udc6043\\leq s\\leq 43 \u2264 italic_s \u2264 4. After pruning these branches, we end up with a set of leaves relevant to the target (highlighted in Fig. 2, Step 1). When we have a combined target (e.g., bandgaps in both 3-4 MHz and 6-7 MHz, as shown in Fig. 2), we need to get the intersection of all the sets of relevant leaves and use that as the final set of relevant leaves (highlighted in Fig. 2, Step 2). Note that a combined target includes cases where there are multiple nonadjacent target ranges (e.g., 3-4 MHz and 6-7 MHz) or when a target range is split by a tree node (e.g., a target range of 4-6 MHz can be split by the node \u201cs\u22645\ud835\udc605s\\leq 5italic_s \u2264 5\u201d, thus we need to consider it as the combination of two target ranges \u2014 4-5 MHz and 5-6 MHz). A more detailed discussion of this step is in Appendix, Sec. B. The next step is to trace up the tree from the N\ud835\udc41Nitalic_N relevant leaves, obtained by Step 2, to the root node (Fig. 2, Step 3). This will result in N\ud835\udc41Nitalic_N decision paths, along which are nodes indicating splitting criteria for design variables \ud835\udc31\ud835\udc31\\mathbf{x}bold_x. Thus, each decision path represents a set of design variable ranges, or in other words, a region in the design space. We assign each region a score equal to the predicted probability at each corresponding leaf. This probability is learned from the training data and equals the proportion of positive data in a leaf. It indicates the tree\u2019s belief in the probability of a design \ud835\udc31\ud835\udc31\\mathbf{x}bold_x satisfying the target \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T if the design falls in the design space region corresponding to the leaf. Therefore, with a single decision tree i\ud835\udc56iitalic_i, we already have the map of likelihood \u2112m(\ud835\udc31|\ud835\udcaf)=\u2119m(\ud835\udcaf|\ud835\udc31)subscript\u2112\ud835\udc5aconditional\ud835\udc31\ud835\udcafsubscript\u2119\ud835\udc5aconditional\ud835\udcaf\ud835\udc31\\mathcal{L}_{m}(\\mathbf{x}|\\mathcal{T})=\\mathbb{P}_{m}(\\mathcal{T}|\\mathbf{x})caligraphic_L start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ( bold_x | caligraphic_T ) = blackboard_P start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ( caligraphic_T | bold_x ) for target satisfaction: each of the N\ud835\udc41Nitalic_N regions has a uniformly distributed likelihood equal to the predicted probability at the corresponding leaf, and the rest of the design space has a likelihood of 0 (Fig. 2, Step 3). Since a single decision tree usually lacks accuracy, robustness, and a way to quantify estimation uncertainty, we still want to take advantage of the random forest as an ensemble model for inverse design. We use Steps 1-3 to derive the likelihood distribution for each of the M\ud835\udc40Mitalic_M trees in the random forest, and simply use the average of these M\ud835\udc40Mitalic_M likelihood distributions as the final likelihood for target satisfaction, \u2112(\ud835\udc31|\ud835\udcaf)=\u2211mM\u2112m(\ud835\udc31|\ud835\udcaf)/M\u2112conditional\ud835\udc31\ud835\udcafsuperscriptsubscript\ud835\udc5a\ud835\udc40subscript\u2112\ud835\udc5aconditional\ud835\udc31\ud835\udcaf\ud835\udc40\\mathcal{L}(\\mathbf{x}|\\mathcal{T})=\\sum_{m}^{M}\\mathcal{L}_{m}(\\mathbf{x}|% \\mathcal{T})/Mcaligraphic_L ( bold_x | caligraphic_T ) = \u2211 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ( bold_x | caligraphic_T ) / italic_M, which is a more complex and smooth function (Fig. 2, Step 4). If more trees believe a design \ud835\udc31\ud835\udc31\\mathbf{x}bold_x has a higher likelihood of satisfying the target, then the design will have a higher likelihood \u2112(\ud835\udc31|\ud835\udcaf)\u2112conditional\ud835\udc31\ud835\udcaf\\mathcal{L}(\\mathbf{x}|\\mathcal{T})caligraphic_L ( bold_x | caligraphic_T ). Finally, to generate new designs, we can sample from \u2112(\ud835\udc31|\ud835\udcaf)\u2112conditional\ud835\udc31\ud835\udcaf\\mathcal{L}(\\mathbf{x}|\\mathcal{T})caligraphic_L ( bold_x | caligraphic_T ) using Markov chain Monte Carlo (MCMC) methods such as Metropolis-Hastings [36] (Fig. 2, Step 5). Compared to prior works, RIGID provides the following unique benefits: It is effective on small data problems as the random forest is less susceptible to overfitting. The training is fast (in seconds of wall time) and does not require computationally-demanding hyperparameter tuning. Once the training is done, no further training or iterative optimization is required to generate designs for different targets. The model is interpretable as one can easily probe the trained model to understand its reasoning behind any decision-making. It estimates the explicit likelihood of target satisfaction for every possible solution in the design space. Given a design target of specific functional behavior, we can generate an unlimited number of solutions based on the likelihood, allowing us to explore alternative solutions that might have desired properties or functionalities beyond the ones considered as the target. When generating design solutions, one can use a single parameter \u2014 the sampling threshold \u2014 to easily tune the trade-off between exploitation (i.e., generated designs have higher chances of satisfying the target) and exploration (i.e., generated designs cover a broader area of the design space), as we will demonstrate in Results. We demonstrate our RIGID method on an acoustic metamaterial design problem, an optical metasurface design problem, and two synthetic design problems. Based on a recent review article by Lee et al. [24] and other related works (e.g., [28]), existing single-shot inverse design methods were demonstrated on training data size ranging from 103superscript10310^{3}10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT to 106superscript10610^{6}10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT in scale. Here we show that our method can work with much smaller-scale datasets (less than 250 training samples). For all the test problems, we used the same random forest hyperparameter settings and did not perform hyperparameter tuning. Specifically, each random forest contains 1,000 trees with a minimum of 2 samples required to split an internal node and a minimum of 1 sample required to be at a leaf node. Gini impurity [37] was used as the splitting criterion at tree nodes. The train-test split ratio was 4:1. Since the positive/negative training data can be highly imbalanced (e.g., the frequency ranges with bandgaps are much narrower than those without), we used the Synthetic Minority Over-sampling TEchnique (SMOTE) [38] to over-sample the positive class. For all the case studies, the random forest training took less than 10 seconds on an Intel Core i5-9300H CPU 2.4GHz and 8GB memory. After training, we generate new designs by sampling from the resulting likelihood distribution using Metropolis-Hastings. In practice, Metropolis-Hastings can generate identical samples, which provides no benefits for design purposes. Thus in this work, we reject the designs identical to previous ones during sampling. Here we consider acoustic metamaterials that can control elastic wave propagation at ultrasound (MHz) frequencies. Varying the microscale geometries of acoustic metamaterials changes the dynamic properties of a material, such as bandgaps [10] (i.e., forbidden frequency ranges of a material) and wave propagation direction [4]. These materials promise applications in waveguides [4, 5], lenses [6, 7], and vibration mitigation [9]. We present the braced cubic design framework (Fig. 3A-B) as a method to tune the size and location of bandgaps (Fig. 3C). In particular, spherical micro-inertia are added to the center and corner of a braced cubic unit cell with strut radius rstrutsubscript\ud835\udc5fstrutr_{\\text{strut}}italic_r start_POSTSUBSCRIPT strut end_POSTSUBSCRIPT. Micro-inertia placed at the center of the brace has radius rcentersubscript\ud835\udc5fcenterr_{\\text{center}}italic_r start_POSTSUBSCRIPT center end_POSTSUBSCRIPT while micro-inertia placed at the corner of the cubic unit cell has radius rcornersubscript\ud835\udc5fcornerr_{\\text{corner}}italic_r start_POSTSUBSCRIPT corner end_POSTSUBSCRIPT. We randomly created 284 sets of geometric parameters \ud835\udc31=(rstrut,rcenter,rcorner)\ud835\udc31subscript\ud835\udc5fstrutsubscript\ud835\udc5fcentersubscript\ud835\udc5fcorner\\mathbf{x}=(r_{\\text{strut}},r_{\\text{center}},r_{\\text{corner}})bold_x = ( italic_r start_POSTSUBSCRIPT strut end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT center end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT corner end_POSTSUBSCRIPT ) with 4 \u2264rstrut\u2264absentsubscript\ud835\udc5fstrutabsent\\leq r_{\\text{strut}}\\leq\u2264 italic_r start_POSTSUBSCRIPT strut end_POSTSUBSCRIPT \u2264 6.41, 0 \u2264rcenter\u2264absentsubscript\ud835\udc5fcenterabsent\\leq r_{\\text{center}}\\leq\u2264 italic_r start_POSTSUBSCRIPT center end_POSTSUBSCRIPT \u2264 20, and 0 \u2264rcorner\u2264absentsubscript\ud835\udc5fcornerabsent\\leq r_{\\text{corner}}\\leq\u2264 italic_r start_POSTSUBSCRIPT corner end_POSTSUBSCRIPT \u2264 20 (unit: \u00b5m). The unit cell size was set at a=60\ud835\udc4e60a=60italic_a = 60 \u00b5m. For each of these designs, we performed Bloch-wave analysis to compute its acoustic dispersion relation. Bandgap location and width were extracted for each design based on its dispersion relation. Out of the 284 sets of design variables and bandgap data, we used 227 samples as training data. We first discretized the entire frequency range into 100 intervals, and trained a random forest to predict bandgap existence y\u2208{0,1}\ud835\udc6601y\\in\\{0,1\\}italic_y \u2208 { 0 , 1 } at a specific interval s\ud835\udc60sitalic_s for a given design \ud835\udc31\ud835\udc31\\mathbf{x}bold_x. The trained model has a test F1 score of 0.82. The resulting confusion matrix on test data is shown in Appendix, Tab. 1. To test the inverse design capability of RIGID, we randomly created 10 targets, each containing 1-2 frequency ranges in which bandgap(s) should exist. We generated 30 designs for each target by sampling from the resulting likelihood distribution over the design space111Note that it is possible for the likelihood to be zero everywhere in the design space when the model believes the target is unachievable. We ignore these cases as it is meaningless and impossible to sample designs from such likelihood distribution.. Bandgaps were identified from dispersion relations computed using Bloch-wave analysis. Figure 3D shows the kernel density estimation (KDE) for the likelihood of the 300 generated designs, conditioned on their target satisfaction. We use \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D and \ud835\udc9ffeassubscript\ud835\udc9ffeas\\mathcal{D}_{\\text{feas}}caligraphic_D start_POSTSUBSCRIPT feas end_POSTSUBSCRIPT to represent the complete set of generated designs and the set of generated designs that actually satisfy the target, respectively. Then \ud835\udc9f\\\ud835\udc9ffeas\\\ud835\udc9fsubscript\ud835\udc9ffeas\\mathcal{D}\\backslash\\mathcal{D}_{\\text{feas}}caligraphic_D \\ caligraphic_D start_POSTSUBSCRIPT feas end_POSTSUBSCRIPT denotes the set of generated designs that cannot fulfill the target in reality. In an ideal scenario, all solutions in \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D would satisfy the target, which means \ud835\udc9f=\ud835\udc9ffeas\ud835\udc9fsubscript\ud835\udc9ffeas\\mathcal{D}=\\mathcal{D}_{\\text{feas}}caligraphic_D = caligraphic_D start_POSTSUBSCRIPT feas end_POSTSUBSCRIPT, and their density profiles should coincide. However, this ideal scenario is not possible due to limited model accuracy. Conveniently, the estimation of target satisfaction likelihood offers us an indicator of what solution is more likely to violate the target. For a reasonable model, most designs in \ud835\udc9f\\\ud835\udc9ffeas\\\ud835\udc9fsubscript\ud835\udc9ffeas\\mathcal{D}\\backslash\\mathcal{D}_{\\text{feas}}caligraphic_D \\ caligraphic_D start_POSTSUBSCRIPT feas end_POSTSUBSCRIPT should have low estimated likelihood values. Consequently, the density of \ud835\udc9ffeassubscript\ud835\udc9ffeas\\mathcal{D}_{\\text{feas}}caligraphic_D start_POSTSUBSCRIPT feas end_POSTSUBSCRIPT\u2019s likelihood is a result of shifting some of \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D\u2019s density from left (low likelihood) to right (high likelihood). This expectation aligns with the observation in Fig. 3D. When sampling new designs or selecting solutions from generated designs, we can put a sampling threshold \u03c4\u2208(0,1)\ud835\udf0f01\\tau\\in(0,1)italic_\u03c4 \u2208 ( 0 , 1 ) on the likelihood values to filter out \u201cless promising\u201d solutions. To further examine model behavior and quantify how \u03c4\ud835\udf0f\\tauitalic_\u03c4 affects the inverse design outcome, we define the following metrics: where \ud835\udc9f\u03d5\u2265\u03c4subscript\ud835\udc9fitalic-\u03d5\ud835\udf0f\\mathcal{D}_{\\phi\\geq\\tau}caligraphic_D start_POSTSUBSCRIPT italic_\u03d5 \u2265 italic_\u03c4 end_POSTSUBSCRIPT is the set of generated designs with the likelihood of at least \u03c4\ud835\udf0f\\tauitalic_\u03c4 (i.e., the selected designs) and qisubscript\ud835\udc5e\ud835\udc56q_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT denotes the percentage overlap between the target and the actual behavior of selected designs. The satisfaction rate evaluates how many selected designs satisfy the target based on a binary criterion (i.e., whether or not a design satisfies the complete target), whereas the average score provides a soft measure where partial satisfaction is also counted. The average score is lower-bounded by the satisfaction rate. As shown in Fig. 3E, the selection rate decreases when \u03c4\ud835\udf0f\\tauitalic_\u03c4 increases since more solutions are filtered out. On the other hand, both the satisfaction rate and the average score increase with \u03c4\ud835\udf0f\\tauitalic_\u03c4, which indicates a high correlation between the estimated likelihood of a solution and its probability of actually achieving the target. As \u03c4\ud835\udf0f\\tauitalic_\u03c4 reaches 0.6, the satisfaction rate and the average score reach 1, indicating that all generated designs satisfy their targets. When sampling or selecting new solutions, we can use the sample threshold \u03c4\ud835\udf0f\\tauitalic_\u03c4 to tune the trade-off between exploitation and exploration \u2014 a low \u03c4\ud835\udf0f\\tauitalic_\u03c4 favors exploration as sampled solutions will cover a larger area of the design space, while a high \u03c4\ud835\udf0f\\tauitalic_\u03c4 favors exploitation as sampled solutions will have a higher chance of satisfying the target. Figure 3F visualizes the geometries and dispersion relations of designs generated based on a randomly created bandgap target. Only the top five designs with the highest likelihood values are shown. In this example, our method generates geometrically different designs that have a high probability of achieving target bandgaps, each yielding a slightly different dispersion relation. This is promising in design applications requiring other material properties, such as dynamic wave velocity or quasi-static stiffness, in which the user can select from a menu of designs with the same target bandgap but other varying properties. Generated designs based on the other nine bandgap targets can be found in Appendix, Figs. 8-10. Optical metasurfaces are artificially engineered systems that can support exotic light propagation building on subwavelength inclusions [39, 40, 41, 42, 43, 44]. Among a diverse array of devices, metamaterial absorbers [45, 46, 47, 48, 49, 50, 51] have been intensely studied for medical imaging, sensing, and wireless communications. In this case study, we consider four types of cross-sections (c\u2208{1,2,3,4}\ud835\udc501234c\\in\\{1,2,3,4\\}italic_c \u2208 { 1 , 2 , 3 , 4 }) chosen from the literature (Fig. 4B). It is assumed that a 3D geometric instance is composed of a stack of three layers of prismatic unit cells, each of which is vertically extruded and stacked (Fig. 4A). The geometries constructed in this way can be regarded as an instantiation of multilayered metasurfaces [52, 53, 54, 55, 56], which offer richer design freedom than the single-layer counterpart. The height of each layer (hl,l=1,2,3formulae-sequencesubscript\u210e\ud835\udc59\ud835\udc59123h_{l},l=1,2,3italic_h start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , italic_l = 1 , 2 , 3) is allowed to continuously vary between 50 and 150 nm. Herein we do not consider parametric variations of a given type of unit cell cross-section; yet those can be trivially incorporated in the proposed design framework if necessary. We also design the material of each layer (ml,l=1,2,3formulae-sequencesubscript\ud835\udc5a\ud835\udc59\ud835\udc59123m_{l},l=1,2,3italic_m start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , italic_l = 1 , 2 , 3). Three dielectric materials of interest, each of which is assigned to a different color in Fig. 4A, are Ti (red), Si (blue), and Ge (yellow). In general, a dielectric material is characterized through a complex refractive index n~\u2208\u2102~\ud835\udc5b\u2102\\tilde{n}\\in\\mathbb{C}over~ start_ARG italic_n end_ARG \u2208 blackboard_C defined as n~=n+jk~\ud835\udc5b\ud835\udc5b\ud835\udc57\ud835\udc58\\tilde{n}=n+jkover~ start_ARG italic_n end_ARG = italic_n + italic_j italic_k, where j=\u22121\ud835\udc571j=\\sqrt{-1}italic_j = square-root start_ARG - 1 end_ARG is the imaginary unit, n\u2208\u211d\ud835\udc5b\u211dn\\in\\mathbb{R}italic_n \u2208 blackboard_R involves the speed at which the light propagates through the material, and k\u2208\u211d\ud835\udc58\u211dk\\in\\mathbb{R}italic_k \u2208 blackboard_R is the extinction coefficient that dictates the energy loss due to the material. Within the frequency regime of interest, those exhibit nonlinear dispersion; both the real and imaginary terms in general are a non-analytic function of excitation wavelength s\ud835\udc60sitalic_s, i.e., n(s)\ud835\udc5b\ud835\udc60n(s)italic_n ( italic_s ) and k(s)\ud835\udc58\ud835\udc60k(s)italic_k ( italic_s ). In addition, the impact of the same material choice on the spectral response A(s)\ud835\udc34\ud835\udc60A(s)italic_A ( italic_s ) varies depending on the layer location at which the material is placed. Thus the highlight of this case study is the combinatorial nature of design, whose spectral responses are affected by the joint contributions of geometry and material. Based on the above configuration, we randomly sampled 258 sets of design variables \ud835\udc31=(c,h1,h2,h3,m1,m2,m3)\ud835\udc31\ud835\udc50subscript\u210e1subscript\u210e2subscript\u210e3subscript\ud835\udc5a1subscript\ud835\udc5a2subscript\ud835\udc5a3\\mathbf{x}=(c,h_{1},h_{2},h_{3},m_{1},m_{2},m_{3})bold_x = ( italic_c , italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) and computed their corresponding absorbance spectra using wave analysis. We set t=0.9\ud835\udc610.9t=0.9italic_t = 0.9 as the absorbance threshold, so that \u201chigh\u201d absorbance means the absorbance A(s)\ud835\udc34\ud835\udc60A(s)italic_A ( italic_s ) is no less than 0.9. We trained a random forest on 206 training data (i.e., 80% of the 258 designs and corresponding absorbance spectra) to predict whether \u201chigh\u201d absorbance is presented (i.e., the binary indicator y=1\ud835\udc661y=1italic_y = 1) at a wavelength s\ud835\udc60sitalic_s for a given design \ud835\udc31\ud835\udc31\\mathbf{x}bold_x. The trained random forest has a test F1 score of 0.83. The confusion matrix on test data is shown in Appendix, Tab. 2. Note that this problem involves inverse design with both continuous and categorical variables, which common optimization and generative modeling-based inverse design cannot handle well without special treatment [57, 58, 59]. On the other hand, our random forest-based method can naturally address such mixed-variable problems without any issues. Similar to the acoustic metamaterial design problem, we use 10 randomly created targets to evaluate the inverse design performance of RIGID, except that here a target is represented as the wavelength range(s) within which absorbance should be at least 0.9. We generated 100 designs for each target by sampling from the estimated likelihood distribution. Among the 1,000 generated solutions, we successfully conducted wave analysis for 911 designs and obtained their absorbance spectra. Figure 4D shows the KDE for the likelihood of these 911 designs, conditioned on their target satisfaction. The densities share similar behavior as in the acoustic problem (Fig. 3D) \u2014unsatisfied/infeasible designs \ud835\udc9f\\\ud835\udc9ffeas\\\ud835\udc9fsubscript\ud835\udc9ffeas\\mathcal{D}\\backslash\\mathcal{D}_{\\text{feas}}caligraphic_D \\ caligraphic_D start_POSTSUBSCRIPT feas end_POSTSUBSCRIPT are concentrated at low likelihood regions, which causes the likelihood density of satisfied/feasible designs \ud835\udc9ffeassubscript\ud835\udc9ffeas\\mathcal{D}_{\\text{feas}}caligraphic_D start_POSTSUBSCRIPT feas end_POSTSUBSCRIPT to be a result of shifting some of \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D\u2019s density from left (low likelihood) to right (high likelihood). The sampling threshold and metrics relation shown in Fig. 4E also follow the same trend as in the acoustic problem (Fig. 3E), which again demonstrates a strong positive correlation between the estimated likelihood and the probability of generated designs actually achieving their targets. Figure 4F shows generated optical metasurface designs with the top five likelihood estimations for a randomly created target. While the materials, cross-section geometries, and layer heights of generated designs can be different, all the designs satisfy the target (Fig. 4F, right panel). To further enhance the diversity of final solutions, we can use sampling strategies such as the one proposed in Ref. [60, 61] to identify a subset (of generated solutions) that simultaneously exhibits high likelihood and high diversity. Generated designs based on the other nine targets can be found in Appendix, Figs. 11-12. While the above metamaterial design problems represent practical use cases, the validation study is time-consuming due to the expensive computation of metamaterials\u2019 responses such as dispersion relations and absorbance spectra. To allow fast validation of the proposed method and easier inspection of the estimated likelihood in the design space, we create two synthetic case studies. Both problems have 2-dimensional \u201cdesign spaces\u201d that allow easy visualization. To construct the first synthetic problem, we used a squared exponential function with tunable parameters a\ud835\udc4eaitalic_a and b\ud835\udc4fbitalic_b to mimic the quantitative functional response of metamaterials. The qualitative response (e.g., \u201chigh\u201d or \u201clow\u201d energy absorption at a wavelength) is defined as: where z\ud835\udc67zitalic_z represents quantitative response and t\ud835\udc61titalic_t is a threshold that converts z\ud835\udc67zitalic_z into a qualitative response I(a,b;s)\ud835\udc3c\ud835\udc4e\ud835\udc4f\ud835\udc60I(a,b;s)italic_I ( italic_a , italic_b ; italic_s ). Specifically, I(a,b;s)=1\ud835\udc3c\ud835\udc4e\ud835\udc4f\ud835\udc601I(a,b;s)=1italic_I ( italic_a , italic_b ; italic_s ) = 1 can mean the existence of a bandgap or high absorbance at a frequency s\ud835\udc60sitalic_s. Then {s|I(a,b;s)=1}conditional-set\ud835\udc60\ud835\udc3c\ud835\udc4e\ud835\udc4f\ud835\udc601\\{s|I(a,b;s)=1\\}{ italic_s | italic_I ( italic_a , italic_b ; italic_s ) = 1 } represents a range of s\ud835\udc60sitalic_s that mimics our design targets, such as the bandgap or the frequency range of high absorbance. By varying a\ud835\udc4eaitalic_a and b\ud835\udc4fbitalic_b, we can produce different synthetic responses and ranges. Therefore, we can use a\ud835\udc4eaitalic_a and b\ud835\udc4fbitalic_b as synthetic design variables. There is a clear relation between these design variables and the range that Eq. 2 creates \u2014 a\ud835\udc4eaitalic_a and b\ud835\udc4fbitalic_b control the center location and the width of the range, respectively. In this design problem, we sampled 100 sets of a\ud835\udc4eaitalic_a and b\ud835\udc4fbitalic_b uniformly at random. We set t\ud835\udc61titalic_t as 0.9. Based on Eq. 2, we obtained the corresponding responses (Fig. 5A). These sets of a\ud835\udc4eaitalic_a, b\ud835\udc4fbitalic_b, and responses constitute a dataset for training and testing our model. Another synthetic design problem was constructed by replacing the squared exponential function in the SqExp problem with a superposed sine function. Given synthetic design variables a\ud835\udc4eaitalic_a and b\ud835\udc4fbitalic_b, we can produce qualitative responses using the following equation: Same as in the SqExp problem, we set t=0.9\ud835\udc610.9t=0.9italic_t = 0.9 and created a dataset with 100 sets of synthetic design variables and corresponding ranges derived from synthetic responses (Fig. 5B). Unlike the squared exponential function, the superposed sine function can be multimodal, which means it can result in multiple synthetic ranges to mimic, for example, multiple bandgaps. The bandgap locations are controlled by a\ud835\udc4eaitalic_a and b\ud835\udc4fbitalic_b. For each synthetic example, we split the data into 80 training data and 20 test data, and trained a random forest, with the same hyperparameter settings as the other problems, to predict the binary indicators I(a,b;s)\ud835\udc3c\ud835\udc4e\ud835\udc4f\ud835\udc60I(a,b;s)italic_I ( italic_a , italic_b ; italic_s ). The F1 scores are 0.85 and 0.86 for the SqExp and the SupSin problems, respectively. The resulting confusion matrices are shown in Appendix, Tables 3-4. We evaluated the inverse design performance with the trained models. Due to the fast evaluation of Equations 2 and 3, we can exhaust all the possible solutions in the design space to obtain the ground-truth feasible region(s) for a target. Figure 6A shows the estimated likelihood values and the ground-truth feasible regions under randomly created targets. In general, high-likelihood regions match actual feasible regions well, which further demonstrates the effectiveness of RIGID. We can also observe that feasible regions in the SqExp and the SupSin problems follow distinct patterns. In the SqExp problem, a\ud835\udc4eaitalic_a and b\ud835\udc4fbitalic_b control the center location and the width of the output range, respectively. Therefore, the position of the feasible region along the a\ud835\udc4eaitalic_a-axis moves with the location of the target range, while the feasible region gradually shrinks as b\ud835\udc4fbitalic_b decreases since the decrease of b\ud835\udc4fbitalic_b (i.e., output range width) causes the choice of a\ud835\udc4eaitalic_a (i.e., output range center location) to be more restricted to fit the target range. In the SupSin problem, there might be multiple bandgaps appearing at the peaks of the superposed sine function in Eq. 3. Design variables a\ud835\udc4eaitalic_a and b\ud835\udc4fbitalic_b control bandgap locations by translating each sine function. Due to the sine function\u2019s periodicity, we can obtain multiple feasible regions along both a\ud835\udc4eaitalic_a- and b\ud835\udc4fbitalic_b-axes. Figure 6A shows that the likelihood estimation by RIGID successfully captured the above-mentioned patterns of feasible regions. Figure 6B demonstrates how the estimated likelihood varies when increasing the number of trees in a random forest. With a single decision tree, the estimated likelihood function is almost a binary function and highly inaccurate. The likelihood in the SqExp case is even zero everywhere, which makes it impossible to sample designs based on the likelihood. As the number of trees increases, the likelihood function becomes smoother and eventually converges. Besides these qualitative visual inspections, we also calculated the metrics proposed in Eq. 1, as shown in Fig. 6C. For each of the two synthetic problems, these metrics were computed on 500 designs generated by giving five random target ranges. Again, the satisfaction rate and the average score increase with the sampling threshold, indicating a strong correlation between the sampling threshold and the probability of generated designs actually achieving their targets. In both problems, all the selected designs satisfy their targets (i.e., the satisfaction rates and average scores reach 1) when the sampling threshold reaches 0.8. We proposed RIGID, a single-shot inverse design method that generates metamaterials to satisfy qualitative behaviors of functional responses. Such qualitative behaviors are important design targets in many applications such as tailoring bandgaps of acoustic metamaterials for wave-guiding, focusing, and vibration mitigation, or tailoring the absorption level of optical metasurfaces at certain wavelengths for medical sensing, imaging, and communication applications. Unlike most existing inverse design methods that require training an inverse model to map targets to designs, the RIGID method takes advantage of the random forest\u2019s interpretability and derives the likelihood of target satisfaction by probing the trained forward model. Incorporated with MCMC, one can sample a desired number of new designs based on the estimated likelihood. Therefore, RIGID functions as a generative model that can capture the conditional distribution of satisfying designs given a target, or in other words, the one-to-many mapping from the target to satisfying designs. Using both real-world and synthetic design problems, we demonstrated that RIGID is efficient and effective on datasets with training sample sizes smaller than 250. Thus, RIGID is particularly useful when data collection is expensive, as in many cases where high-fidelity simulation or experimental data are needed. We used both qualitative and quantitative approaches to validate the proposed method. The quantitative results revealed a strong correlation between the estimated likelihood of a solution and its probability of actually achieving the target, which demonstrated the effectiveness of the likelihood estimation. Due to the fast evaluation of output responses and the transparency of ground-truth solutions, the proposed synthetic problems can be used for future benchmarking studies of metamaterial design problems. While we address qualitative design targets in this study, the idea of using random forest-based models for inverse design has the potential to generalize to quantitative targets. Such problems can be, for example, generating optical metasurface designs with specific optical spectra [62, 21], generating functional materials with target nonlinear constitutive relations [33, 63], or generating programmable metamaterials with prescribed functional responses [64, 65]. It is also straightforward to adjust the target to achieve multifunctionality (e.g., negative/positive Poisson\u2019s ratio under low/high compression rate [66]). Although this study only demonstrates the RIGID method on parametric design (i.e., designs are represented by geometric and/or material parameters), the method also applies to shape or topological design problems where the shape or topology of designs can vary without being restricted to a limited number of geometric parameters [62, 29, 21, 31, 67]. In those cases, as valid designs only lie on a lower-dimensional manifold of the design space, the likelihood of target satisfaction will be zero almost everywhere in the original design space. Thus before applying RIGID, we need to obtain a latent representation that compactly captures the manifold of valid designs [68, 69], and use the latent representation as design variables for inverse design. This work was supported by the startup funds from the J. Mike Walker \u201966 Department of Mechanical Engineering at Texas A&M University, the National Science Foundation (NSF) BRITE Fellow program (CMMI 2227641), the NSF CSSI program (OAC 1835782), the Kansas City National Security Campus (PDRD #705288), and NSF CAREER Award (CMMI-2142460). R.S. acknowledges financial support from the NSF Graduate Research Fellowship Program. Figure 7 shows the schematic diagrams of three mainstream machine learning models for single-shot inverse design of metamaterials. The purpose of Step 2 is to obtain the intersection of relevant design space regions for all the ranges of s\ud835\udc60sitalic_s in a target. We are approximating this goal by simply obtaining the intersection of relevant leaves. However, some non-intersecting leaves may still have intersecting design space regions. When assigning the probability to the intersecting region of two non-intersecting leaves A\ud835\udc34Aitalic_A and B\ud835\udc35Bitalic_B, we need to consider the predicted probabilities at both leaves (PAsubscript\ud835\udc43\ud835\udc34P_{A}italic_P start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT and PBsubscript\ud835\udc43\ud835\udc35P_{B}italic_P start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT). Specifically, the assigned probability at this intersecting region should be PAPBsubscript\ud835\udc43\ud835\udc34subscript\ud835\udc43\ud835\udc35P_{A}P_{B}italic_P start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT, which can be small. Therefore, we adopt the simplification of only considering the intersection of relevant leaves and ignoring the intersecting regions associated with non-intersecting leaves. The results also demonstrate that this is a reasonable approximation. We performed Bloch-wave analysis in COMSOL Multiphysics to compute the dispersion relations of acoustic metamaterials. Poisson\u2019s ratio of 0.49, Young\u2019s modulus of 2.7 GPa, and density of 1170 kg/m33{{}^{3}}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT were set as material properties with \u223c1.5\u00d7104similar-toabsent1.5superscript104\\sim 1.5\\times 10^{4}\u223c 1.5 \u00d7 10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT mesh elements per unit cell. We used Floquet-Bloch periodic boundary conditions to obtain the first 60 eigenfrequencies along all symmetry domains of the cubic irreducible Brillouin zone (Fig. 3B) for all lattices, thus generating a dispersion relation. We computed the absorbance spectra for optical metasurfaces using wave analysis inspired by Zhang et al. [56]. The RF Module of COMSOL Multiphysics\u00ae [70] was used to evaluate the spectral response of concern, which is the energy absorbance A(s)\ud835\udc34\ud835\udc60A(s)italic_A ( italic_s ) in the visible regime (380-700 nm). An absorbance spectrum is computed with respect to 33 wavelength components sksubscript\ud835\udc60\ud835\udc58s_{k}italic_s start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT that are uniformly discretized over the specified range. An incident plane wave is assumed to be given from the port, located at the top face of the analysis domain. We set the periodicity of the analysis domain as 400 nm. The periodic boundary condition on electromagnetic fields is imposed on the lateral faces of the analysis domain. A substrate made of SiO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT is placed right below a given unit cell instance (the black layers in Fig. 4A-B). With full electric fields computed through the wave analysis, the energy absorbance at a single wavelength s\ud835\udc60sitalic_s is quantified as A(s)=1\u2212|S11(s)|2\ud835\udc34\ud835\udc601superscriptsubscript\ud835\udc4611\ud835\udc602A(s)=1-|S_{11}(s)|^{2}italic_A ( italic_s ) = 1 - | italic_S start_POSTSUBSCRIPT 11 end_POSTSUBSCRIPT ( italic_s ) | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, where Sijsubscript\ud835\udc46\ud835\udc56\ud835\udc57S_{ij}italic_S start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT is the component of the S\ud835\udc46Sitalic_S-parameter matrix that specifies energy transfer between ports. We use the data presented in Ref. [71] to set the material dispersion of the dielectric. Test data for all the design problems are designs \ud835\udc31\ud835\udc31\\mathbf{x}bold_x (that the random forests have never seen during training) and their corresponding qualitative behaviors y\u2208{0,1}\ud835\udc6601y\\in\\{0,1\\}italic_y \u2208 { 0 , 1 } based on functional responses. In the acoustic metamaterial design problem, we have 57 test designs, yielding 5,700 test points as the entire frequency range of dispersion relations is discretized into 100 intervals for each design. In the optical metasurface design problem, we have 52 test designs, with each functional response discretized into 33 points, which yields 1,716 test points in total. In each of the synthetic design problems, we have 20 synthetic test designs, with each synthetic response discretized into 100 points. This results in 2,000 test data points in total. Confusion matrices showing test performances are in Tables 1-4. Figures 8-12 show the rest of generated designs and their corresponding responses in addition to those in Figures 3F and 4F."
    },
    {
        "id": 4,
        "title": "Building Open-Ended Embodied Agent via Language-Policy \nBidirectional Adaptation",
        "corpus": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on. Authors: achieve the best HTML results from your LaTeX submissions by selecting from this list of supported packages. Building open-ended learning agents involves challenges in pre-trained language model (LLM) and reinforcement learning (RL) approaches. LLMs struggle with context-specific real-time interactions, while RL methods face efficiency issues for exploration. To this end, we propose OpenContra, a co-training framework that cooperates LLMs and GRL to construct an open-ended agent capable of comprehending arbitrary human instructions. The implementation comprises two stages: (1) fine-tuning an LLM to translate human instructions into structured goals, and curriculum training a goal-conditioned RL policy to execute arbitrary goals; (2) collaborative training to make the LLM and RL policy learn to adapt each, achieving open-endedness on instruction space. We conduct experiments on Contra, a battle royale FPS game with a complex and vast goal space. The results show that an agent trained with OpenContra comprehends arbitrary human instructions and completes goals with a high completion ratio, which proves that OpenContra may be the first practical solution for constructing open-ended embodied agents. \\ul The challenge of building general-capable agents in the field of AI remains a significant endeavor (Stanley et al., 2017; Parker-Holder, 2022). This challenge necessitates the development of agents with the ability to continuously learn new skills and even create novel ones, a domain commonly referred to as open-ended learning. The research in open-ended learning is broadly categorized into two main factions: (1) pre-trained LLM agents for open-ended planning (Wang et al., 2023; Ouyang et al., 2022), and (2) RL agents for open-ended control (Team et al., 2021; Balduzzi et al., 2019). LLM-based methods, particularly those with closed source architectures, focus on planning based on general knowledge acquired during the pre-training stage (Wang et al., 2023). However, they may struggle to comprehend agent interactions in specific contexts and be impossible to learn in the case of real-time interaction. In contrast, RL methods conduct open-ended learning in an end-to-end manner, developing in diverse methodologies such as population-based training (Jaderberg et al., 2019; Team et al., 2021) and curriculum reinforcement learning (Wang et al., 2019; Samvelyan et al., 2022). While RL methods naturally learn contextual understanding by enabling agents to interact directly with environments, these approaches can be inefficient when aiming for open-endedness over the entire goal space, especially under tight computation budgets. Additionally, achieving meaningful open-endedness is essential under practical conditions, grounded by human instructions. To address these challenges, we propose a collaborative training framework that combines LLM and RL. In this framework, the LLM is responsible for contextual understanding and translating human instructions into structured goals, while the RL agent makes decisions to execute these goals. While some attempts have been made in this direction, existing studies often focus on single-sided learning, where one module is frozen while the other is trainable. This overlooks the potential cooperation between the LLM and RL agent. More specifically, existing efforts typically aim to improve training efficiency or reduce module interaction costs (Hu et al., 2023; Du et al., 2023). To substantiate our proposal, we present OpenContra, a two-stage implementation. In the first stage, OpenContra conducts independent training, enabling the LLM to generate goals corresponding to natural language instructions and environment contexts and RL agents to be capable of completing as many goals as possible. The LLM is initially trained with human-annotated datasets to generate formatted goal descriptions, followed by multi-step fine-tuning to enhance the precision of goal generation. A warm-up from non-goal RL is performed for RL training to master basic skills. Subsequently, a curriculum GRL procedure (Liu et al., 2022; Narvekar et al., 2020) is introduced to train a goal-conditioned policy for goal execution, starting from the well-trained non-goal RL agent. In the second stage, we collaboratively train the LLM and the RL agent to adapt to each other and complete goals corresponding to human instructions. Specifically, we create a dataset of instructions from diverse sources and then random sample instructions to combine with environment states to generate goals with the LLM. These goals are further used to train the goal-conditioned RL agent as in the previous stage. Simultaneously, for the training of the LLM, we introduce Reinforcement Learning with Agent Feedback (RLAF), which considers both environmental feedback and instruction completion. Thus, the LLM will be optimized towards comprehending agent interactions. We employ Contra as a testbed, a Battle Royale FPS game requiring high-frequency skill operation, with a high-dimensional goal space. Our evaluation includes human-instructed tests to validate open-endedness, wherein arbitrary human instructions are given to the LLM module, and the generated goals\u2019 quality and completion ratio is examined (Figure 1). Formally, the goal-conditioned reinforcement learning (GRL) could be formulated as a goal-augmented Markov Decision Process \u2133\u2133\\mathcal{M}caligraphic_M (Liu et al., 2022) as follows. Denoting \u2133\u2133\\mathcal{M}caligraphic_M a tuple \u27e8\ud835\udcae,\ud835\udc9c,\ud835\udca2,\ud835\udcab,\u211b,\u03b3\u27e9\ud835\udcae\ud835\udc9c\ud835\udca2\ud835\udcab\u211b\ud835\udefe\\langle\\mathcal{S},\\mathcal{A},\\mathcal{G},\\mathcal{P},\\mathcal{R},\\gamma\\rangle\u27e8 caligraphic_S , caligraphic_A , caligraphic_G , caligraphic_P , caligraphic_R , italic_\u03b3 \u27e9, where \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S, \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A, \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G the state space, action space and goal space, respectively. In general, \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G is a projection of \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S, i.e., \ud835\udca2=Proj(\ud835\udcae)\ud835\udca2\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc57\ud835\udcae\\mathcal{G}=Proj(\\mathcal{S})caligraphic_G = italic_P italic_r italic_o italic_j ( caligraphic_S ). \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P defines the state transition function that given a state and action tuple (st,at)subscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61(s_{t},a_{t})( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) at timestep t\ud835\udc61titalic_t, i.e., \ud835\udcab:\ud835\udcae\u00d7\ud835\udc9c\u2192\u0394(\ud835\udcae):\ud835\udcab\u2192\ud835\udcae\ud835\udc9c\u0394\ud835\udcae\\mathcal{P}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\Delta(\\mathcal{S})caligraphic_P : caligraphic_S \u00d7 caligraphic_A \u2192 roman_\u0394 ( caligraphic_S ), where \u0394(\u22c5)\u0394\u22c5\\Delta(\\cdot)roman_\u0394 ( \u22c5 ) denotes a distribution. \u211b:\ud835\udcae\u00d7\ud835\udc9c\u00d7\ud835\udca2\u2192\u211d:\u211b\u2192\ud835\udcae\ud835\udc9c\ud835\udca2\u211d\\mathcal{R}:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{G}\\rightarrow\\mathbb{R}caligraphic_R : caligraphic_S \u00d7 caligraphic_A \u00d7 caligraphic_G \u2192 blackboard_R defines the reward function r(s,a,g)\ud835\udc5f\ud835\udc60\ud835\udc4e\ud835\udc54r(s,a,g)italic_r ( italic_s , italic_a , italic_g ) with a given state, action, and goal. At the beginning of an episode \u03c4\ud835\udf0f\\tauitalic_\u03c4, a goal g\ud835\udc54gitalic_g will be sampled from a distribution Pgsubscript\ud835\udc43\ud835\udc54P_{g}italic_P start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, which generally defines a target for the agent to solve \u2133\u2133\\mathcal{M}caligraphic_M. We could further formulate a policy function for the agent that denotes the decision-making at each time step as \u03c0:\ud835\udcae\u00d7\ud835\udca2\u2192\u0394(\ud835\udc9c):\ud835\udf0b\u2192\ud835\udcae\ud835\udca2\u0394\ud835\udc9c\\pi:\\mathcal{S}\\times\\mathcal{G}\\rightarrow\\Delta(\\mathcal{A})italic_\u03c0 : caligraphic_S \u00d7 caligraphic_G \u2192 roman_\u0394 ( caligraphic_A ), which is a distribution over the action space. To solving a \u2133\u2133\\mathcal{M}caligraphic_M, the agent with policy \u03c0\ud835\udf0b\\piitalic_\u03c0 should maximize its accumulative reward over the goal space as where Pssubscript\ud835\udc43\ud835\udc60P_{s}italic_P start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT the state distribution determines the sampling of s0subscript\ud835\udc600s_{0}italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, \u03b3\u2208[0,1)\ud835\udefe01\\gamma\\in[0,1)italic_\u03b3 \u2208 [ 0 , 1 ) discounted the reward at each time step to guarantee the convergence of policy learning. Normally, r(st,at,g)\ud835\udc5fsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61\ud835\udc54r(s_{t},a_{t},g)italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_g ) is binary to denote whether g\ud835\udc54gitalic_g has been completed, i.e., r(st,at,g)\ud835\udc5fsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61\ud835\udc54r(s_{t},a_{t},g)italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_g ) values 0 or 1 according to whether st+1\u223c\ud835\udcab(\u22c5|st,at)s_{t+1}\\sim\\mathcal{P}(\\cdot|s_{t},a_{t})italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT \u223c caligraphic_P ( \u22c5 | italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) satisfies g\ud835\udc54gitalic_g, i.e., To approximate the accumulative reward related to the policy \u03c0(\u22c5|st,g)\\pi(\\cdot|s_{t},g)italic_\u03c0 ( \u22c5 | italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_g ), GRL suggests using the Universal Value Function Approximator (UVFA) V(st,g)\ud835\udc49subscript\ud835\udc60\ud835\udc61\ud835\udc54V(s_{t},g)italic_V ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_g ) to replace the state value function in traditional RL (Schaul et al., 2015a). The goal representation is still an open question, generally determined by the task to be resolved. We summarize the most popular categories as vector-based (Florensa et al., 2018; Ren et al., 2019; Nair et al., 2018; Fang et al., 2019; Pitis et al., 2020), vision-based (Campero et al., 2020; Nair & Finn, 2019; Warde-Farley et al., 2018; Mendonca et al., 2021; Bousmalis et al., 2023; Groth et al., 2021) and text-based (Brohan et al., 2023; Lynch & Sermanet, 2020). In this work, we model the goal space as a structured vector corresponding to multiple sub-goal spaces. One research direction of goal-conditioned RL is to learn a policy (or agent) that completes as many goals as possible, which could be regarded as achieving open-endedness in the goal space \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G. However, it is almost impossible to traverse all goals from \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G as (1) Pgsubscript\ud835\udc43\ud835\udc54P_{g}italic_P start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT may be agnostic to an agent, and (2) \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G may be uncountable and continuous. To satisfy the open-endedness requirements, a popular solution is leveraging curriculum reinforcement learning (Weng, 2020), i.e., automatically discovering novel goals from past training. Despite numerous curriculum approaches for achieving open-endedness, a critical challenge hinders the progress, i.e., sample inefficiency, due to the setting of binary reward signals (Equation 2). As a solution to this problem, existing research is central on reward shaping (Ng et al., 1999; Ecoffet et al., 2021; Ding et al., 2023; Trott et al., 2019) and hindsight goal relabelling (Andrychowicz et al., 2017; Fang et al., 2019; Zheng et al., 2022). Reward shaping is a straightforward and efficient idea to address the sparse rewarding problem, while it relies on domain knowledge and complex feature engineering. Hindsight relabelling is inspired by (Schaul et al., 2015b), which introduces a prioritized replay mechanism to improve the sample efficiency of RL methods. LLMs a class of networks that execute in auto-regressive for text generation tasks. Assuming a given sequence of text tokens is x1:t=(x1,\u2026,xt)subscript\ud835\udc65:1\ud835\udc61subscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc61x_{1:t}=(x_{1},\\dots,x_{t})italic_x start_POSTSUBSCRIPT 1 : italic_t end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), and the generation of a next token xt+1subscript\ud835\udc65\ud835\udc611x_{t+1}italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT is formulated as a probabilistic model: xt+1=argmaxx\u2208\ud835\udc9fP(x|x1:t)subscript\ud835\udc65\ud835\udc611subscript\ud835\udc65\ud835\udc9f\ud835\udc43conditional\ud835\udc65subscript\ud835\udc65:1\ud835\udc61x_{t+1}=\\arg\\max_{x\\in\\mathcal{D}}P(x|x_{1:t})italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_x \u2208 caligraphic_D end_POSTSUBSCRIPT italic_P ( italic_x | italic_x start_POSTSUBSCRIPT 1 : italic_t end_POSTSUBSCRIPT ), which satisfies the derivation of Markov chain. As for the training of LLMs, the target is equivalently to find a parameter set \u03b8LLMsubscript\ud835\udf03LLM\\theta_{\\text{LLM}}italic_\u03b8 start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT which satisfies the optimal generation, i.e., \u03b8LLM=argmax\u03b8P(xt+1|x1:t;\u03b8)subscript\ud835\udf03LLMsubscript\ud835\udf03\ud835\udc43conditionalsubscript\ud835\udc65\ud835\udc611subscript\ud835\udc65:1\ud835\udc61\ud835\udf03\\theta_{\\text{LLM}}=\\arg\\max_{\\theta}P(x_{t+1}|x_{1:t};\\theta)italic_\u03b8 start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT italic_P ( italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 1 : italic_t end_POSTSUBSCRIPT ; italic_\u03b8 ). With the booming of LLM research (Brown et al., 2020; Touvron et al., 2023; OpenAI, 2023), it is feasible to leverage LLMs as an interface or human behavior model to interact with autonomous agents. This research lies in human-AI interaction (or human-AI coordination/collaboration), and lots of work focuses on achieving collaboration by planning and learning with human behavior models (Nikolaidis & Shah, 2013; Sadigh et al., 2016; Swamy et al., 2020; Carroll et al., 2020). Our work is also related to this direction, as our evaluation is conducted with human-given instructions, so the LLM module in our work acts as an interface for the human-AI interaction. The generalization capability could be a key to human-AI interaction, as we want an agent to collaborate with arbitrary humans. Thus, there is a part of existing work focuses on training a best-response policy to a diverse set of human-like policies so that the agent may generalize to new coming humans (Cui et al., 2023; Strouse et al., 2022; Charakorn et al., 2022). In our work, an agent acquires the generalization capability by achieving open-endedness in the goal space. Contra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elements inherent in first-person shooting games (Choi & Kim, ; Gautam et al., 2021). The game unfolds with multiple hostile teams, necessitating players to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena. The agent\u2019s objectives encompass individual survival and the elimination of encountered enemies. An agent in Contra mandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and item collection. As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles and coordinating tactics with teammates. This characteristic defines an open-ended learning process where the agent continually explores the game environment to refine mastered skills and acquire new ones. Before delving into the detailed introduction of OpenContra, it is necessary to highlight two critical engineering designs to enhance training efficiency. The training inefficiency is often exacerbated in the context of open-ended learning and large-scale tasks where extensive rollouts are required for experience data collection and evaluations. To address this challenge, OpenContra incorporates a distributed RL framework inspired by AlphaStar (Vinyals et al., 2019) with modifications, resulting in the formation of the Actor-League-Learner architecture. In this architecture, the League is responsible for distributing rollout tasks to a cluster of Actors (CPU nodes) for data collection and evaluation, while optimization tasks are delegated to the Learner (GPU node) for policy updates. This distributed approach significantly enhances rollout throughput, thereby improving overall training efficiency. Another efficiency challenge stems from the iterative development of Contra. Unlike most of RL research that often relies on a fixed environment for algorithm development, Contra\u2019s environmental attributes continuously change over an eight-month development period. To mitigate the burden of retraining caused by these dynamic changes, we employ surgery (Berner et al., 2019) to retain learned skills at the lowest training cost, enabling adaptation to a changing observation/goal space while ensuring compatibility with network inputs. Detailed information on the distributed RL framework can be found in Appendix F, and version changes are documented in Table 8. This section focuses on the algorithmic design aspects of OpenContra. In the realm of GRL, the prevalent approach involves curriculum learning a goal-conditioned policy from scratch. However, achieving goal completion requires an agent to first master basic skills in interacting with the environment. This necessity imposes an additional computational burden on policy learning due to the extensive exploration required. Given the challenges posed by high-dimensional goal exploration, this work diverges from the common practice of leveraging goal decomposition to alleviate the issue. Instead, we opt for an explicit separation of basic skill learning and goal-conditioned learning. A key element in achieving this separation is ensuring that the agent acquires basic skills through non-goal RL before engaging in goal-conditioned learning. To fulfill this objective, we employ Proximal Policy Optimization (PPO) (Schulman et al., 2017), a highly efficient Actor-Critic algorithm (Sutton & Barto, 2018). Furthermore, we introduce a multi-head state value function designed to enhance the efficiency of policy learning, as depicted in Figure 4. This design encompasses two distinct heads: (1) a head dedicated to skill learning and (2) a head focused on learning to avoid obstacles when navigating. In the context of encouraging the agent to learn and achieve arbitrary goals in Contra, we define a goal space \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G as a Cartesian product of 68 sub-goal spaces: \ud835\udca2=\ud835\udca21\u00d7\u22ef\u00d7\ud835\udca268\ud835\udca2superscript\ud835\udca21\u22efsuperscript\ud835\udca268\\mathcal{G}=\\mathcal{G}^{1}\\times\\cdots\\times\\mathcal{G}^{68}caligraphic_G = caligraphic_G start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT \u00d7 \u22ef \u00d7 caligraphic_G start_POSTSUPERSCRIPT 68 end_POSTSUPERSCRIPT. Each \ud835\udca2isuperscript\ud835\udca2\ud835\udc56\\mathcal{G}^{i}caligraphic_G start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT (i\u2208[1,68]\ud835\udc56168i\\in[1,68]italic_i \u2208 [ 1 , 68 ]) is a set {gi|\u2205,g1i,\u2026,g|\ud835\udca2i|\u22121i}conditional-setsuperscript\ud835\udc54\ud835\udc56subscriptsuperscript\ud835\udc54\ud835\udc561\u2026subscriptsuperscript\ud835\udc54\ud835\udc56superscript\ud835\udca2\ud835\udc561\\{g^{i}|\\emptyset,g^{i}_{1},\\dots,g^{i}_{|\\mathcal{G}^{i}|-1}\\}{ italic_g start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | \u2205 , italic_g start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_g start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT | caligraphic_G start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | - 1 end_POSTSUBSCRIPT }, where \u2205\\emptyset\u2205 indicates that a sub-goal is not selected as a valid value. For a given goal g\u2208\ud835\udca2\ud835\udc54\ud835\udca2g\\in\\mathcal{G}italic_g \u2208 caligraphic_G, it is represented as g=(g1,g2,\u2026,g68)\ud835\udc54superscript\ud835\udc541superscript\ud835\udc542\u2026superscript\ud835\udc5468g=(g^{1},g^{2},\\dots,g^{68})italic_g = ( italic_g start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_g start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , \u2026 , italic_g start_POSTSUPERSCRIPT 68 end_POSTSUPERSCRIPT ), with gi\u2208\ud835\udca2isuperscript\ud835\udc54\ud835\udc56superscript\ud835\udca2\ud835\udc56g^{i}\\in\\mathcal{G}^{i}italic_g start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT \u2208 caligraphic_G start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT. The composition of each goal considers two types of features: unit features corresponding to the agent and other players, and environment features that can be determined or affected by agent interactions. Refer to Table 7 for detailed specifications. To encode a goal instance, we use a Residual-Network (He et al., 2016), denoted as ResNet(\u22c5)ResNet\u22c5\\textsc{ResNet}(\\cdot)ResNet ( \u22c5 ), resulting in ResNet(g)ResNet\ud835\udc54\\textsc{ResNet}(g)ResNet ( italic_g ). This encoded goal is then concatenated with the corresponding state embedding of g\ud835\udc54gitalic_g and used as input for the policy and value networks. In the realm of curriculum learning, we explore both automatic and manual paradigms. Automatic curricula, such as Unsupervised Environment Design (UED) (Dennis et al., 2020), were tested with slight modifications, but the agent struggled to explore efficiently. This may be attributed to our goal space being significantly vaster than those tested in previous works, resulting in a sparse regret distribution unsuitable for scheduling goal learning. Consequently, we opt for a manual curriculum, staging goal-conditioned learning through curriculum random and hindsight goal-generation. This approach aims to warm up the agent, facilitating exploration and better comprehension of goal input. Given the expensive cost of exploring the high-dimensional goal space, we gradually activate five dimensions until the full set is activated. Activation for new dimensions is determined by whether the agent has converged on the completion ratio. While random goal-generation aids exploration, it introduces challenges, including generating many unreasonable goals comprising contradictory sub-goals (Table 3) and encouraging the agent to explore more complex goals. The former arises because random goal-generation is independent of a specified environment context, and the latter is due to the goal-generation independent of the agent policy. To address these issues and promote open-endedness, we propose training a goal generator with Hindsight Replay Buffer (HER) (Andrychowicz et al., 2017), as summarized in Algorithm 2 (refer to Appendix H). As introduced in the aforementioned, sparse rewarding is a critical issue in Goal-RL, exacerbating inefficient exploration. To mitigate this, we extend the multi-head value function with a goal-completion value head to encourage the agent to complete goals. The corresponding reward is calculated as the Euclidean norm difference between two consecutive states and a goal: where \u2225\u22c5\u2225p\\|\\cdot\\|_{p}\u2225 \u22c5 \u2225 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT indicates the p\ud835\udc5dpitalic_p-norm. This reward provides a denser signal to the agent about its proximity to the goal, offering more nuanced information than a binary signal indicating whether it has reached the goal or not. In our current implementation, we set p=1\ud835\udc5d1p=1italic_p = 1. To further address degeneration, a KL-divergence distance between non-goal and goal-conditioned policies is introduced as a regularizer. The policy learning objective is reformulated as: where \u03c0\u22c6superscript\ud835\udf0b\u22c6\\pi^{\\star}italic_\u03c0 start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT represents the optimized non-goal policy, J(\u03c0\u03b8)\ud835\udc3dsubscript\ud835\udf0b\ud835\udf03J(\\pi_{\\theta})italic_J ( italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ) is the return, and \ud835\udfd9g=\u2205subscript1\ud835\udc54\\mathbb{1}_{g=\\emptyset}blackboard_1 start_POSTSUBSCRIPT italic_g = \u2205 end_POSTSUBSCRIPT indicates that the KL-divergence term is only activated for an empty goal. We fine-tune a pre-trained LLM to enable the LLM to generate goals with natural language instructions from set \u2110\u2110\\mathcal{I}caligraphic_I and abstracted environmental states from set \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O as input. To construct the training dataset \ud835\udc9f:=\u2110\u00d7\ud835\udcaa\u00d7\ud835\udca2assign\ud835\udc9f\u2110\ud835\udcaa\ud835\udca2\\mathcal{D}:=\\mathcal{I}\\times\\mathcal{O}\\times\\mathcal{G}caligraphic_D := caligraphic_I \u00d7 caligraphic_O \u00d7 caligraphic_G, we first build \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O a dataset of abstracted state from trajectories collected by the RL agent, where each abstraction o\u2208\ud835\udcaa\ud835\udc5c\ud835\udcaao\\in\\mathcal{O}italic_o \u2208 caligraphic_O contains essential features of all players and the present environment state (Appendix B). By accompanying \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O with a instruction set \u2110\u2110\\mathcal{I}caligraphic_I, we form a temporary dataset \ud835\udc9fx={(o,i)|(o,i)\u2208\ud835\udcaa\u00d7\u2110}subscript\ud835\udc9f\ud835\udc65conditional-set\ud835\udc5c\ud835\udc56\ud835\udc5c\ud835\udc56\ud835\udcaa\u2110\\mathcal{D}_{x}=\\left\\{(o,i)|(o,i)\\in\\mathcal{O}\\times\\mathcal{I}\\right\\}caligraphic_D start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = { ( italic_o , italic_i ) | ( italic_o , italic_i ) \u2208 caligraphic_O \u00d7 caligraphic_I }. Then, we leverage GPT-4 (OpenAI, 2023) to generate appropriate goals for \ud835\udc9fxsubscript\ud835\udc9f\ud835\udc65\\mathcal{D}_{x}caligraphic_D start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT to construct \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D. To ensure the goals generated by GPT-4 conform to the format we want, a comprehensive prompt engineering endeavor was conducted to establish a set of predetermined rules for GPT-4. The rule-based prompts that guide GPT-4\u2019s responses are meticulously documented in Table 10, and the examples of prompts for generation are listed in Table 17. As for the source of instruction set \u2110\u2110\\mathcal{I}caligraphic_I, we leverage multiple types of goal generations to ensure its diversity and scale. Specifically, we consider four types, including (1) HI (Human Instructions): constructed with human-annotated commands; (2) SI (State Instructions): GPT-4-generated instructions by giving a pair of states where the first comes from agent trajectories and the second is generated by modifying features of the first item; (3) AI (Agent Instructions): GPT-4-generated instructions by giving agent trajectory and the corresponding initial state; and (4) RI (Random Instructions): a mixture of previous three datasets to form a supplementary dataset. For more details, refer to Appendix E. With these datasets, we conduct fine-tuning for a ChatGLM-6B (Zeng et al., 2023; Du et al., 2022) with LoRA (Hu et al., 2021) in three steps, as illustrated in Figure 7, including (1) CoT-assisted fine-tuning (CoFT): each question as a task to fine-tune the LLM, aiming to expand the training data volume, enhancing the reasoning and understanding to \ud835\udc9fxsubscript\ud835\udc9f\ud835\udc65\\mathcal{D}_{x}caligraphic_D start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT; (2) Supervised Fine-tuning (SFT): to format the LLM-generated goals strictly, and further enhancing the accuracy; and (3) Ensemble Fine-tuning (EFT): multiple model checkpoints are used to generate goal candidates for each (o,i)\u2208\ud835\udc9fx\ud835\udc5c\ud835\udc56subscript\ud835\udc9f\ud835\udc65(o,i)\\in\\mathcal{D}_{x}( italic_o , italic_i ) \u2208 caligraphic_D start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT, then sub-goals with top counts will be used to reconstruct as ground truth to fine-tune the model to improve the precision of goal generation. After the training in previous steps, the RL agent obtained the capability to complete assigned goals, while the LLM learned to interpret human instructions and translate them into goals in the correct format. In the next stage, we aim to enhance the goal completion ratio corresponding to arbitrary human instructions, thereby achieving open-endedness of human instructions. Since the RL agent and LLM are independently trained, the LLM lacks execution feedback to tune the goal generation, which may result in unreasonability (Table 3), and the RL agent may perform a low completion ratio to the goals generated by the LLM if they were not or less explored in previous training. Thus, introducing co-training aims to resolve the above limitations and make the goals generated by the LLM linguistically sound and not be divorced from the agent\u2019s ability to execute them. The implementation of RLAF relies on PPO with a multi-factor reward that considers multiple aspects to ensure the LLM-based goal generation toward a high completion ratio and consistency with given human instructions. Specifically, they are (1) the evaluation of goal completion: the higher the completion ratio, the higher the reward value; (2) the evaluation of crucial sub-goal completion: for each batch of training data, we set the intersection of instructions in the batch and SI as the examination cases, and pairing each examination instruction with a preset target sub-goal required for successful execution, if the LLM outputs without such a sub-goal, then reward \u221222-2- 2, and +22+2+ 2 in vice, and further positive rewards are bestowed if a target sub-goal has been achieved; and (3) the evaluation of outputting the proper goal format: we punish the LLM according to edit distance. For more details, refer to Appendix K. We observed continual training making the LLM and the RL agent tend to compromise to a local optimal, i.e., the LLM outputs goals tend to comfort a high completion ratio for the agent but neglect consistency with human instructions, and the RL agent simultaneously rewards the LLM with a high completion ratio. To fix this issue, we propose a periodic reset of the LLM training, i.e., the parameters of the LLM will be reset for every set number of steps so that the two components can avoid being trapped in a local convergence, achieving enhanced goal completion, and keeping goals consistent with human instructions. Considering the training efficiency of LLMs, we conduct LoRA (Hu et al., 2021) to update the model weights. Figure 7 illustrates the whole training process, and Algorithm 1 summarizes the corresponding pseudo-code. We conduct empirical experiments to evaluate the efficacy of both stages of our proposed OpenContra. To make the Contra satisfy the learning requirements, we give well-designed spaces and reward functions as follows. The observation space encompasses many factors, including unit features detailing the agent\u2019s status, those of other players, and monsters. It also encompasses environmental features capturing interaction events and changes in the safe zone. Additionally, an agent-centric RGB bird\u2019s-eye-view of the local environment is considered. Due to the heterogeneity in the shape and data type of these features, we process them independently and concatenate them as inputs to both the policy and value network. Detailed information is available in Table 4. The action space is implemented on top of Contra\u2019s micro-operation API, comprising a collection of multi-grained actions. These actions range from fine-grained movements, such as six-degrees-of-freedom movement and weapon usage, to compound actions in coarse-grained categories, such as firing at a target. The total size of the action space is 54. Further details in Table 6. A comprehensive representation is employed for the reward function, considering various factors contributing to policy learning and goal completion. These factors are organized as a linear combination to formulate the reward function. The determination of weights for this combination follows a two-fold principle: (1) assigning weights to reward items based on their scales and emphasizing important factors; (2) dynamically adjusting weights in response to learning feedback, such as decreasing or increasing the weights of corresponding factors. Additional information is available in Appendix A. We evaluate the curriculum GRL within the OpenContra from three distinct perspectives: (1) the completion ratio, (2) generalization capability concerning unseen goals, and (3) robustness when integrating goal-conditioned learning atop non-goal learning. Given that the curriculum GRL in OpenContra comprises random and hindsight stages, our evaluation involves a comparative analysis with two baselines, namely, (1) Random: training the RL agent with randomly generated goals, and (2) Hindsight: training the RL agent with hindsight goal generation. For clarity, we refer to our method as Hindsight from Random. LABEL:fig:rl_pretrain_goal_train presents a comparison of the goal completion ratio across different pretraining methods on a validation dataset where goals are generated using random and hindsight goal generators. As depicted in LABEL:fig:rl_pretrain_goal_train, Hindsight from Random surpasses all baselines by 4% and 1%, respectively. LABEL:fig:rl_pretrain_goal_test evaluates the generalization on unseen goals, addressing the second aspect mentioned earlier. It is noteworthy that the unseen goals are re-combinations of goals obtained with Hindsight and LLM. As indicated in LABEL:fig:rl_pretrain_goal_test, Hindsight from Random excels over the other two baselines in terms of completion ratio. LABEL:fig:degradation_reward_kil answers the third point by comparing the use of KL-divergence regularizer for policy learning, considering changes in overall performance and the ability to eliminate enemies. Three metrics are designed for evaluation: (1) Mean basic reward per step, which indicates whether the current policy degenerates in performing basic skills per step against a well-trained non-goal policy, and intentional to emphasize the agent\u2019s immediate responsiveness over final results; (2) #Enemies killed, representing the average number of enemies killed by the agent per episode; and (3) #Enemies knocked down, representing the average number of enemies knocked down by the agent per episode. We assessed the performance of the LLM through two comparative experiments on a GPT-4 generated instruction dataset, aiming to investigate the impact of different dataset construction and fine-tuning paradigms. Evaluation metrics employed include precision, recall, and F1 score. It\u2019s worth noting that determining the precision of LLM outputs for certain sub-goals, such as associating the sub-goal \u201cmoving speed\u201d with \u201cvery fast\u201d versus \u201cfast\u201d poses challenges, as it might be perceived as a negative instance under precision measurement. Consequently, we assert that generating such sub-goals should weigh more heavily in performance evaluation than specific values, necessitating the introduction of corresponding metrics. As a solution, we propose three choice-based metrics: precision (choice), recall (choice), and F1 (choice), to address this requirement. Table 1 provides a comparison of five types of training datasets used in the three-stage fine-tuning process for the LLM, with \u201cALL\u201d representing the proportional mixture of the four base datasets. The comparison reveals that utilizing a mixture significantly outperforms individual base datasets, which indicates a mixture aids the LLM in capturing human command habits, understanding the implications of each abstracted state, and refining the initial execution capability of the policy from various perspectives, thereby enhancing goal generation. Table 2 compares four kinds of fine-tuning approaches with the three-staged fine-tuning introduced in Section 4.3, including (1) SFT: only use the target prompt without CoT data to supervised fine-tuning, which can be regarded as a baseline for a general SFT approach; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF \u2192\u2192\\rightarrow\u2192 SFT: further SFT target prompt after CoTF; (4) CoTF \u2192\u2192\\rightarrow\u2192 SFT\u2192\u2192\\rightarrow\u2192EFT: further ensemble fine-tuning target prompt after CoTF. With the comparison results, we conclude that CoTF and SFT can improve each other and achieve better performance. Furthermore, ensemble fine-tuning significantly enhances precision while marginally decreasing recall, making it more suitable for generating accurate concise goals. We conducted an analysis of the completion ratio corresponding to different goal dimensions during the co-training process, as illustrated in LABEL:fig_co-training_reset_a. It is evident that the completion ratio for human instructions is relatively low when the agent and the LLM have not been co-trained initially. However, as the co-training proceeds, the completion ratio gradually improves, particularly for goals with dimensions ranging from 2 to 4. This indicates that co-training effectively enhances the LLM\u2019s understanding of the agent\u2019s capabilities and preferences. Conversely, as depicted in LABEL:fig_co-training_reset_b, the initial completion ratio for each training loop increases with each reset, suggesting that co-training enhances the policy\u2019s ability to execute the language model\u2019s preferred outputs. Furthermore, the combined improvements in the LLM and the agent effectively enhance the ability to execute human instructions. Additionally, we investigated the change in sub-goal distribution during co-training, as depicted in LABEL:fig_co-training_reset_c and LABEL:tab_state_distributionB. The former illustrates changes within a loop, while the latter indicates changes across loops. The associated explanations for each Gisubscript\ud835\udc3a\ud835\udc56G_{i}italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are provided in Table 14. As training progresses, the probabilities associated with each sub-goal undergo gradual modifications. For instance, sub-goals with augmented probabilities include variables such as movement speed and the prone state due to their relatively attainable nature and influence in directing the agent toward accomplishing other objectives. Conversely, sub-goals with diminished probabilities encompass inter-agent distances and enemy visibility, which are linked to interactions with other agents and are not directly related to fulfilling linguistic instructions. The language model tends to generate outputs for these sub-goals only when absolutely necessary. As a case study, we have identified various instances in which objectives have been modified following the co-training process, as shown in Table 3. Evidently, after collaborative training, the LLM demonstrates its capacity to eliminate contradictory and irrational elements within the initial objectives. Furthermore, it exhibits the ability to introduce new objective components, thereby rendering the overall goal more attainable, all while retaining its exceptional semantic comprehension capabilities. These findings underscore the deepened comprehension of the linguistic model regarding the environment achieved through collaborative training. In this paper, we implement a co-training framework named OpenContra experts on learning an open-ended embodied agent, which leverages large language models, goal-conditioned reinforcement learning, and distributed learning to fill the blank of cooperating LLM and RL for efficient open-ended learning. We take a battle royale FPS game, Contra, as its testbed to support our claim, and the empirical results represent that OpenContra shows the potential as a practical solution. Despite the positive results, we admit there are still some limitations to our work that would be expected to be researched in the future\u2014for instance, a truly open-ended goal description instead of the handcrafted goal space in the current version; supporting multi-modality input/output to free from expensive feature engineering. The authors confirm their contribution as follows: Shaopeng Zhai: team leadership, open-ended learning, LLM/RLAF training, agent analysis, architecture design Jie Wang: infrastructure/framework engineering, non-goal agent training, open-ended learning, ablation studies, feature engineering Tianyi Zhang: non-goal agent training, open-ended learning, feature engineering Fuxian Huang: non-goal agent training, paper writing, open-ended learning Qi Zhang: LLM training, RLAF training, paper writing, ablation studies Ming Zhou: co-training framework, curriculum research, paper writing Jing Hou: LLM training, paper writing Contra seamlessly merges the last-man-standing gameplay dynamics with the survival, exploration, and scavenging elements inherent in first-person shooting games (Choi & Kim, ; Gautam et al., 2021). The game unfolds with multiple hostile teams, necessitating players to collaborate with teammates, withstand adversaries, and strive to outlast others in the ever-changing arena. The agent\u2019s objectives encompass individual survival and the elimination of encountered enemies. An agent in Contra mandates a sequential acquisition of skills, starting from fundamental abilities like walking, jumping, running, and item collection. As the learning proceeds, an agent must master more intricate skills such as evading enemy projectiles and coordinating tactics with teammates. This characteristic defines an open-ended learning process where the agent continually explores the game environment to refine mastered skills and acquire new ones. Figure 2 illustrates a map of Contra, which has diverse terrains such as plains, deserts, and snow-capped mountains. The observation space encompasses various factors, comprising unit features delineating the agent\u2019s status, as well as that of other players. Additionally, it includes environmental features characterizing interaction events and alterations in safe-zone configurations. Furthermore, an agent-centric RGB bird\u2019s-eye-view of the local environment is incorporated. Given the heterogeneity in the shapes and data types of these features, we adopt an independent processing approach, subsequently concatenating them to serve as input for the policy and value networks. Figure 3 illustrates the network for preprocessing an observation instance in our cases, where an observation includes four types of features as listed in Table 4, and we leverage independent encoding for each of them. Furthermore, considering the iterative development of the environment, it is convenient to leverage surgery with this network architecture to handle these changes. As introduced in Table 6, the instantiation of the action space is achieved through the utilization of the micro-operation API within the Contra framework. This process gives rise to a compilation of actions characterized by diverse levels of granularity, including fine-grained actions, alongside a suite of compound actions encompassing coarse-grained activities. In a detailed breakdown, the action space comprises several distinct types, namely movement direction, yaw direction, pitch direction, body action, basic action, and switch weapon. The movement direction action provides 16 discrete choices, each evenly distributed across a 360-degree spectrum. Likewise, the yaw direction action offers 16 choices with an equitable division of the 360-degree range. The pitch direction action encompasses three distinct values: -45, 0, 45. The body action incorporates nine diverse values: slide, stop, crouch, run, jump, ground, open or close door, rescue, and none. The basic action presents seven different values: fire, reloading, treat, pick up supply, drop supply, stop and fire, stop adjust and fire. Finally, the switch weapon action manifests three values: switch slot 0, switch slot 1, and none. The aggregated dimensionality of the action space is quantified at 54 in total. The primary objective of our training regimen is to equip the agent with the ability to play with other players in Contra while concurrently optimizing its success in overcoming opponents. To achieve this objective, we have formulated a diverse array of rewards designed to guide the agent\u2019s learning trajectory. However, the complexity involved in designing and fine-tuning these rewards is evident. Thus, our approach to reward weight design is characterized by a two-fold set of principles. Firstly, we allocate weights based on the expected value of each reward, ensuring a proportionate influence on the learning process. Secondly, we integrate a mechanism for smooth weight adjustments, facilitating dynamic and gradual modifications to the reward weightings. These principles collectively contribute to the construction of the reward function, learning an agent policy conducive to the desired optimal performance against opponents. In accordance with the first principle, we assume a referenced maximum return of 20, with different rewards assigned proportions based on their relative importance. For critical actions such as knocking down or killing an enemy, their values are set to approximately 4 (20% out of 20). Conversely, for less critical actions like scouting or incurring time penalties, their values are set to less than 1 (5% out of 20). It is crucial to highlight that the value estimation is derived through the discounted reward, with a discounting coefficient (\u03b3\ud835\udefe\\gammaitalic_\u03b3) set to 0.995. Concerning the second principle, throughout the training process, emphasis may be placed on learning specific skills during certain periods. For instance, the coefficient associated with a skill can be gradually amplified before mastery and subsequently reduced after proficiency is achieved. Detailed specifications are outlined in Table 5. In accordance with the aforementioned principles, the reward function has been constructed to systematically amalgamate multiple factors in a linear combination, facilitating their collaborative influence on guiding policy learning. As delineated in Algorithm 2, these factors are broadly classified into three categories: fundamental rewards rbasicsuperscript\ud835\udc5f\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc56\ud835\udc50r^{basic}italic_r start_POSTSUPERSCRIPT italic_b italic_a italic_s italic_i italic_c end_POSTSUPERSCRIPT, obstacle avoidance rewards roasuperscript\ud835\udc5f\ud835\udc5c\ud835\udc4er^{oa}italic_r start_POSTSUPERSCRIPT italic_o italic_a end_POSTSUPERSCRIPT, and goal achievement reward rgsuperscript\ud835\udc5f\ud835\udc54r^{g}italic_r start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT. The fundamental rewards are primarily directed at steering the agent towards enhanced game-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc. In the case of roasuperscript\ud835\udc5f\ud835\udc5c\ud835\udc4er^{oa}italic_r start_POSTSUPERSCRIPT italic_o italic_a end_POSTSUPERSCRIPT, the objective is to promote natural navigation and forestall the agent from encountering obstacles, such as stones and trees. Regarding the implementation, penalties are imposed on the agent for deviations from the optimal path. This optimal trajectory is determined by assessing the cosine similarity between the agent\u2019s current movement direction, a 3D unit vector, provided as an environmental signal, and the ideal obstacle-free trajectory derived from the action sequence in the trajectory: where \ud835\udc1dtenvsubscriptsuperscript\ud835\udc1d\ud835\udc52\ud835\udc5b\ud835\udc63\ud835\udc61\\mathbf{d}^{env}_{t}bold_d start_POSTSUPERSCRIPT italic_e italic_n italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT the actual movement direction of the agent, \ud835\udc1dtidealsubscriptsuperscript\ud835\udc1d\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc4e\ud835\udc59\ud835\udc61\\mathbf{d}^{ideal}_{t}bold_d start_POSTSUPERSCRIPT italic_i italic_d italic_e italic_a italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT the ideal movement direction, which is derived by combining the ideal movement direction from the previous moment with the movement action taken at the current moment: To address the issue of the agent getting stuck on obstacles due to short-term action sequences, we employ a smaller \u03b3\ud835\udefe\\gammaitalic_\u03b3 for the corresponding value head. Specifically, we set the \u03b3\ud835\udefe\\gammaitalic_\u03b3 value to 0.92. This adjustment helps mitigate the impact of the obstacle avoidance reward on long-term credit assignment, allowing for a more balanced consideration of immediate and future rewards in the agent\u2019s decision-making process. As for the goal-achieving reward, we\u2019ve introduced in the main text, please refer to Section 4.2. For a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process due to the scarcity of textual information within the simulation environment. The need arises to articulate non-linguistic elements, and the interaction between an agent and the simulation environment is centered on continuously observing the environment\u2019s state and generating corresponding actions. Therefore, the key aspects requiring verbalization primarily involve the state and actions of the agent. However, given the abundance of possible observation states in the simulation environment, it is impractical to use all of these states directly as prompts for the language model, especially considering token limitations. Consequently, there is a crucial need to extract and linguistically transform the most significant meta-states to facilitate successful model interaction. It is noteworthy that smaller language models have limitations in comprehending and manipulating numerical values effectively. To address this challenge, a deliberate effort is made to minimize the use of numerical values during the environmental linguistics process. For example, instead of specifying an agent\u2019s speed with specific numeric metrics like \u201cspeed: 1m/s \u2192\u2192\\rightarrow\u2192 3m/s\u201d a qualitative representation such as \u201cspeed: slower \u2192\u2192\\rightarrow\u2192 faster\u201d is adopted. This technique transforms the original continuous state into a limited, discrete meta-state, thereby enhancing the language model\u2019s understanding. Similarly, for expediting language model understanding, a discrete action space is adopted, with each action accompanied by a succinct artificial language description. This discreet articulation of actions contributes to the overall interpretability of the language model within the simulation environment. We list the details in Table 7. Figure 4 shows the network architecture used for reinforcement learning. On top of observation preprocessing in Appendix A, we introduce a backbone implemented with a fully-connected layer followed by three Residual Blocks. As for the policy head and three value heads, we implemented each of them as two connected Residual Blocks. It is noteworthy that the invisible enemy information, such as the nearest enemy\u2019s location, has also been introduced as an input to the value estimation, for the consideration of stabilizing the policy learning (Vinyals et al., 2019). As the project proceeded, Contra was continuously improved to satisfy richer featured environment dynamics. However, such an iterative development poses some challenges to the research of open-ended learning in an embodied situation, as the changes in API and environment attributes will make the training be non-stationary. A popular solution to resolve this issue is the surgery introduced by (Berner et al., 2019), which significantly reduces training time by maximizing retention of previously learned abilities. Similarly, we leverage surgery in four aspects to ensure the training adapts to the new changes, including model architecture, observation space, action space, and reward functions. Table 8 illustrates the main changes we conducted and the corresponding parameters. For the surgery of observation space and model architecture, we have introduced a decoupled encoding in Appendix A; for the surgery of action space, we directly extend the policy head in width to satisfy the new action space; for the reward functions, the essentials are to include the newly introduced features which can contribute to the learning, as we introduced in Appendix A, a linear combination has been considered to satisfy this requirement. In our experiment, we propose three novel surgery methods, where two for model architecture and one for observation space. The game environment has changed several times since the training started. The changes are mainly about adding player characters, adding player skills, adding weapon, modifying the map, etc. For all these changes, the proportion of new environments in which we train our policy grows slowly from 0%percent00\\%0 % to 100%percent100100\\%100 %. In this case, the variance is relatively small and the performance would quickly recover in the new environment. Figure 5 evaluates the utility of surgery, illustrating the changes in basic value during the training. It can be seen that the values change smoothly for most surgeries. Meanwhile, the values remain stable after the surgeries. These results prove the effectiveness of our surgery. The process of fine-tuning the language model is operationalized through a question and answer paradigm. In this framework, we provide the language model with a comprehensive depiction of the present conditions pertaining to the agent, its companions, and adversaries. Additionally, we furnish the model with the linguistic instructions furnished by its teammates. Subsequently, the language model is tasked with generating the agent\u2019s subsequent target meta state in accordance with the specifics outlined in the question and answer dataset, as elucidated in Table 17. The response is generated by GPT-4 and subsequently subjected to parsing and formatting processes facilitated by the rule-based coding mechanism. To ensure alignment between the responses generated by GPT-4 and the targeted meta-state format, a comprehensive prompt engineering endeavor was conducted to establish a set of predetermined rules for GPT-4. The rule-based prompts, which were employed to guide GPT-4\u2019s responses, are meticulously documented in Table 10. 1. Analyze the verbal orders of teammates and players, what do you want to do? According to the command, also analysis the relevant states of teammates and enemies that need attention. The verbal command of the teammate player is \u2018You should lie in wait\u2019, which means teammate player wants the agent to set up an ambush or take a hiding position. 2. Analyze which states of the agents are most relevant to the verbal commands of teammate player. The agents in the unselected states will adjust themselves to complete your plan. According to the teammate\u2018S command: 2.1. Need to hide: \u2018Whether prone position\u2019, \u2018Whether crouch position\u2019 2.2. Reduce movement to stay unnoticed: \u2018Average velocity\u2019, \u2018Length of distance moved\u2019 2.3. Ensure readiness for potential attack: \u2018Whether hold a gun\u2019 3. Plan how these key states need to be adjusted. According to the teammate\u2018S command: 3.1. \u2018Whether prone position\u2019: Need to hide: \u2018False\u2019 -> \u2018True\u2019 3.2. \u2018Whether crouch position\u2019: Alternative hiding posture if not prone: \u2018False\u2019 -> \u2018True\u2019 3.3. \u2018Average velocity\u2019: Reduce movement: \u2018Fast\u2019 -> \u2018Static\u2019 3.4. \u2018Length of distance moved\u2019: Reduce movement: \u2018Short\u2019 -> \u2018No movement\u2019 3.5. \u2018Whether hold a gun\u2019: Ensure readiness for potential attack: \u2018True\u2019 -> \u2018True\u2019 4. Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments may conflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustments in 3: 4.1. \u2018Whether prone position\u2019: \u2018False\u2019 -> \u2018True\u2019 4.2. \u2018Average velocity\u2019: \u2018Fast\u2019 -> \u2018Static\u2019 4.3. \u2018Length of distance moved\u2019: \u2018Short\u2019 -> \u2018No movement\u2019 4.4. \u2018Whether hold a gun\u2019: \u2018True\u2019 -> \u2018True\u2019 (Removed \u2018Whether crouch position\u2019 as it conflicts with \u2018Whether prone position\u2019 and prone position gives better cover for an ambush.) 5. According to the analyze and the planing of the verbal command, further analyze the behavior tendency required in the adjustment process. Mobile: Need to stop all movement: 0% Offense: Must be ready to attack if necessary: 20% Waiting: The main goal is to set up an ambush and wait: 70% Supplies: No need for supplies during this phase: 0% Scouting: Need to observe the surroundings, but not actively scout: 10% 6. Analyze how long the current command needs to be kept. According to the command of the teammate, taking a hiding position or setting up an ambush is usually done with the intention of waiting for an opportune moment to strike or observe the enemy\u2018S movement: The current command needs to be kept by \u2018long term\u2019. 1.Only select the most relevant and necessary states for planning, and the unplanned states will be adjusted by the agent itself 2.[Choose 1, Choose 2, \u2026] indicates the values that can be selected for the state. When you plan, you can only choose the value of the state from it, and do not invent new value not listed in [Choice1, Choice2, \u2026]. 3.The selected state can change the current value or maintain the current value. The agent will try to achieve and maintain the value of the state you choose after you give the plan. 4.Agents don\u2019t voluntarily discard items (for example guns, bullets, medical kits) unless items are reduced or set as False in your plan, so there is no need to keep them, only to choose when making changes. 5.Do not plan and adjust the states of teammates and enemies, they can move freely and cannot be controlled. 6.Avoid conflicts of states planing. For example, agent unable to move quickly when lying down, and unable to see enemies when length of distance from agent to enemy is far away. 7.Avoid the repetition of states planing. For example, if the Average velocity has been adjusted to be Fast, there is no need to adjust the Whether prone position to False, because the agent can automatically adjust state to fit overlapping meanings. 8.When it is necessary to refer to enemy or teammate information for planing, describe the specific state value during analysis. In an effort to mitigate the risk of overfitting our model to concise formatted outputs, thereby preserving its capacity for environmental reasoning, we augmented our dataset with a substantial volume of Chain of Thought data. This augmentation approach entails a systematic procedure whereby the large language model is guided through a step-by-step ideation process, ultimately culminating in the attainment of the intended target state. Concretely, our methodology commences with an initial inquiry into the semantic interpretation of the given instruction, followed by the identification of pertinent states, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporal considerations. Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chain of Thought procedure can be found in Tables 16 and 9. It is noteworthy that traditional Chain of Thought processes in existing large language models often generate sequential thoughts, a method characterized by a relatively protracted temporal trajectory. This sequential reasoning approach may not be well-suited to the high real-time demands typically encountered in first-person shooter (FPS) games. Furthermore, the singular-step reasoning capabilities inherent in smaller language models are intrinsically modest and prone to errors. Consequently, the amplification of error probabilities within the Chain of Thought reasoning process may not yield superior outcomes. In light of these considerations, we have undertaken a strategy that amalgamates Chain of Thought data with the final target state data, thereby enhancing the fine-tuning of our language model. In the course of test reasoning exercises, the language model promptly generates the ultimate target state, with the Chain of Thought information being implicitly encoded within the neural network parameters. To cover a comprehensive range of instruction types and state distributions, we generated four types of instruction sets, which, when combined with states sampled from the environment, result in four different datasets. These are the HI (Human Instruction) dataset, constructed based on human-annotated commands; the SI (State Instruction) dataset, built by reverse-generating commands based on state transitions specified by the intelligent agent; the AI (Agent Instruction) dataset, constructed by main kinds of instruction which can be complete by pre-trained Agent; and the RI (Random Instruction) dataset, generated through random sampling of agent state transitions and random commands. Human Instruction Dataset. In this dataset, we generate open-ended instructions manually, while the corresponding states are sampled from the intelligent agent\u2019s interaction logs. These are combined and annotated using GPT-4 based on the prompting method previously described. We found that due to varying frequencies of state changes during the agent\u2019s interactions, some states are difficult to capture comprehensively only using random sampling. To ensure a more comprehensive distribution of states in the data and to facilitate better understanding by the language model, we employ a multi-round rejection sampling approach to construct state set. Let S\ud835\udc46Sitalic_S be the set of states waiting to be sampled. We perform multiple rounds of sampling on S\ud835\udc46Sitalic_S, with Sigetsubscriptsuperscript\ud835\udc46\ud835\udc54\ud835\udc52\ud835\udc61\ud835\udc56S^{get}_{i}italic_S start_POSTSUPERSCRIPT italic_g italic_e italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT representing the set of states sampled in the i\ud835\udc56iitalic_i-th round, initially empty. Next, we sample a state s\ud835\udc60sitalic_s from S\ud835\udc46Sitalic_S without replacement and check whether s\ud835\udc60sitalic_s has any state values not present in Sigetsubscriptsuperscript\ud835\udc46\ud835\udc54\ud835\udc52\ud835\udc61\ud835\udc56S^{get}_{i}italic_S start_POSTSUPERSCRIPT italic_g italic_e italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. If it does, we accept it and add it to Sigetsubscriptsuperscript\ud835\udc46\ud835\udc54\ud835\udc52\ud835\udc61\ud835\udc56S^{get}_{i}italic_S start_POSTSUPERSCRIPT italic_g italic_e italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, otherwise we reject it. Once all states in S\ud835\udc46Sitalic_S have been sampled, one round is completed. Sigetsubscriptsuperscript\ud835\udc46\ud835\udc54\ud835\udc52\ud835\udc61\ud835\udc56S^{get}_{i}italic_S start_POSTSUPERSCRIPT italic_g italic_e italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the result of i\ud835\udc56iitalic_i-th round\u2019s sampling, and S\ud835\udc46Sitalic_S will be reset for the next round. This sampling method is employed to enhance the comprehensiveness of state coverage in all datasets except the Random Instruction dataset. State Instruction Dataset. In this dataset, we aim to cover a broader range of state changes in the instructions to enhance the language model\u2019s understanding of various state transitions. To achieve this, we design corresponding goals and instructions for all states. Specifically, for each value of each state, we generate a series of instructions that require the corresponding state and value. These are then annotated using GPT-4 based on the prompting methods previously described. The annotated results are checked; if they do not have corresponding states and values, manual annotation and modification are performed to include the relevant states. Agent Instruction. In this dataset, we aim to initially align the planning capabilities of the language model with the pre-trained abilities of an intelligent agent based on reinforcement learning policies. To do so, we generate potential corresponding instructions based on actual state changes in agent interactions. Specifically, we first sample a series of agent state pairs at 5-second intervals. For a subset of these, we manually annotate possible corresponding instructions. We then use these manual annotations as a knowledge base and employ the \"langchain\" method to use these examples to guide the annotation of the remaining data using ChatGPT-3.5. Finally, we represent all the instructions as vectors using OpenAI\u2019s embedding API and perform clustering. We select the 14 most representative types of instructions and pair them cyclically with two rounds of sampled states, ultimately constructing a dataset that better reflects the fundamental execution capabilities of the intelligent agent. Random Instruction. This dataset is primarily designed to enrich the data distribution. It is constructed by randomly generating instructions and fully randomly sampling states, and then annotated using GPT-4 based on the prompting methods previously described. The quantaty of the aforementioned four types of datasets is 507 for HI, 1098 for SI, 1441 for AI and 1382 for RI. Moreover, the test dataset construct instructions that differ from those used in the training data, then utilize GPT-4 to generate draft labels of goals and modified with manually filtered and annotated. This test dataset used for evaluating the model\u2019s ability to plan reasonably in response to instructions. And the size of dataset for each tuning step is 26,568 for CoT-assited fine-tuning, 4,428 for supervised fine-tuning, and 4,994 for ensembling fine-tuning. To improve the training efficiency, we adopt a distributed training system, shown in Figure 6. In this system, the Actors run over CPU nodes to collect training data, then send the collected data to the Learner which is deployed on a GPU node. We further take a LLM server to enable multiple replicas of LLM for goal generation, which improve the throughput of rollout when the RL training is switch to goal-conditioned cases. The hyperparameters used in our experiment are illustrated in Table 11. Some of them are set by following the official implementation of PPO (Schulman et al., 2017). Due to the limitation of compute resource, we did not tune these hyperparameters. Other dynamic hyperparameters are introduced their corresponding parts. Algorithm 2 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Gtbasicsubscriptsuperscript\ud835\udc3a\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc56\ud835\udc50\ud835\udc61G^{basic}_{t}italic_G start_POSTSUPERSCRIPT italic_b italic_a italic_s italic_i italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, Gtoasubscriptsuperscript\ud835\udc3a\ud835\udc5c\ud835\udc4e\ud835\udc61G^{oa}_{t}italic_G start_POSTSUPERSCRIPT italic_o italic_a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and Gtgsubscriptsuperscript\ud835\udc3a\ud835\udc54\ud835\udc61G^{g}_{t}italic_G start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT represent the basic return, obstacle avoidance return and goal-reaching return from time step t\ud835\udc61titalic_t till the termination, respectively. Inspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilize the collected trajectories for learning a goal generator GHERsubscript\ud835\udc3a\ud835\udc3b\ud835\udc38\ud835\udc45G_{HER}italic_G start_POSTSUBSCRIPT italic_H italic_E italic_R end_POSTSUBSCRIPT which accepts a state as input. We conclude its training in two steps: (1) constructing (s,g)\ud835\udc60\ud835\udc54(s,g)( italic_s , italic_g ) pairs with collected trajectories as illustrated in LABEL:fig:hindsight_goal_generation; (2) supervised training GHERsubscript\ud835\udc3a\ud835\udc3b\ud835\udc38\ud835\udc45G_{HER}italic_G start_POSTSUBSCRIPT italic_H italic_E italic_R end_POSTSUBSCRIPT with the above pairs and an MSE loss between the labeled goals and predicted goals. For step (1), we split trajectories into many segments with length of 200 timesteps. Then, we randomly sample a state s\ud835\udc60sitalic_s from the first 150 steps and sample a state s\u2032superscript\ud835\udc60\u2032s^{\\prime}italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT from the last 20 steps to derive a goal g=Proj(s\u2032)\ud835\udc54\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc57superscript\ud835\udc60\u2032g=Proj(s^{\\prime})italic_g = italic_P italic_r italic_o italic_j ( italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ), with a distribution proportional to their basic value V(s\u2032)\ud835\udc49superscript\ud835\udc60\u2032V(s^{\\prime})italic_V ( italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ). For step (2), we train GHERsubscript\ud835\udc3a\ud835\udc3b\ud835\udc38\ud835\udc45G_{HER}italic_G start_POSTSUBSCRIPT italic_H italic_E italic_R end_POSTSUBSCRIPT with s\ud835\udc60sitalic_s, \u0394t\u0394\ud835\udc61\\Delta troman_\u0394 italic_t, Vbasic(s)superscript\ud835\udc49\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc56\ud835\udc50\ud835\udc60V^{basic}(s)italic_V start_POSTSUPERSCRIPT italic_b italic_a italic_s italic_i italic_c end_POSTSUPERSCRIPT ( italic_s ) and Vg(s)superscript\ud835\udc49\ud835\udc54\ud835\udc60V^{g}(s)italic_V start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT ( italic_s ) as input to generate goals, where \u0394t\u0394\ud835\udc61\\Delta troman_\u0394 italic_t the time slot of goal completion, Vg(s)superscript\ud835\udc49\ud835\udc54\ud835\udc60V^{g}(s)italic_V start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT ( italic_s ) the goal-achieving value, Vbasic(s)superscript\ud835\udc49\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc56\ud835\udc50\ud835\udc60V^{basic}(s)italic_V start_POSTSUPERSCRIPT italic_b italic_a italic_s italic_i italic_c end_POSTSUPERSCRIPT ( italic_s ) the basic state value. We evaluate the impact of the rank parameter on performance during Lora fine-tuning of large language model neural networks. Generally speaking, the larger the rank parameter, the more comprehensive and thorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger. The experimental results are shown in Table 12. The size of lora rank has little impact on model performance indicators, but a large rank will cause the model training time and the size of the saved parameter file to increase dramatically. We next verified which neural networks in fine-tuning the ChatGLM-6B large language model can achieve the best performance. The experimental results are shown in Table 13. It is worth noting that only fine-tuning the MLP network without fine-tuning the attention network can achieve the best training results. Although generally speaking, the mainstream fine-tuning task of large language models is to fine-tune the attention layer network, but that task usually focuses more on answer semantics. In our task, we pay more attention to the format to meet the metastate parsing requirements, so fine-tuning the MLP network can achieve better results. We conduct experiments of various models with four percentages of fine-tuning train set, i.e., 100%, 30%, 10%, 3%, on the goal generation task. The results are shown in Table 15. It can be seen that as the amount of data gradually decreases, the performance of various training indicators gradually deteriorates under various settings. However, the smaller the amount of data, the greater the improvement brought by pre-training of our proposed CoTF method. The results show that the CoTF method we proposed can effectively collect and expand the chain of thought data related to the final goal, thereby avoiding overfitting of the training set in the case of small data. The calculation of the agent feedback reward is multifaceted, aiming to reflect the degree of completion as feedback for the training of the LLM. Specifically, three aspects are considered to satisfy the requirements, and the total agent feedback reward is given by the sum of them: rgfsubscriptsuperscript\ud835\udc5f\ud835\udc53\ud835\udc54r^{f}_{g}italic_r start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT - Minimal Distance to a Goal When Satisfying Environment Termination. As depicted by Equation 7, the agent progressively reduces the distance between the initial state and the goal, scaling it by the magnitude of the initial state-goal difference: rkeepfsubscriptsuperscript\ud835\udc5f\ud835\udc53\ud835\udc58\ud835\udc52\ud835\udc52\ud835\udc5dr^{f}_{keep}italic_r start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_e italic_e italic_p end_POSTSUBSCRIPT - Reward Indicating How Long the Goal Can Be Kept. As depicted by Equation 8, upon accomplishing the goal, the agent receives a reward proportional to the cumulative number of steps taken to sustain the goal state, scaled by the count of distinct sub-goals between the initial state s0subscript\ud835\udc600s_{0}italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and the goal g\ud835\udc54gitalic_g, i.e. n(g\u2229Proj(s0))\ud835\udc5b\ud835\udc54\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc57subscript\ud835\udc600n(g\\cap Proj(s_{0}))italic_n ( italic_g \u2229 italic_P italic_r italic_o italic_j ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ): rrndfsubscriptsuperscript\ud835\udc5f\ud835\udc53\ud835\udc5f\ud835\udc5b\ud835\udc51r^{f}_{rnd}italic_r start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r italic_n italic_d end_POSTSUBSCRIPT - Reward Indicating Whether the Generated Goal Satisfies the Projection of the Current State. As shown in Figure 9, an RND network is deployed for the evaluation of a state-goal pair. The calculation of rrndfsubscriptsuperscript\ud835\udc5f\ud835\udc53\ud835\udc5f\ud835\udc5b\ud835\udc51r^{f}_{rnd}italic_r start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r italic_n italic_d end_POSTSUBSCRIPT indicates that frequently appearing state-goal pairs are feasible, while those that never appear tend to be infeasible in the environment: where \u03c6\u22c6superscript\ud835\udf11\u22c6\\varphi^{\\star}italic_\u03c6 start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT a target network which shares the same architecture as the RND predictor but the network is non-trainable. The examination reward function is introduced as an intrinsic signal to encourage the LLM to generate goals with essential sub-goals. We use the SI dataset as the examination set \u2110Ssubscript\u2110\ud835\udc46\\mathcal{I}_{S}caligraphic_I start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT. For each training iteration, a batch of instructions \u2110trainsubscript\u2110\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\\mathcal{I}_{train}caligraphic_I start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT is randomly sampled from the full instruction dataset \u2110\u2110\\mathcal{I}caligraphic_I, and corresponding goals g\ud835\udc54gitalic_g are generated. After the agent finishes its rollout, the examination reward for each batch is computed based on the intersection \u2110\u2229=\u2110S\u2229\u2110trainsubscript\u2110subscript\u2110\ud835\udc46subscript\u2110\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\\mathcal{I}_{\\cap}=\\mathcal{I}_{S}\\cap\\mathcal{I}_{train}caligraphic_I start_POSTSUBSCRIPT \u2229 end_POSTSUBSCRIPT = caligraphic_I start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT \u2229 caligraphic_I start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT. For non-empty \u2110\u2229subscript\u2110\\mathcal{I}_{\\cap}caligraphic_I start_POSTSUBSCRIPT \u2229 end_POSTSUBSCRIPT, an examination reward for each instruction in \u2110\u2229subscript\u2110\\mathcal{I}_{\\cap}caligraphic_I start_POSTSUBSCRIPT \u2229 end_POSTSUBSCRIPT is computed as: Then, Resuperscript\ud835\udc45\ud835\udc52R^{e}italic_R start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT is calculated as Re=1|\u2110\u2229|\u2211i\u2208\u2110\u2229re(i,g,gsub|g=LLM(i,s))superscript\ud835\udc45\ud835\udc521subscript\u2110subscript\ud835\udc56subscript\u2110superscript\ud835\udc5f\ud835\udc52\ud835\udc56\ud835\udc54conditionalsubscript\ud835\udc54\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc54\ud835\udc3f\ud835\udc3f\ud835\udc40\ud835\udc56\ud835\udc60R^{e}=\\frac{1}{|\\mathcal{I}_{\\cap}|}\\sum_{i\\in\\mathcal{I}_{\\cap}}r^{e}(i,g,g_{% sub}|g=LLM(i,s))italic_R start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG | caligraphic_I start_POSTSUBSCRIPT \u2229 end_POSTSUBSCRIPT | end_ARG \u2211 start_POSTSUBSCRIPT italic_i \u2208 caligraphic_I start_POSTSUBSCRIPT \u2229 end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_r start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ( italic_i , italic_g , italic_g start_POSTSUBSCRIPT italic_s italic_u italic_b end_POSTSUBSCRIPT | italic_g = italic_L italic_L italic_M ( italic_i , italic_s ) ). The formatting reward for each generated goal is calculated by computing an edit distance, utilizing the Wagner-Fischer algorithm (Wagner & Fischer, 1974). With the defined reward functions, RLAF is applied with a reward function R=Rf+Re+Rm\ud835\udc45superscript\ud835\udc45\ud835\udc53superscript\ud835\udc45\ud835\udc52superscript\ud835\udc45\ud835\udc5aR=R^{f}+R^{e}+R^{m}italic_R = italic_R start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT + italic_R start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT + italic_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT and Proximal Policy Optimization (PPO) for each data point in a batch. In order to complete the command \u2018You should lie in wait\u2019, let us plan the states of the agent step by step using the following template: 1. Analyze the verbal orders of teammates and players, what do you want to do? According to the command, also analysis the relevant states of teammates and enemies that need attention. The verbal command of the teammate player is [Command], which means teammate player wants the agent\u2026 2. Analyze which states of the agents are most relevant to the verbal commands of teammate player. The agents in the unselected states will adjust themselves to complete your plan (analyze the reason first, then select key states one by one as few as possible and as important as possible according to the degree of importance)? According to the teammate\u2019s command: 2.1. [Reason1]: [State1] 2.2. [Reason2]: [State2] \u2026 3. Plan how these key states need to be adjusted (analyze the reason first, and then make adjustments one state by one state, the state can be changed or remain the same, and must be selected from the value range of the game state [Choice 1, Choice 2, \u2026])? According to the teammate\u2019s command: 3.1. [State1]: [Reason1]: [Current_value1] ->[Target_value2] 3.2. [State2]: [Reason2]: [Current_value1] ->[Target_value2] \u2026 4. Modify the adjustment that may be wrong, and refer to the Rules to analyze which state adjustments may conflict, repeat or be unnecessary, and output the modified adjustment plan: According to the states adjustments in 3\u2026 4.1. [State1]: [Current_value1] ->[Target_value2] 4.2. [State2]: [Current_value1] ->[Target_value2] \u2026 5. According to the analyze and the planing of the verbal command, further analyze the behavior tendency required in the adjustment process (the proportion of Mobile, Offense, Waiting, Supplies, Scouting, first analyze the reason, and then calculate the percentage) Mobile: [Reason1]: [Percent1] Offense: [Reason2]: [Percent2] Waiting: [Reason3]: [Percent3] Supplies: [Reason4]: [Percent4] Scouting: [Reason5]: [Percent5] 6. Analyze how long the current command needs to be kept (for example, the command of \u2018killing the enemy\u2019 needs to be kept for a \u2018short term\u2019, and the command of \u2018pay attention to reconnaissance\u2019 needs to be kept for a \u2018long term\u2019. First analyze the reason and then make a judgment). According to the command of the teammate, [Analysis]: The current command needs to be kept by \u2018[XX term]\u2019. If you see phrases like [Context] in answer template, replace the entire phrase according to the meaning of the Context, do not repeat the content; make analogy expansion for \u2018\u2026\u2019; keep \u2018:\u2019; absolutely do not modify others in template."
    },
    {
        "id": 5,
        "title": "Turing\u2019s Test, a Beautiful Thought Experiment",
        "corpus": "In the wake of large language models, there has been a resurgence of claims and questions about the Turing test and its value for AI, which are reminiscent of decades of practical \u201cTuring\u201d tests. If AI were quantum physics, by now several \u201cSchr\u00f6dinger\u2019s\u201d cats could have been killed. Better late than never, it is time for a historical reconstruction of Turing\u2019s beautiful thought experiment. In this paper I present a wealth of evidence, including new archival sources, give original answers to several open questions about Turing\u2019s 1950 paper, and address the core question of the value of Turing\u2019s test. In recent decades, the Turing test has been used as a practical experiment for publicity purposes and aptly criticized (Shieber, 1994a, b; Vardi, 2014), and has been the whipping boy of AI (Hayes and Ford, 1995), cognitive sciences and analytic philosophy (Shieber, 2004), and increasingly, with the rise of AI, the humanities and social sciences (Brynjolfsson, 2022). It is not uncommon for criticism coming from all these areas to take Turing\u2019s test literally, assuming that he encouraged deception as a criterion and/or proposed a crucial experiment to establish the existence of machine intelligence. Now, in the wake of large language models, science and technology outlets ask whether Turing\u2019s test can be a \u2018benchmark\u2019 for AI (Biever, 2023), and whether it is \u2018dead\u2019 (Wells, 2023). Based on recent primary research (Gon\u00e7alves, 2023e, a, b, c, d), in this paper I present a mass of evidence, including newly discovered archival sources, and a new perspective on Turing\u2019s test. I address a few problems that we will keep track as we go along: (P1subscript\ud835\udc431P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) Why would Turing design a test based on imitation, which can be seen as encouraging deception? (P2subscript\ud835\udc432P_{2}italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) Why to present multiple versions of a test, as opposed to a well-defined, controlled experiment? (P3subscript\ud835\udc433P_{3}italic_P start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT) Why gender imitation in a test of machine intelligence, and why conversation for an intelligence task? (P4subscript\ud835\udc434P_{4}italic_P start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT) Why are practical \u201cTuring\u201d tests circular, as Hayes and Ford (Hayes and Ford, 1995) claimed? Finally, the core problem, (P5subscript\ud835\udc435P_{5}italic_P start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT) What is the value of Turing\u2019s test for AI? Section 2 presents a reading of Turing\u2019s 1950 paper. Section 3 introduces newly discovered archival sources and examines Turing\u2019s concept of imitation, addressing P1subscript\ud835\udc431P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Section 4 shows that his presentation of the imitation game fits \u201cthe basic method of thought experiments\u201d (Mach, 1897), addressing P2subscript\ud835\udc432P_{2}italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Section 5 reconstructs the historical conditions of Turing\u2019s proposal, addressing P3subscript\ud835\udc433P_{3}italic_P start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT. Section 6 draws a parallel with the history of \u201cthe most beautiful experiment in the history of science\u201d (Palmieri, 2005), addressing P4subscript\ud835\udc434P_{4}italic_P start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT. Finally, Section 7 revisits the history of AI, addressing P5subscript\ud835\udc435P_{5}italic_P start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT, and Section 8 concludes the paper. In 1950, Alan Turing (Fig. 1) published the second of his three seminal papers, \u2018Computing machinery and intelligence\u2019 (Turing, 1950). The text has 27 pages, divided into seven sections, \u00a71 to \u00a77. Three main logical steps can be identified in his argument: the proposal (\u00a71 to \u00a73), the science (\u00a74 to \u00a75), and the discussion (\u00a76 to \u00a77). The proposal sought to replace the question \u201cCan machines think?,\u201d which he considered \u201ctoo meaningless to deserve discussion\u201d (p. 442),111Turing was coming from unstructured multidisciplinary debates in at least two editions of a seminar, \u201cMind and Machine,\u201d held at the Philosophy Department of Manchester University in October and December, 1949. Of the latter, one participant wrote in a Christmas postcard sent to Warren McCulloch: \u201cI wish you had been with us a few days ago we had an amusing evening discussion with Thuring [sic], Williams, Max Newman, Polyani [sic], Jefferson, J Z Young & myself \u2026\u2026\\ldots\u2026 An electronic analyser and a digital computer (universal type) might have sorted the arguments out a bit.\u201d Jules Y. Bogue to McCulloch, c. December 1949. American Philosophical Society, Warren S. McCulloch Papers, Mss.B.M139_005. Thanks to J. Swinton for this archival finding. with the imitation game. The purpose was to change the common meaning of the word \u2018machine\u2019 (e.g., a steam engine, a bulldozer) in light of the new mathematical science of \u2018universal\u2019 digital computing. The imitation game would allow for a grounded discussion of \u2018machine\u2019 and \u2018thinking,\u2019 seeking to expand the meaning of \u2018thinking\u2019 and detach it from the human species, much as the meaning of \u2018universe\u2019 was once detached from the Earth. In 1950, one of the OED definitions of \u2018machine\u2019 was:222New English Dictionary. Oxford, Vol. VI, Part II, M-N, p. 7. \u201ca combination of parts moving mechanically as contrasted with a being having life, consciousness and will \u2026\u2026\\ldots\u2026 Hence applied to a person who acts merely from habit or obedience to a rule, without intelligence, or to one whose actions have the undeviating precision and uniformity of a machine.\u201d Thus, by definition, common sense did not allow the meanings of \u2018machine\u2019 and \u2018thinking\u2019 to overlap. Despite Turing\u2019s emphasis in his opening paragraph that he did not intend to discuss how these words were \u201ccommonly used\u201d (p. 433), the hostility to his proposal can be seen from one of the first reactions, from a participant in the 1949 Manchester seminars, who quoted the above OED definition to appeal to common sense (Mays, 1952). The new question, which Turing considered to have a \u201cmore accurate form\u201d (Turing, 1950, p. 442), would be based on a vivid image, his \u201ccriterion for \u2018thinking\u201d\u2019 (p. 436), which he called interchangeably the \u2018imitation game\u2019 and his \u2018test.\u2019333For Turing\u2019s exact references to his test in all known sources, see (Gon\u00e7alves, 2023e, p. 2). The new question is whether a machine, playing A, the deceiver, can imitate a woman, a man, a human being, or another machine, playing B, the assistant, in a remotely played conversation game, and deceive an average interrogator, playing C, the judge, about its machine condition. However, the details and exact conditions of the imitation game as an experiment slipped through Turing\u2019s text in a series of variations that defies interpretation. A structural reading of the text identifies four different conditions of the game with respect to players A-B, namely, man-woman (p. 433), machine-woman (p. 434), machine-machine (p. 441), and machine-man (p. 442). These different conditions relate to four variants of the \u201cnew\u201d question that Turing posed to replace his \u201coriginal\u201d question (see Box 1). In addition to varying the genus/species (types) of the players, he also increased the storage and speed of the machine and provided it with a hypothetically appropriate program (Q\u2032\u2032\u2032superscript\ud835\udc44\u2032\u2032\u2032Q^{\\prime\\prime\\prime}italic_Q start_POSTSUPERSCRIPT \u2032 \u2032 \u2032 end_POSTSUPERSCRIPT), and suggested a base time for the interrogation session (Q\u2032\u2032\u2032\u2032superscript\ud835\udc44\u2032\u2032\u2032\u2032Q^{\\prime\\prime\\prime\\prime}italic_Q start_POSTSUPERSCRIPT \u2032 \u2032 \u2032 \u2032 end_POSTSUPERSCRIPT). Other seemingly relevant parameters were not mentioned, such as the number of interrogators used to arrive at a statistically sound conclusion, although their profile is mentioned \u2014 they should be \u201caverage\u201d \u2014, and later reiterated \u2014 they \u201cshould not be expert about machines.\u201d444\u2018Can automatic calculating machines be said to think?\u2019, Broadcast on BBC Third Programme, 14 and 23 Jan. 1952. Archives Centre, King\u2019s College, Cambridge, AMT/B/6. Because the machine must imitate stereotypes of what it is not, Turing\u2019s proposal has often been criticized for encouraging fakes and tricks. But this is part of the literal reading that Turing would have meant his test as a practical experiment about short conversations, gendered machines, and how to fine-tune them to fool average human interrogators. It misses the point of Turing\u2019s irony (Gon\u00e7alves, 2023b) and that his notion of imitation in 1950 was largely in continuity with his 1936 paper (Turing, 1936), as hinted at in his 1947 lecture,555\u2018Lecture to L.M.S. Feb. 20 1947.\u2019 Archive Centre, King\u2019s College, Cambridge, AMT/B/1. and as newly discovered correspondence with the Mexican-Canadian computer pioneer Beatrice Worsley (1921-1972) helps to clarify (see Box 2). In his letter to Worsley, Turing seems to be more interested in the relations between \u201cthe motions\u201d of Turing machines and infinite computers, whose behavior can be non-periodic. Perhaps he thought of the living human brain as an infinite computer, in the sense that it has a continuous interface with its environment, which constantly intervenes and changes its logical structure.666Cf. \u201cIntelligent Machinery\u201d, written in 1948 as a technical report to the National Physical Laboratory. Archives Centre, King\u2019s College, Cambridge, AMT/C/11. Now, the imitation game puts into empirical form the relation between digital computers, whose behavior is ultimately periodic, and the behavior of the human players. Can the behavior of their brains be approximated by a digital computer? Turing pursued this question. For his May 1951 broadcast, he wrote: \u201cthe view which I hold myself, that it is not altogether unreasonable to describe digital computers as brains \u2026\u2026\\ldots\u2026 If it is accepted that real brains, as found in animals, and in particular in men, are a sort of machine it will follow that our digital computer, suitably programmed, will behave like a brain.\u201d777\u2018Can digital computers think?\u2019, broadcast on BBC Third Programme, 15 May 1951. Archives Centre, King\u2019s College, Cambridge, AMT/B/5. Even if the human brain can only be compared to an infinite computer, could it not be simulated by a digital computer equipped with a sufficiently large memory? An excerpt of another newly discovered Turing letter to Worsley from mid-1951 can give more contour and provide further insight into Turing\u2019s views (see Box 3). A highlight in this excerpt is Turing\u2019s view that to the extent that the behavior of a neuron can be described as a stochastic process, it would be possible to \u201ccalculate random samples\u201d of the mechanism that embodies the brain and then imitate it.999Note the connection with the imitation game and his wartime experience studying and imitating the behavior of the Enigma machines used by the Nazi forces (Hodges, 1983). An effective imitation of the brain by a machine would require knowledge of the anatomy and physiology of the brain to inspire an appropriate program, as well as much more storage and speed than was available to the Ferranti Mark I at the time (see Fig. 2). Another important element in the excerpt is Turing\u2019s point that, even if a thinking machine is possible, the relation he has in mind is not one of identity but one of analogy: \u201cIt\u2019ll just be another species of the thinking genus.\u201d An original answer to problem P1subscript\ud835\udc431P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, why design a test based on imitation, which can be seen as encouraging deception, is as follows. Actually, imitation was Turing\u2019s fundamental principle of the new science of universal digital computing. He conceived his 1950 paper largely in continuity with his 1936 paper. Both were based on his core concepts of machine and imitation, i.e., what it takes for a machine to imitate another machine. A point of difference is that by 1950 he had generalized the machine architecture and how \u2018universal\u2019 imitation can be achieved. Using Turing\u2019s 1948 language,101010\u201cIntelligent Machinery,\u201d op. cit. universality can be achieved by starting with an \u2018organized\u2019 machine (1936), or with an \u2018unorganized\u2019 machine (1948/1950). Whereas in 1936 the machine would be given an a priori, well-defined and fixed table of instructions for each task, in 1950 it would also be capable of performing a new task by changing its logical structure as a result of learning from experience, much as the brain does, \u201cby changing its neuron circuits by the growth of axons and dendrites.\u201d111111Turing to Ross Ashby, circa November 19, 1946. British Library, Collection \u2018W. Ross Ashby: correspondence of W. Ross Ashby\u2019, Add MS 89153/26. The kind of forgery and trickery that occurs in commercial and publicity-based practical \u201cTuring\u201d tests has nothing to do with Turing\u2019s 1950 proposal. In 1951, he warned: \u201cIt would be quite easy to arrange the [machine\u2019s] experiences in such a way that they automatically caused the structure of the machine to build up into a previously intended form, and this would obviously be a gross form of cheating, almost on a par with having a man inside the machine.\u201d121212\u2018Intelligent machinery, a heretical theory\u2019, a lecture given to \u201851 Society\u2019 at Manchester, c. 1951. Archives Centre, King\u2019s College, Cambridge, AMT/B/4. The \u201chuman fallibility\u201d that Turing encouraged the machine to show was meant to be a by-product of learning from experience. This 1950 passage clarifies: \u201cAnother important result of preparing our machine for its part in the imitation game by a process of teaching and learning is that \u2018human fallibility\u2019 is likely to be [mimicked] in a rather natural way, i.e., without special \u2018coaching\u2019 \u201d (Turing, 1950, p. 459). That is, for a machine to be a valid player of Turing\u2019s test, it cannot be specially prepared for it. This means that we have never seen a practical Turing test. The various rhetorical questions Turing posed, Q\u2032\u2026Q\u2032\u2032\u2032\u2032superscript\ud835\udc44\u2032\u2026superscript\ud835\udc44\u2032\u2032\u2032\u2032Q^{\\prime}\\ldots Q^{\\prime\\prime\\prime\\prime}italic_Q start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2026 italic_Q start_POSTSUPERSCRIPT \u2032 \u2032 \u2032 \u2032 end_POSTSUPERSCRIPT, to replace the original question, Q\ud835\udc44Qitalic_Q, can be generalized as follows (Gon\u00e7alves, 2023e): Question Q\u22c6superscript\ud835\udc44\u22c6Q^{\\star}italic_Q start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT: could player A imitate intellectual stereotypes associated with player B\u2019s type successfully (well enough to deceive player C), despite A and B\u2019s physical differences? It has been largely unnoticed that the various questions instantiating Q\u22c6superscript\ud835\udc44\u22c6Q^{\\star}italic_Q start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT follow a case-control methodology, applied in two stages. At the more obvious intra-game level, A plays the case, and B plays the control. However, at the inter-game level, two variants set the case (machine-woman and the machine-man) and the other two set the control (man-woman and the machine-machine). While the first two are open, creating suspense around the test, the latter two are resolved as follows. It is known that a man (A) can possibly imitate gender stereotypes associated with a woman (B) to successfully deceive an interrogator (C) despite their physical differences. This is the very premise of the parlor games that existed at the time. Further, regarding the machine-machine variant, it is also known that a digital computer (A), because of its universality property, as Turing explained in the paper (Turing, 1950, \u00a7\u00a74, 5), can successfully imitate any discrete-state machine (B), despite their physical differences. We can now explore how Turing\u2019s presentation of his test conforms to Ernst Mach\u2019s conception of \u201cthe basic method of thought experiments,\u201d which is variation, continuously if possible. Mach is the author of perhaps the most classic text on thought experiments in the modern scientific tradition (Mach, 1897), in which he developed keen observations and insights based on countless examples from the history of modern physics, mathematics, and common sense experience. He wrote: \u201cBy varying the conditions (continuously if possible), the scope of ideas (expectations) tied to them is extended: by modifying and specializing the conditions we modify and specialize the ideas, making them more determinate, and the two processes alternate\u201d (p. 139). Mach illustrated his point with the process of discovery of universal gravitation (Mach, 1897, pp. 138-139): A stone falls to the ground. Increase the stone\u2019s distance from the earth, and it would go against the grain to expect that this continuous increase would lead to some discontinuity. Even at lunar distance the stone will not suddenly lose its tendency to fall. Moreover, big stones fall like small ones: the moon tends to fall to the earth. Our ideas would lose the requisite determination if one body were attracted to the other but not the reverse, thus the attraction is mutual and remains so with unequal bodies, for the cases merge into one another continuously \u2026\u2026\\ldots\u2026 discontinuities are quite conceivable, but it is highly improbable that their existence would not have betrayed itself by some experience. Besides, we prefer the point of view that causes less mental exertion, so long as it is compatible with experience. The conditions, i.e., the distance of the fall and the size of the stones, are continuously varied in the physicist\u2019s mind and eventually stretched to the celestial scale. Reciprocally, the concept of a celestial body, such as the Earth or the Moon, becomes interchangeable with the concept of a stone, and quite unequal stones can then become mutually attracted. The cases continuously merge into one another, and a conceptual integration is established that connects near-earth bodies to celestial bodies under a unified concept. Turing\u2019s imitation game extended the scope of ideas and expectations established earlier in his 1936 paper, moving from machine-machine and restricted human-machine imitation in 1936151515\u201cWe may compare a man in the process of computing a real number to a machine which is only capable of a finite number of conditions\u201d (Turing, 1936, p. 231). to more general human-machine imitation in 1950. To understand this better, let us take a brief look at Turing\u2019s 1948 report \u201cIntelligent Machinery\u201d (op. cit.). In section (\u00a73) \u2018Varieties of machinery,\u2019 he noted: \u201cAll machinery can be regarded as continuous, but when it is possible to regard it as discrete it is usually best to do so.\u201d A brain, he wrote, \u201cis probably\u201d a \u2018continuous controlling\u2019 machine, but in light of the digital nature of neural impulses, it \u201cis very similar to much discrete machinery.\u201d In section (\u00a76) \u2018Man as Machine,\u2019 he referred to the imitation of \u201cany small part of a man\u201d by machines: \u201cA great positive reason for believing in the possibility of making thinking machinery is the fact that it is possible to make machinery to imitate any small part of a man\u201d (p. 420). In light of this, he argued: \u201cOne way of setting about our task of building a \u2018thinking machine\u2019 would be to take a man as a whole and to try to replace all the parts of him by machinery.\u201d But Turing dismissed such a method as \u201caltogether too slow and impracticable,\u201d and later alluded to moral and aesthetic reasons as well.161616For the May 1951 broadcast (op. cit.), he wrote: \u201cI certainly hope and believe that no great efforts will be put into making machines with the most distinctively human, but non-intellectual characteristics such as the shape of the human body; it appears to me to be quite futile to make such attempts and their results would have something like the unpleasant quality of artificial flowers.\u201d We can now follow Turing\u2019s use of the method of continuous variation in the design of his imitation tests. The essential question (Q\u22c6superscript\ud835\udc44\u22c6Q^{\\star}italic_Q start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT) Turing asks is whether the intellectual and cultural performances (the stereotypes)171717S. Sterrett first emphasized the importance of stereotypes in the imitation game (Sterrett, 2000). associated with woman, man, machine (the types) could be imitated, and thus softly transposed. Note that for any arbitrarily chosen type, say, a woman, further specific subtypes can be continuously conceived and considered as varied conditions of the imitation game: women having property p\ud835\udc5dpitalic_p, women having subproperty p\u2032\u2282psuperscript\ud835\udc5d\u2032\ud835\udc5dp^{\\prime}\\subset pitalic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2282 italic_p, and so on. For any two arbitrarily chosen types, a new type can be conceived, whether as a specialization or a modification (\u201cany small part of a man\u201d). Because concepts are fluid entities, there is an evolving continuum of levels and types. The question across the various versions of the game can be posed this way: how does C\u2019s perception of A\u2019s performance against B\u2019s performance change as the game\u2019s conditions are (continuously) varied? Will it change if gendered verbal behavior is required as a subtype of human verbal behavior? Will it change if the machine\u2019s hardware is increased and/or its learning program is modified? For Turing, there is no conceptual discontinuity among the various conditions that instantiate his thought experiment. From 1948 to 1952, Turing presented various imitation tests based on both the game of chess and conversation. A historically sound problem, because it does not struggle with the materiality of Turing\u2019s texts and their chronological coherence, does not erase some of his tests in favor of others, does not ignore the historical conditions of his proposal, is (P2subscript\ud835\udc432P_{2}italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) Why would Turing present multiple versions of his test, as opposed to a well-defined, controlled experiment? I have presented an answer by reconstructing Turing\u2019s use of the method of thought experiments, and this will be reinforced in the next section. Turing\u2019s purpose in his 1950 paper was not to propose a \u2018benchmark\u2019 for the non-existent field of AI, but to respond to critics. Especially in 1949, he felt compelled to point out that the new science of universal digital computing would eventually have an impact and expand our view of \u2018thinking.\u2019 As is often the case with thought experiments, Turing proposed his test out of a controversy (Gon\u00e7alves, 2023d). He was coming from his continuing disputes with the physicist and computer pioneer, Fellow of the Royal Society (FRS), Douglas Hartree (1897-1958), over the meaning of the newly existing digital computers, which had started in 1946 (Gon\u00e7alves, 2023c). Now, in mid-1949, new opponents had arrived, notably the neurosurgeon Geoffrey Jefferson (1886-1961), and the chemist and philosopher Michael Polanyi (1891-1976), both also FRS and based at the same institution as Turing, the University of Manchester, where Turing had spent a year as a Reader in the Department of Mathematics (Hodges, 1983). These three thinkers challenged Turing\u2019s claims about the future possibilities and limitations of digital computers. In June 1949, Hartree published his Calculating Instruments and Machines (Hartree, 1949), in which Ada Lovelace\u2019s work was acknowledged seemingly for the first time by a twentieth-century computer pioneer (Gon\u00e7alves, 2023c). Since November 1946, Hartree had been opposing the use of the term \u2018electronic brain.\u2019 He wrote in a letter to the Times: \u201cThese machines can only do precisely what they are instructed to do by the operators who set them up.\u201d181818\u201cThe \u2018Electronic Brain\u2019: A Misleading Term; No Substitute for Thought,\u201d Times, November 7, 1946. Now in 1949, Hartree added strength to his argument by quoting the words of Ada Lovelace from the 1840s about Charles Babbage\u2019s machine: \u201cThe Analytical Engine has no pretensions to originate anything \u2026\u2026\\ldots\u2026 It can do whatever we know how to order it to perform\u201d (her emphasis) (Hartree, 1949, p. 70). Noting Hartree\u2019s anachronism in taking Lovelace\u2019s words out of their time and place, Turing further developed his earlier, 1947 response to Hartree\u2019s challenge,191919\u2018Lecture to L.M.S. Feb. 20 1947\u2019, op. cit., p. 22. now calling it \u2018(6) Lady Lovelace\u2019s objection\u2019 (Turing, 1950, p. 450). Turing argued that intelligent behavior is the result of learning, a capability he had no problem attributing to future digital computers. He also questioned the implicit assumption of Hartree\u2019s challenge: \u201cWho can be certain that \u2018original work\u2019 that he has done was not simply the growth of the seed planted in him by teaching, or the effect of following well-known general principles\u201d (p. 450). In the imitation game, Turing suggested, the interrogator would be able to evaluate the machine\u2019s ability to learn: \u201cThe game (with the player B omitted) is frequently used in practice under the name of viva voce to discover whether some one really understands something or has \u2018learnt it parrot fashion\u2019 \u201d (p. 446). But then we might ask, what is player B doing in the imitation game? Following the 1949 events will suggest an answer. On June 9, in London, Jefferson delivered his prestigious Lister Oration on \u2018The Mind of Mechanical Man,\u2019 which was published in the debuting British Medical Journal on June 25 (Jefferson, 1949). His lecture was headlined in the Times on June 10,202020\u2018No Mind For Mechanical Man.\u2019 Times, 10 June 1949, p. 2. emphasizing his claim that \u201cNot until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain\u201d (p. 1110). This rendered Turing\u2019s famous response: \u201cI do not think you can even draw the line about sonnets, though the comparison is perhaps a little bit unfair because a sonnet written by a machine will be better appreciated by another machine.\u201d212121\u2018The Mechanical Brain.\u2019 Times, 11 June 1949, p. 4. In October and December 1949, two seminars on \u2018Mind and Machine\u2019 were organized by Polanyi et al., and attended by Jefferson, Turing et al., at the Philosophy Department in Manchester (Polanyi, 1958, p. 275; cf. also note 1 above). These seminar discussions, followed by Jefferson giving Turing an offprint of his Lister Oration,222222This may have happened in the evening of the December meeting of the Manchester seminar (op. cit.), when, according to a later letter from Jefferson to Ethel S. Turing, Turing and J.Z. Young went to dinner at Jefferson\u2019s house (Turing, 1959, p. xx). which Turing read and marked with a pencil,232323Off-print, \u2018The mind of mechanical man\u2019 by Geoffrey Jefferson. Archives Centre, King\u2019s College, Cambridge, AMT/B/44. led him to write his 1950 paper and propose his test (Gon\u00e7alves, 2023d). In his Lister Oration (Jefferson, 1949), Jefferson had characterized intelligence as an emergent property of the animal nervous system. He emphasized that \u201csex hormones introduce peculiarities of behaviour often as inexplicable as they are impressive\u201d (p. 1107). Because \u201cmodern automata\u201d are not moved by male and female sex hormones, they could not exhibit such peculiarities to imitate the actions of animals or \u2018men.\u2019 Specifically, he used a thought experiment to criticize Grey Walter\u2019s mechanical turtles by suggesting that gendered behavior is causally related to the physiology of sex hormones (ibid.): [\u2026It] should be possible to construct a simple animal such as a tortoise (as Grey Walter ingeniously proposed) that would show by its movements that it disliked bright lights, cold, and damp, and be apparently frightened by loud noises, moving towards or away from such stimuli as its receptors were capable of responding to. In a favourable situation the behaviour of such a toy could appear to be very lifelike \u2014 so much so that a good demonstrator might cause the credulous to exclaim \u2018This is indeed a tortoise.\u2019 I imagine, however, that another tortoise would quickly find it a puzzling companion and a disappointing mate. Jefferson thus brought forward the image of a genuine individual of a kind, which is placed side by side with the artificial one so that the latter\u2019s artificiality is emphasized. The function of the genuine individual is to expose the artificiality of the impostor. The means of exposure is to fail at demonstrating interesting (sexual) behavior. This can explain Turing\u2019s introduction of a (gendered) control player B, who appears in Turing\u2019s 1950 test, whose design was prompted by his reading of Jefferson, but not in Turing\u2019s 1948, 1951, and 1952 tests. In discussing \u201c(4) The Argument from Consciousness,\u201d Turing addressed Jefferson directly and quoted in full his conditions for agreeing \u201cthat machine equals brain,\u201d including \u201cbe warmed by flattery\u201d and \u201cbe charmed by sex\u201d (Turing, 1950, pp. 445-446). In discussing the \u201c(5) Argument from Various Disabilities,\u201d Turing again mentioned Jefferson and argued that to say that a machine could never \u201cfall in love\u201d or \u201cmake someone fall in love with it\u201d was a flawed scientific induction from the capabilities of present machines. Thus, an answer to the first part of problem P3subscript\ud835\udc433P_{3}italic_P start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, why gender imitation in a test of machine intelligence, is that Turing\u2019s design of his test was an ironic response to Jefferson\u2019s association of sex and gender with intelligence, particularly his suggestion that gendered behavior is causally related to the physiology of male and female sex hormones. It remains to address the second part of problem P3subscript\ud835\udc433P_{3}italic_P start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, why Turing chose conversation as the intelligence task of his test. Surviving minutes of the \u2018Mind and Machine\u2019 seminar held on October 27, 1949, were published in 2000 by a participant, Wolfe Mays.242424W. Mays, \u2018Turing and Polanyi on minds and machines.\u2019 Appraisal, 3(2), 55-62. Andrew Hodges also published it on his website: https://www.turing.org.uk/sources/wmays1.html. Accessed December 6, 2023. In the first session, Polanyi presented a statement, \u2018Can the mind be represented by a machine?,\u2019252525Polanyi, Michael. Papers, Box 22, Folder 19, Hanna Holborn Gray Special Collections Research Center, University of Chicago Library. which was a G\u00f6delian argument that humans can do things that machines cannot. Although Turing had already addressed this argument in his 1947 lecture (op. cit.), Polanyi\u2019s insistence may help explain Turing\u2019s inclusion of \u201c(3) The Mathematical Objection\u201d (Turing, 1950). Further, the minutes (op. cit.) show that Polanyi tried to distinguish the formal \u201crules of the logical system\u201d from the informal \u201crules which determine our own behaviour,\u201d and this helps explain Turing\u2019s inclusion of \u201c(8) The Argument from Informality of Behaviour\u201d (Turing, 1950). Years later (Polanyi, 1958, p. 275), Polanyi remembered \u201ca communication to a Symposium held on \u2018Mind and Machine\u2019 at Manchester University in October, 1949,\u201d in which \u201cA.M. Turing has shown that it is possible to devise a machine which will both construct and assert as new axioms an indefinite sequence of G\u00f6delian sentences.\u201d262626Polanyi added that \u201cthis is foreshadowed\u201d in Turing\u2019s 1938 paper based on his Ph.D. thesis, \u2018Systems of Logic Based on Ordinals,\u2019 J. London Math. Soc. s2-45(1), 161-228. Polanyi resumed, showing that he assimilated the punch: \u201cAny heuristic process of a routine character\u2014for which in the deductive sciences the G\u00f6delian process is an example\u2014could likewise be carried out automatically.\u201d However, Polanyi used the same argument to dismiss the game of chess as a testbed for machine intelligence, noting: \u201cA routine game of chess can be played automatically by a machine, and indeed, all arts can be performed automatically to the extent to which the rules of the art can be specified.\u201d Chess, not conversation, had been Turing\u2019s chosen field to illustrate, develop, and test machine intelligence since at least February 1946.272727\u2018Proposed electronic calculator,\u2019 February 1946. Archives Centre, King\u2019s College, Cambridge, AMT/C/32. On p. 16, Turing asks: \u201cCan the machine play chess?\u201d In his 1948 \u2018Intelligent Machinery\u2019 report (op. cit.), Turing had discussed a tradeoff between convenient and impressive intellectual fields for exploring machine intelligence. After discussing \u201cvarious games e.g. chess,\u201d Turing wrote: \u201cOf the above possible fields the learning of languages would be the most impressive, since it is the most human of these activities.\u201d However, he avoided language learning because it seemed \u201cto depend rather too much on sense organs and locomotion to be feasible,\u201d stuck with chess, and ended up describing a chess-based imitation game. Now in October 1949, he saw chess being dismissed as an unimpressive to make the case for machine intelligence because its rules could be specified. Some time later, probably around Christmas 1949, Turing will read Jefferson\u2019s Lister Oration (Jefferson, 1949) and mark the passage quoting Ren\u00e9 Descartes (p. 1106), which starts: \u201cDescartes made the point, and a basic one it is, that a parrot repeated only what it had been taught and only a fragment of that; it never used words to express its own thoughts.\u201d Overall, Jefferson suggested \u2018speech\u2019 to be the distinguishing feature of human intelligence compared to other kinds of animal intelligence: \u201cGranted that much that goes on in our heads is wordless \u2026\u2026\\ldots\u2026 we certainly require words for conceptual thinking as well as for expression \u2026\u2026\\ldots\u2026 It is here that there is the sudden and mysterious leap from the highest animal to man, and it is in the speech areas of the dominant hemisphere \u2026\u2026\\ldots\u2026 that Descartes should have put the soul, the highest intellectual faculties\u201d (p. 1109). Unlike chess, which is governed by definite rules, good performance in conversation cannot be easily specified. Turing\u2019s 1950 choice for \u2018the learning of languages\u2019 as the intellectual field addressed in his test can be best understood as yet another concession to Jefferson and, in this case, to Polanyi as well. In summary, Turing varied the design of his imitation tests to respond to the challenges posed by Hartree, Polanyi, and Jefferson. This fits neatly into Popper\u2019s methodological rule for \u201cthe use of imaginary experiments in critical argumentation\u201d (Popper, 1959): \u201cthe idealizations made must be concessions to the opponent, or at least acceptable to the opponent\u201d (p. 466, no emphasis added). On August 2, 1971, more than three centuries after Galileo\u2019s death, a live anecdotal demonstration of Galileo\u2019s legendary tower experiment was performed for the television cameras by astronaut David Scott during the final Apollo 15 moonwalk (Fig. 3). Far from the Earth\u2019s atmosphere, essentially in a vacuum, the astronaut simultaneously released a heavy object (an aluminum geological hammer) and a light object (a falcon feather) from approximately the same height, which fell to the ground at the same rate to the naked eye. The performer, who attributed their successful mission in part to \u201ca rather significant discovery about falling objects in gravity fields\u201d made long ago by \u201ca gentleman named Galileo,\u201d celebrated: \u201cHow about that! Mr Galileo was correct in his findings.\u201d282828For footage and a technical description of the demonstration, see \u00a1http://nssdc.gsfc.nasa.gov/planetary/lunar/apollo_15_feather_drop.html\u00bf. Accessed December 6, 2023. Variants of Galileo\u2019s falling-bodies experiment first appeared in his De motu drafts written in the 1590s. Decades later came the punchy presentation of his 1638 Two New Sciences (Galilei, 1638, pp. 66-67): SALVIATI: But without experiences, by a short and conclusive demonstration, we can prove clearly that it is not true that a heavier moveable is moved more swiftly than another, less heavy, these being of the same material, and in a word, those of which Aristotle speaks. Tell me, Simplicio, whether you assume that for every heavy falling body there is a speed determined by nature such that this cannot be increased or diminished except by using force or opposing some impediment to it \u2026\u2026\\ldots\u2026 [SIMPLICIO agrees] Then if we had two moveables whose natural speeds were unequal, it is evident that were we to connect the slower to the faster, the latter would be partly retarded by the slower, and this would be partly speeded up by the faster \u2026\u2026\\ldots\u2026 From the outset, Galileo makes Simplicio accept Salviati\u2019s carefully formulated assumption that for every heavy falling body there is a natural speed that cannot be altered except by external intervention. However, the Aristotelian could have found a way out by noting the imprecision and denying that weight and natural speed are physically determinate for connected but not unified bodies (Gendler, 1998). Could Galileo have run a variant of this experiment from the Leaning Tower of Pisa and obtained the results claimed in the story? Physicists went to the laboratory and concluded that it is doubtful (Adler and Coulter, 1978). Decades earlier, it was found (Cooper, 1935) that the only source for the legendary story was actually an apologetic biography written by Galileo\u2019s disciple Vincenzo Viviani 12 years after Galileo\u2019s death and first published in 1717. For centuries, Galileo was largely considered \u201cthe first true empiricist,\u201d but the role of experiment in Galilean science was more complex than previously thought (Segre, 1989; Palmieri, 2005). Galileo\u2019s falling-bodies experiment suggests an anomaly in Aristotle\u2019s theory of motion under certain idealized conditions, the existence of motion in a void, which was unacceptable at the time. Testing such an existential hypothesis was in infinite regress with the conditions it required, and creating those conditions would require long-term scientific and technological progress. The Galilean impasse could only be broken by the power of his thought experiments as propaganda292929\u2018Propaganda\u2019 is meant here in its pre-Nazi, neutral sense of propagating, spreading. for a next generation of scientists (Gon\u00e7alves, 2023a). Robert Boyle (1627-1692) was one of them. As a teenager, he visited Florence shortly before Galileo\u2019s death and was impressed by \u201cthe new paradoxes of the great star-gazer Galileo\u201d (Fulton, 1960, p. 119). In his career, Boyle built air pumps and special chambers to study vertical fall in small evacuated environments, and became an exponent of experimental philosophy in the Royal Society. There is no record of Boyle interpreting Galilean science literally and performing any tower experiments. The value of Galileo\u2019s thought experiments was to lay conceptual foundations and to conjecture a class of idealized phenomena to be pursued by progressive science and technology. There is a path that connects Galileo\u2019s thought experiments and Boyle\u2019s vacuum chambers to the space programs of the 1950s and finally to the anecdotal confirmation of Galileo\u2019s hypothesis by the crew of the Apollo 15 mission to the moon. Now note the analogy with the Turing test (Gon\u00e7alves, 2023a). As Hayes and Ford claimed: \u201cThe tests are circular: they define the qualities they are claiming to be evidence for\u201d (p. 974). Turing\u2019s existential hypothesis of a Turing test-passing machine is in infinite regress with the conditions assumed by the test: an idealized computer equipped with a hypothetically appropriate program (Box 1). But if these conditions exist, why do we need a Turing test at all? This shows that practical Turing tests can serve at best as anecdotal confirmation of Turing\u2019s hypothesis, and at worst, as we have seen for decades, as publicity stunts. The value of Turing\u2019s test must lie elsewhere. By May 1953, John McCarthy and Claude Shannon were working on their collection Automata Studies (McCarthy and Shannon, 1956), which revolved around \u201cthe theory of Turing machines\u201d (p. vii), and to which they invited Turing to contribute.303030Shannon and McCarthy to Turing, May 18, 1953. Alan Turing Papers (Additional), University of Manchester Library, GB133 TUR/Add/123. Turing declined the invitation, saying that he had been working for the last two years on \u201cthe mathematics of morphogenesis,\u201d although he expected \u201cto get back to cybernetics very shortly.\u201d313131Turing to Shannon, June 3, 1953 (ibid.). One year and four days later, Turing was dead, and early AI would not note his biological turn. Commenting on \u201cthe Turing definition of thinking\u201d (p. vi), McCarthy and Shannon found it \u201cinteresting\u201d because it \u201chas the advantages of being operational or, in the psychologists\u2019 term, behavioristic \u2026\u2026\\ldots\u2026 No metaphysical notions of consciousness, ego and the like are involved.\u201d They also thought that this very strength could be a weakness, because it has \u201cthe disadvantage\u201d of being susceptible to a memorizing machine playing the imitation game by looking up \u201ca suitable dictionary.\u201d McCarthy and Shannon referred interchangeably to \u2018definition\u2019 and to a word that Turing actually used, \u2018criterion:\u2019 \u201cWhile certainly no machines at the present time can even make a start at satisfying this rather strong criterion, Turing has speculated that within a few decades it will be possible to program general purpose computers in such a way as to satisfy this test\u201d (McCarthy and Shannon, 1956, p. v, emphasis added). In 1955, before the publication of Automata Studies, McCarthy and Shannon, together with Marvin Minsky and Nathaniel Rochester, co-authored their well-known \u2018Proposal\u2019 for AI research (McCarthy et al., 1955). Unlike Turing himself, they seem to have thought of machine intelligence in terms of Turing machines, as their opening paragraph suggests: \u201cThe study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\u201d Whether or not they followed Turing on the \u2018how,\u2019 they certainly followed him on the \u2018what,\u2019 in writing: \u201cFor the present purpose the artificial intelligence problem is taken to be that of making a machine behave in ways that would be called intelligent if a human were so behaving\u201d (p. 7). This definition \u2014 compare it with \u201cthe Turing definition of thinking\u201d \u2014 would stay. In the early 1960s, Edward Feigenbaum and Julian Feldman noted in Computers and Thought (Feigenbaum and Feldman, 1963) that Turing\u2019s 1950 paper \u201cappeared five years before concrete developments in intelligent behavior by machine began to occur;\u201d and \u201cyet,\u201d they continued, \u201cit remains today one of the most cogent and thorough discussions in the literature on the general question \u201cCan a machine think?\u201d (pp. 9-10). They observed Turing\u2019s \u201cbehavioristic posture relative to the question,\u201d which \u201cis to be decided by an unprejudiced comparison of the alleged \u2018thinking behavior\u2019 of the machine with normal \u2018thinking behavior\u2019 in human beings\u201d (emphasis added). They concluded: \u201cHe proposes an experiment \u2014 commonly called \u2018Turing\u2019s test\u2019 \u2014 in which the unprejudiced comparison could be made \u2026\u2026\\ldots\u2026 Though the test has flaws, it is the best that has been proposed to date.\u201d Minsky, in the preface to his 1967 collection (Minsky, 1968), reiterates the definition of AI as \u201cthe science of making machines do things that would require intelligence if done by men\u201d (p. v). Around the same time, Minsky collaborated with Stanley Kubrick and Arthur Clarke on their 1968 screenplay, also written as a novel, 2001: A Space Odyssey (Clarke, 1968), which featured a futuristic computer named HAL: Whether HAL could actually think was a question which had been settled by the British mathematician Alan Turing back in the 1940s. Turing had pointed out that, if one could carry out a prolonged conversation with a machine \u2014 whether by typewriter or microphone was immaterial \u2014 without being able to distinguish between its replies and those that a man might give, then the machine was thinking, by any sensible definition of the word. HAL could pass the Turing test with ease. The \u201cTuring definition of thinking\u201d was to become legendary. Stuart Shieber studied McCarthy and Shannon\u2019s memorizing machine objection in depth, elaborated on its assumptions, and concluded that it is invalid (Shieber, 2014). But McCarthy\u2019s concept of \u2018memorizing\u2019 may have been more elastic, as his later comment on Deep Blue\u2019s defeat of Gary Kasparov seems to suggest (McCarthy, 1997). He expressed disappointment that it was mostly an achievement of computational power rather than thinking, and gave a clear argument why he thought so. Essentially, McCarthy pointed out, computer chess advanced by replacing heuristic techniques, which relied on the expertise of human players to prune the search space of possible moves, with brute force computing. \u201c[I]t is a measure of our limited understanding of the principles of artificial intelligence,\u201d McCarthy wrote, \u201cthat this level of play requires many millions of times as much computing as a human chess player does.\u201d It may be, but that the problem was \u201clargely a quantitative matter\u201d was hinted at by Turing in his letter to Worsley of c. June 1951 (Box 3). Ten years after Deep Blue vs. Kasparov, McCarthy referred to Turing\u2019s 1947 lecture (op. cit.) as \u201cthe first scientific discussion of human level machine intelligence,\u201d and to Turing\u2019s 1950 paper as \u201camplifying\u201d that discussion into a \u201cgoal\u201d (McCarthy, 2007, p. 1174). In 1992, Minsky co-authored a work of fiction, The Turing Option (Warner, New York), in which Turing\u2019s test is featured in the preface. In 1995, Minsky took a stand against Loebner\u2019s Weinzenbaum experiments, pleading to \u201crevoke his stupid prize, save himself some money, and spare us the horror of this obnoxious and unproductive annual publicity campaign.\u201d323232\u2018Annual Minsky Loebner Prize Revocation Prize 1995 Announcement,\u2019 2 March 1995. Available at: https://groups.google.com/g/comp.ai/c/dZtU8vDD_bk/m/QYaYB18qAToJ. Accessed 25 Nov 2023. In 2013, when asked about the Turing test in a taped interview, Minsky said: \u201cThe Turing test is a joke, sort of, about saying \u2018A machine would be intelligent if it does things that an observer would say must be being done by a human\u2019 \u2026\u2026\\ldots\u2026 it was suggested by Alan Turing as one way to evaluate a machine but he had never intended it as being the way to decide whether a machine was really intelligent.\u201d333333\u2018Marvin Minsky on AI: the Turing test is a joke!\u2019, from 23\u2019 35\u201d to 24\u201945\u201d. Available at https://www.singularityweblog.com/marvin-minsky/. Accessed Dec. 6, 2023. This materially connects McCarthy et al.\u2019s definition of \u201cthe AI problem\u201d with Turing\u2019s test, if material evidence were still needed. Overall, it seems that all of these AI pioneers understood and were inspired by Turing\u2019s test at the level of conceptual foundations. Even if some of them also used the term \u2018experiment,\u2019 none of them took it literally as a practical experiment, which would indeed imply an astonishing lack of imagination on their part. Turing\u2019s test moved the burgeoning field of AI away from unproductive debates about the meaning of words, for example, allowing Minsky to write in 1967 (Minsky, 1967): \u201cTuring discusses some of these issues in his brilliant article, \u2018Computing Machines and Intelligence\u201d [sic], and I will not recapitulate his arguments \u2026\u2026\\ldots\u2026 They amount, in my view, to a satisfactory refutation of many such objections\u201d (p. 107). The value of Turing\u2019s test (P5subscript\ud835\udc435P_{5}italic_P start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT) is that it has long been and still is a unifying \u2018definition,\u2019 a \u2018criterion,\u2019 a \u2018goal\u2019 for, in the words of McCarthy et al., the science and engineering of \u201cmaking a machine behave in ways that would be called intelligent if a human were so behaving.\u201d Every time AI succeeds in automating a new task that was once reserved for humans because it requires intelligence, \u201cthe Turing definition\u201d conquers new territory, and the significance of Turing\u2019s early message to his contemporaries becomes clearer. In this paper, I presented a mass of evidence, including newly discovered archival sources, and a new perspective on Turing\u2019s test. New light was shed on Turing\u2019s concept of imitation, emphasizing that it does not give a license for deception in AI. Rather, it is a mathematical concept, largely in continuity with his 1936 paper. I also showed that Turing\u2019s presentation of the various versions of his test fits what Mach characterized as \u201cthe basic method of thought experiments\u201d in science. I reconstructed the historical conditions of Turing\u2019s proposal, explaining that gender imitation was his ironic response to Jefferson, and conversation was yet another concession to his opponents. Further, I paralleled Turing\u2019s test with Galileo\u2019s falling-bodies experiment, showing that the problem of circularity is inherent in existential hypotheses, and that the solution may lie in propaganda and the progressive scientific and technological developments of a next generation of scientists. I then revisited the history of AI, showing that Turing\u2019s test provided McCarthy, Minsky, and others with a definition of the AI problem that, at the level of conceptual foundations, still drives AI research today. But whatever its utility, Turing\u2019s test has secured its place as one of the most beautiful thought experiments in the history of science."
    },
    {
        "id": 6,
        "title": "Professional Network Matters:\nConnections Empower Person-Job Fit",
        "corpus": "Online recruitment platforms typically employ Person-Job Fit models in the core service that automatically match suitable job seekers with appropriate job positions. While existing works leverage historical or contextual information, they often disregard a crucial aspect: job seekers\u2019 social relationships in professional networks. This paper emphasizes the importance of incorporating professional networks into the Person-Job Fit model. Our innovative approach consists of two stages: (1) defining a Workplace Heterogeneous Information Network (WHIN) to capture heterogeneous knowledge, including professional connections and pre-training representations of various entities using a heterogeneous graph neural network; (2) designing a Contextual Social Attention Graph Neural Network (CSAGNN) that supplements users\u2019 missing information with professional connections\u2019 contextual information. We introduce a job-specific attention mechanism in CSAGNN to handle noisy professional networks, leveraging pre-trained entity representations from WHIN. We demonstrate the effectiveness of our approach through experimental evaluations conducted across three real-world recruitment datasets from LinkedIn, showing superior performance compared to baseline models. With the rapid development of the Internet, online recruitment platforms (e.g., LinkedIn111https://www.linkedin.com, Indeed222https://www.indeed.com, and ZipRecruiter333https://www.ziprecruiter.com) are becoming essential for recruiting and job seeking. A considerable number of talent profiles and job descriptions are posted on these platforms. Taking LinkedIn as an example, more than 900 million members have registered, and 90 jobs were posted every second by the first quarter of 2023444https://news.linkedin.com/about-us#Statistics. Considering such a large number of options, Person-Job Fit (PJF) (Malinowski et al., 2006) has become a critical research topic for improving the efficiency of recruitment and job seeking. Person-Job Fit aims to automatically link the right talents to the right job positions according to talent competencies and job requirements. Previous works on Person-Job Fit mainly focus on leveraging two types of information, namely, (1) historical job application information and (2) textual information in profiles and job descriptions. Collaborative filtering-based methods (Shalaby et al., 2017; Bian et al., 2020) are applied to capture co-apply relations between job seekers and co-applied relations between job positions in the historical application information. Manually-engineered textual features and deep language models have been widely adopted to leverage the textual information (Bian et al., 2019; Qin et al., 2018; Yan et al., 2019; Lu et al., 2013). However, workplace social connections among members, commonly called professional networks, have been overlooked as a pivotal source of information. A survey reported on the Official LinkedIn Blog555https://blog.linkedin.com/2017/june/22/the-best-way-to-land-your-next-job-opportunity showed that out of more than 15,000 LinkedIn members, 80% believed that professional networking could help find new job opportunities, an even more striking 70% gained job opportunities directly through their connections. As an important source for achieving employment relationships, professional networks have great potential for helping with Person-Job Fit. Incorporating professional networks into the models can offer two main benefits: (1) Professional networks directly improve Person-Job Fit by bridging the gap between job seekers and potential job opportunities. A Person-Job Fit model enriched with professional network information can suggest job opportunities to users based on their professional connections. (2) Professional networks can also help complete job seekers\u2019 profiles. Often job seekers\u2019 online profiles lack comprehensive details, yet one\u2019s professional experience and skills can be, to some extent, discernible through their professional connections. While professional networks offer advantages, they often contain a lot of noise. This noise includes connections that aren\u2019t relevant and information from relevant connections that don\u2019t help improve Person-Job Fit. To illustrate, irrelevant connections might involve job seekers\u2019 former classmates or recruiters from unrelated industries. Even when the connections are relevant to the job seeker, they could still have information unrelated to the job seeker\u2019s aspirations for the role. To tackle these noises, a promising strategy involves using heterogeneous knowledge, such as job seekers\u2019 skills, work experience, and educational background, to gauge professional connections\u2019 relevance and extract pertinent information to elevate Person-Job Fit. In this paper, we propose a graph neural network-based framework that utilizes heterogeneous knowledge to integrate professional networks into Person-Job Fit. We address the challenge of social noise in professional networks by designing a job-specific attention mechanism. Initially, we define a Workplace Heterogeneous Information Network (WHIN) that captures heterogeneous knowledge, including professional connections. We employ a heterogeneous graph pre-training technique to learn the representations of various entities in the WHIN. Subsequently, we introduce the Contextual Social Attention Graph Neural Network (CSAGNN), designed to supplement users\u2019 lacking information with contextual information from their professional connections. To tackle the social noise in the workplace social network, we infuse a job-specific attention mechanism into CSAGNN, capitalizing on the pre-trained entity representations from WHIN. The main contributions of this study are as follows: To our knowledge, we are the first to define a heterogeneous information network that incorporates heterogeneous knowledge in the Person-Job Fit scenario. We systematically utilize professional networks in a two-stage approach, WHIN pre-training and CSAGNN, to address the Person-Job Fit task. We present a novel Contextual Social Attention Graph Neural Network (CSAGNN) specifically designed to handle noisy professional networks, effectively mitigating the impact of irrelevant information while focusing on related professional connections and contexts for Person-Job Fit. We evaluate our approach on three real-world datasets across diverse industries. Experimental results show that our model outperforms baseline models. The related work of our study can be grouped into two main categories, namely, Person-Job Fit and Heterogeneous Information Network-based Recommendation. As a core function of the online recruitment platform, Person-Job Fit (Malinowski et al., 2006) has received widespread attention. Mainstream works view Person-Job Fit as a text-matching problem between member profiles and job descriptions to fully use the rich contextual knowledge. For example, JLMIA (Shen et al., 2018) is a latent variable model to model job descriptions and member profiles jointly. PJFNN (Zhu et al., 2018) encodes member profiles and job descriptions by hierarchical CNN. BPJFNN (Qin et al., 2018) leverage BiLSTM to get the semantic representation of each word in member profiles and job descriptions. APJFNN (Qin et al., 2018) automatically weighs abilities mentioned in textual information based on historical recruitment results. A transferable deep global match network (Bian et al., 2019) is proposed to solve the domain adaptation problem in three levels for Person-Job Fit. JRMPM (Yan et al., 2019) proposes a matching network with preference modeled to explore the latent preference given the history of the matching process. Some works take structure knowledge into consideration. For example, MV-CoN (Bian et al., 2020) adopts a co-teaching mechanism to capture semantic and structure knowledge at the same time. DPGNN (Yang et al., 2022) explicitly models the two-way selection preference for PJF using GCN. KGRM (Yao et al., 2022) model members and jobs as two graphs and fuse prior external knowledge, e.g., skill knowledge graph, into the graph representations. Previous studies have made good use of textual information, skill entity, and direct interaction between members and jobs to help with Person-Job Fit. However, these studies often overlook the importance of workplace social relations and the incorporation of diverse, heterogeneous knowledge. Recommender systems have been widely deployed on the Internet to alleviate information overload (Jacoby, 1984). Due to the excellent ability to model complex auxiliary information, Heterogeneous Information Network (i.e., HIN) has become one of the mainstream approaches in recommendation tasks. Depending on the training method, HIN-based models can be divided into two-stage training-based models and end-to-end training-based models (Liu et al., 2022). Two-stage training-based models learn low-dimensional representations of nodes and graph structure using unsupervised tasks and use these representations on various downstream tasks (Du et al., 2022; Bi et al., 2022; Huang et al., 2023; Bi et al., 2023). Inspired by DeepWalk (Perozzi et al., 2014) and node2Vec (Grover and Leskovec, 2016), metapath2vec (Dong et al., 2017) leverages metapath-based random walks to construct a heterogeneous neighborhood of nodes and leverages skip-gram model to generate node representations. Similarly, HIN2Vec (Fu et al., 2017) defines an unsupervised metapath prediction task and jointly learns metapath predictors and node representations. Many models are proposed based on the graph representation methods such as those above. For example, HERec generates node sequences with metapath-based random walks, uses node2vec to learn node representations, and completes recommendations by representation similarity. MAGNN (Fu et al., 2020) leverages node content features and information of intermediate nodes along the metapath by node content transformation, intra- and inter-metapath aggregation. Besides, there are some methods not relying on manual metapath, such as HRLHG (Jiang et al., 2018), NREP (Yang et al., 2019), and ECHCDR (Li et al., 2020b). Compared with two-stage training-based models, end-to-end-based models can use supervision signals directly while training. Thus, they are more customized to specific downstream tasks. For HINs with rich relations, relation-aware graph neural networks can achieve great results. For example, RGCN (Schlichtkrull et al., 2018) assigns a weight matrix to each type of relation, thus extending GCN (Welling and Kipf, 2016) to multi-relation graphs. DisenHAN (Wang et al., 2020) projects nodes into different subspaces using type-specific transformation and uses intra- and inter-aggregation to learn node representations. HAN (Wang et al., 2019) uses a dual attention mechanism to aggregate information from different metapaths. HPN (Ji et al., 2021) designs a semantic fusion mechanism for learning the importance of metapath and fusing them judiciously. By using only its immediate connections as input, HGT (Hu et al., 2020) can learn and extract relevant metapaths for various tasks through its automatic and implicit attention mechanism without requiring manual metapath design. In this paper, we adopt the two-stage training-based paradigm. During the pre-training stage, we integrate heterogeneous knowledge specific to the Person-Job Fit scenario to obtain diverse entity representations that encompass both structural and textual information. In the downstream stage, we design a novel model that leverages the information from the pre-training stage to filter out noise within professional networks. As a point of comparison, we have also selected two end-to-end-based models as baselines. Here we introduce our two-stage approach to improve the performance of the Person-Job Fit task by incorporating professional networks. The task and notation are formally defined in Section 3.1. In the first stage, we pre-train on the Workplace Heterogeneous Information Network (WHIN), which is detailed in Section 3.2. We introduce the pre-training approach on WHIN in Section 3.3. To overcome the noise in professional networks, we propose the contextual social attention graph neural network (CSAGNN) in the second stage, as described in Section 3.4. Person-Job Fit aims to match a set of job seekers with a set of job opportunities. Assume that there is a set of members \u2133={m1,m2,\u2026,mn}\u2133subscript\ud835\udc5a1subscript\ud835\udc5a2\u2026subscript\ud835\udc5a\ud835\udc5b\\mathcal{M}=\\left\\{m_{1},m_{2},\\dots,m_{n}\\right\\}caligraphic_M = { italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_m start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } who may be seeking job opportunities and a set of jobs \ud835\udca5={j1,j2,\u2026,jm}\ud835\udca5subscript\ud835\udc571subscript\ud835\udc572\u2026subscript\ud835\udc57\ud835\udc5a\\mathcal{J}=\\left\\{j_{1},j_{2},\\dots,j_{m}\\right\\}caligraphic_J = { italic_j start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_j start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_j start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }. Formally, given a candidate pair (mj,jk)subscript\ud835\udc5a\ud835\udc57subscript\ud835\udc57\ud835\udc58(m_{j},j_{k})( italic_m start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ), the model is required to learn a function \u2131\u2131\\mathcal{F}caligraphic_F to predict the probability of whether the member applied for the job. The set of candidate pairs can be defined as . In addition to the sets of members (\u2133\u2133\\mathcal{M}caligraphic_M) and jobs (\ud835\udca5\ud835\udca5\\mathcal{J}caligraphic_J), there exist three other sets of entities that are strongly related to members and jobs: skills (\ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S), companies (\ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C), and schools (\u210b\u210b\\mathcal{H}caligraphic_H). These entities are referred as auxiliary information and can be leveraged to improve the model\u2019s accuracy. Given an entity e\ud835\udc52eitalic_e and a relation r\ud835\udc5fritalic_r, an interaction map \ud835\udc9cersuperscriptsubscript\ud835\udc9c\ud835\udc52\ud835\udc5f\\mathcal{A}_{e}^{r}caligraphic_A start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT is used to specify the destination entities related to e\ud835\udc52eitalic_e through r\ud835\udc5fritalic_r. For example, \ud835\udc9cmiapplysuperscriptsubscript\ud835\udc9csubscript\ud835\udc5a\ud835\udc56\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc59\ud835\udc66\\mathcal{A}_{m_{i}}^{apply}caligraphic_A start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_p italic_p italic_l italic_y end_POSTSUPERSCRIPT represents the set of jobs applied by misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. If misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT has not applied for any jobs, \ud835\udc9cmiapply=\u2205superscriptsubscript\ud835\udc9csubscript\ud835\udc5a\ud835\udc56\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc59\ud835\udc66\\mathcal{A}_{m_{i}}^{apply}=\\emptysetcaligraphic_A start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_p italic_p italic_l italic_y end_POSTSUPERSCRIPT = \u2205. The Person-Job Fit scenario contains a wide range of heterogeneous knowledge, including various entities such as skills, companies, and schools, as well as relationships such as professional connections and skill requirements. We define a Workplace Heterogeneous Information Network (WHIN) to model this knowledge. As shown in Figure 1, WHIN is a heterogeneous information network consisting of five types of entities, namely Member, Job, Skill, Company, and School. It also contains nine types of relations, which are introduced to capture the different types of connections between the entities. The Workplace Heterogeneous Information Network primarily consists of natural relations among diverse entities. Besides, two additional metapaths have been artificially constructed. The first metapath, co-apply (\u2133\u2133\\mathcal{M}caligraphic_M-\ud835\udca5\ud835\udca5\\mathcal{J}caligraphic_J-\u2133\u2133\\mathcal{M}caligraphic_M), connects members who have applied for the same job. The second metapath, co-applied (\ud835\udca5\ud835\udca5\\mathcal{J}caligraphic_J-\u2133\u2133\\mathcal{M}caligraphic_M-\ud835\udca5\ud835\udca5\\mathcal{J}caligraphic_J), connects multiple jobs that have been applied for by the same member. These metapaths were constructed based on prior knowledge and have proven to be useful in enhancing the performance of the WHIN pre-training process (Fu et al., 2020). We utilize a heterogeneous graph neural network with a link-level pre-training task to acquire node representations with rich structural knowledge. To improve the scalability of the pre-training process, we create mini-batches by sampling subgraphs from the entire Workplace Heterogeneous Information Network (WHIN), as illustrated in Figure 2 (a) and (b). Specifically, a batch of candidate pairs is selected as the source nodes. From these source nodes, their k-hop neighbors are sampled to construct a subgraph based on each direct relation or metapath. These identified relations and metapaths serve as positive samples. In addition, to introduce negative samples, we randomly select pairs of entities that do not exhibit the expected relationships. The link-level pre-training task revolves around determining the existence of the given samples. We employ an encoder-decoder structure, as illustrated in Figure 2 (c). The encoder learns node representations within each subgraph, while the decoder predicts link existence. During the pre-training phase, we select a fixed number of tokens from the textual content of each entity type. These selected tokens are then processed through the BERT model (Devlin et al., 2019). The resulting BERT outputs are averaged to create initial representations for each entity. In a formal context, when working with a textual representation associated with an entity denoted as ei\u2208\u2130subscript\ud835\udc52\ud835\udc56\u2130e_{i}\\in\\mathcal{E}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_E, we construct a corresponding initial embedding referred to as zi(0)superscriptsubscript\ud835\udc67\ud835\udc560z_{i}^{(0)}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT by utilizing BERT. Specifically, We leverage member profiles, job descriptions, skill names, and other textual descriptions to initialize entity representations, thereby enhancing the initial embeddings for each entity type. After obtaining an initialized representation with semantic information, we utilize an encoder, RGCN (Schlichtkrull et al., 2018), to integrate the information between entities. The message-passing process in the RGCN encoder is represented by the following equation: where Wr(l)superscriptsubscript\ud835\udc4a\ud835\udc5f\ud835\udc59W_{r}^{(l)}italic_W start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT is a parameter matrix at the l\ud835\udc59litalic_l-th layer of relation r\ud835\udc5fritalic_r, W0(l)superscriptsubscript\ud835\udc4a0\ud835\udc59W_{0}^{(l)}italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT is used for self-loop at the l\ud835\udc59litalic_l-th layer and ci,r=|\ud835\udc9ceir|subscript\ud835\udc50\ud835\udc56\ud835\udc5fsuperscriptsubscript\ud835\udc9csubscript\ud835\udc52\ud835\udc56\ud835\udc5fc_{i,r}=\\left|\\mathcal{A}_{e_{i}}^{r}\\right|italic_c start_POSTSUBSCRIPT italic_i , italic_r end_POSTSUBSCRIPT = | caligraphic_A start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT | is a normalization constant. We employ L\ud835\udc3fLitalic_L encoder layers for Link Prediction, utilizing the final representation zi(L)superscriptsubscript\ud835\udc67\ud835\udc56\ud835\udc3fz_{i}^{(L)}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT for entity eisubscript\ud835\udc52\ud835\udc56e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. We apply a multi-layer perceptron (MLP) to score each relation or metapath risubscript\ud835\udc5f\ud835\udc56r_{i}italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, represented by vector Mrisubscript\ud835\udc40subscript\ud835\udc5f\ud835\udc56M_{r_{i}}italic_M start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT. The score for a particular link (s,r,d)\ud835\udc60\ud835\udc5f\ud835\udc51(s,r,d)( italic_s , italic_r , italic_d ) is determined as follows: We employ cross entropy as the loss function for optimizing the pre-training model. Notably, the described pre-training process incorporates professional network information into entity embeddings. As discussed earlier, professional networks are inherently noisy. To capture job-specific insights from the profiles of misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\u2019s professional connections, a multi-head attention (MHA) mechanism (Vaswani et al., 2017) is employed. As shown in Eq. 4, cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the profile representation of member misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT obtained from a pre-trained BERT model. The MHA mechanism uses each required skill embedding zsksubscript\ud835\udc67subscript\ud835\udc60\ud835\udc58z_{s_{k}}italic_z start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT as the query, and cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as both the key and value. It computes attention weights between cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and the required skill embeddings. The weighted sum averages these attention outputs for all required skills. This results in the job-specific contextual feature Fmic(jk)superscriptsubscript\ud835\udc39subscript\ud835\udc5a\ud835\udc56\ud835\udc50subscript\ud835\udc57\ud835\udc58F_{m_{i}}^{c}(j_{k})italic_F start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) that captures relevant information from misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\u2019s connections for job jksubscript\ud835\udc57\ud835\udc58j_{k}italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. As shown in Eq. 5, contextual feature Fmic(jk)superscriptsubscript\ud835\udc39subscript\ud835\udc5a\ud835\udc56\ud835\udc50subscript\ud835\udc57\ud835\udc58F_{m_{i}}^{c}(j_{k})italic_F start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) is concatenated with the WHIN pre-trained feature Fmissuperscriptsubscript\ud835\udc39subscript\ud835\udc5a\ud835\udc56\ud835\udc60F_{m_{i}}^{s}italic_F start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT to form member\u2019s jksubscript\ud835\udc57\ud835\udc58j_{k}italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT-specific initial feature. The relevance degree of professional connections to the job jksubscript\ud835\udc57\ud835\udc58j_{k}italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT needs to be considered in the messaging process. Here, we use the distance between WHIN pre-trained representations of skills mastered by members and the skills required by jobs as the weights during message passing. Given a set of skills S^^\ud835\udc46\\hat{S}over^ start_ARG italic_S end_ARG, average embedding can be described in Eq. 6 and distance between members\u2019 and jobs\u2019 skills dmi(jk)subscript\ud835\udc51subscript\ud835\udc5a\ud835\udc56subscript\ud835\udc57\ud835\udc58d_{m_{i}}(j_{k})italic_d start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) can be described in Eq. 7. Further, due to some members or jobs may have hundreds of related skills, we sampled the skill set like \ud835\udc9cmimastersuperscriptsubscript\ud835\udc9csubscript\ud835\udc5a\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5f\\mathcal{A}_{m_{i}}^{master}caligraphic_A start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m italic_a italic_s italic_t italic_e italic_r end_POSTSUPERSCRIPT and \ud835\udc9cjkrequiresuperscriptsubscript\ud835\udc9csubscript\ud835\udc57\ud835\udc58\ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc56\ud835\udc5f\ud835\udc52\\mathcal{A}_{j_{k}}^{require}caligraphic_A start_POSTSUBSCRIPT italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r italic_e italic_q italic_u italic_i italic_r italic_e end_POSTSUPERSCRIPT in Eq. 4 and Eq. 7. The number of sampled skills, nssubscript\ud835\udc5b\ud835\udc60n_{s}italic_n start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, is a hyperparameter of CSAGNN. We will discuss the influence of nssubscript\ud835\udc5b\ud835\udc60n_{s}italic_n start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT in Section 4.3. As shown in Eq. 8, we can get the messaging function of professional network enhanced GNN by text attention and skill distance attention where \u03b1mj(jk)subscript\ud835\udefcsubscript\ud835\udc5a\ud835\udc57subscript\ud835\udc57\ud835\udc58\\alpha_{m_{j}}(j_{k})italic_\u03b1 start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) is normalized message aggregation weight described in Eq. 9. We use the average of each layer\u2019s representation as the final representation of misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT: Compared with member profiles, job descriptions provided by recruiters often have a high quality. Thus, we directly use contextual features and WHIN pre-trained features as the final representation for all job entities. hjk(mi)subscript\u210esubscript\ud835\udc57\ud835\udc58subscript\ud835\udc5a\ud835\udc56h_{j_{k}}(m_{i})italic_h start_POSTSUBSCRIPT italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and hmi(jk)subscript\u210esubscript\ud835\udc5a\ud835\udc56subscript\ud835\udc57\ud835\udc58h_{m_{i}}(j_{k})italic_h start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) will be concatenated and passed through a vanilla multi-layer Perceptron (MLP) to get the predicted value ymi,jksubscript\ud835\udc66subscript\ud835\udc5a\ud835\udc56subscript\ud835\udc57\ud835\udc58y_{m_{i},j_{k}}italic_y start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT, and the cross entropy loss could be formulated as Eq. 13, where ymi,jk\u2032superscriptsubscript\ud835\udc66subscript\ud835\udc5a\ud835\udc56subscript\ud835\udc57\ud835\udc58\u2032y_{m_{i},j_{k}}^{\\prime}italic_y start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_j start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is the ground truth: In this section, we will validate the performance of the CSAGNN model on three real datasets and answer the following research questions: RQ1: Can CSAGNN outperform existing models on real-world datasets? RQ2: What are the effects of different components in our model? RQ3: Are professional networks helpful for the Person-Job Fit task? RQ4: Does the WHIN pre-training provide meaningful representations of skills for CSAGNN? We validate our model on three large real-world datasets from LinkedIn, a well-known workplace social platform. Datasets are divided by the industry to which the members and jobs belong. The first two datasets contain members and jobs from the technology and finance industries. Due to the singularity of the industry, social noise is limited. The third dataset, which includes members and jobs from various industries such as healthcare, education, and semiconductors, is intended to test the ability of CSAGNN to filter noise in professional networks. This hybrid dataset implies more complex social relationships and a wider range of job opportunities. The WHINs constructed from these three datasets all contain about 200,000 entities and 10 million links. Table 2 displays the statistics of the datasets. The skills, schools, and companies in the dataset all come from explicit annotations by members or jobs on LinkedIn. We cannot provide further detailed information about the datasets to protect the users\u2019 privacy. Each candidate pair within the datasets is labeled as either positive or negative based on the feedback provided by members. We compare our model with the following baseline models. LightGCN (He et al., 2020) is a simplified Graph Convolutional Network model for collaborative filtering with competitive performance and less complexity. PJFNN (Zhu et al., 2018) is a method based on a convolutional neural network (CNN). Hierarchical CNN encodes Resumes and job descriptions independently, and the matching degree is calculated by cosine similarity. BPJFNN (Qin et al., 2018) leverages bidirectional LSTM to learn the representations of resumes and job descriptions. APJFNN (Qin et al., 2018) leverages bidirectional LSTM and hierarchical attention mechanism to learn the representations of resumes and job descriptions. MV-CoN (Bian et al., 2020) combines text matching model and RGCN to learn representations of resumes and job descriptions. HAN (Wang et al., 2019) uses a dual attention mechanism to aggregate neighbor information via different metapaths. HGT (Hu et al., 2020) designs node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge. These baselines can be divided into three groups: (1) context-based models that treat Person-Job Fit as a text match problem and use contextual knowledge from members\u2019 profiles and jobs\u2019 descriptions: PJFNN, BPJFNN, APJFNN, and MV-CoN. In particular, MV-CoN additionally introduces structural information to enhance model performance. (2) collaborative filtering-based model that uses direct interactions between members and jobs: LightGCN. (3) end-to-end heterogeneous graph neural network models: HAN, HGT. We use four widely used metrics to evaluate the ranking performance: AUC, accuracy (ACC), F1, and average precision (AP). The Person-Job Fit models, namely PJFNN, BPJFNN, and APJFNN, are implemented using RecBole\u2014an established open-source recommendation library (Zhao et al., 2021). The MV-CoN model leverages the original code provided in their respective paper (Bian et al., 2020). Other models are implemented with PyTorch Geometric (Fey and Lenssen, 2019). We have employed BERT-Tiny666https://huggingface.co/prajjwal1/bert-tiny to reduce computational costs. The embedding dimensions for all models are standardized at 32. As discussed in Section 3.3.1, subgraph sampling for mini-batch WHIN pre-training is executed using the PyTorch Geometric subgraph sampler (Fey and Lenssen, 2019). Subgraph construction starts from nodes within a batch of candidate pairs, and each hop samples five neighbors via relations (or metapaths). Reconnecting all relations (and metapaths) between the sampled nodes follows this. We set the number of sampled hops to 3. The number of sampled hops is fixed at 3. In CSAGNN, the count of sampled skills (nssubscript\ud835\udc5b\ud835\udc60n_{s}italic_n start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT) is designated as 10, and CSAGNN comprises two layers. The first 128 tokens from all text inputs are captured for all models. All models are optimized using the Adam optimizer (Kingma and Ba, 2014), with a learning rate adjusted from 0.01 to 0.0001. For the purpose of evaluation, the dataset utilized across all models is randomly partitioned into a ratio of 8:1:1 for training, validation, and testing respectively. Three independent experiments are conducted, each one repeated, to ascertain consistent and reliable outcomes. Table 1 shows the performance of all baseline models and our model, CSAGNN. The results indicate better performance on the hybrid dataset than on individual industry datasets, possibly because jobs within the same industry are more similar and thus harder to distinguish. End-to-end heterogeneous graph neural network models have shown superior performance on all three datasets compared to context-based and CF-based models. HGT, utilizing heterogeneous knowledge, has improved AUC scores by 3.20%, 3.31%, and 1.55% compared to the best baseline MV-CoN, which employs homogeneous knowledge. CSAGNN further improved AUC scores by 1.39%, 1.71%, and 2.37% compared to HGT. Our approach employs two main techniques: WHIN pre-training, which integrates heterogeneous knowledge, including professional networks, and CSAGNN, which incorporates professional networks with an attention mechanism. We conducted ablation studies to analyze the effectiveness of different techniques, where we considered the following variants of CSAGNN: (1) CSAGNN w/o S removes messages from professional connections but retains the job-specific attention mechanism for members themselves. (2) CSAGNN w/o A removes the job-specific attention mechanism. (3) CSAGNN w/o CSA removes both professional connections and job-specific attention mechanisms, using only structural knowledge from the WHIN pre-train approach. (4) CSAGNN w/o CSA&H removes WHIN pre-trained embeddings and CSA mechanism, only using text information as input and leverages MLP to predict. As detailed in Table 3, within single industry datasets with low social noise, the job-specific attention mechanism and professional connections\u2019 messages yielded limited benefits. However, WHIN pre-training embeddings were crucial for enhancing performance. On hybrid datasets with high social noise, where candidate pairs may span different industries, the job-specific attention mechanism, along with the WHIN pre-training embedding, significantly improved the model. To investigate the specific roles of professional networks and job-specific attention mechanisms in our model, we performed hyperparameter tuning experiments by fixing one module and varying the other to observe its impact on performance. Specifically, we first fixed the number of CSAGNN layers to 1 and tested the model\u2019s performance with different numbers of sampled skills (nssubscript\ud835\udc5b\ud835\udc60n_{s}italic_n start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT), as described in Section 3.4. We then fixed nssubscript\ud835\udc5b\ud835\udc60n_{s}italic_n start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT to 10 and tested the model with different numbers of CSAGNN layers. The results on all datasets showed that, compared to the neighbor sampling range, the gain in model performance from the attention mechanism saturates when the number of sampled skills is relatively small. This observation led us to explore the balance between model performance and efficiency by selecting a smaller value for nssubscript\ud835\udc5b\ud835\udc60n_{s}italic_n start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT. The value of utilizing professional networks is illustrated in the example shown in Figure 5. In this case, we have rephrased the information to preserve the users\u2019 privacy while retaining its essential meaning. Even though the member\u2019s profile contains minimal information, the CSAGNN model adeptly predicts their classification by harnessing job-related insights from the member\u2019s professional connections. This contrasts sharply with the incorrect predictions made by the collaborative filtering-based LightGCN and context-based PJFNN models. This specific example accentuates the vital role of professional connections in the classification process and underlines the CSAGNN model\u2019s distinctive capability to leverage such relationships for precise predictions. To demonstrate the effectiveness of WHIN pre-training in capturing meaningful skill representations for CSAGNN, we analyzed the pre-trained embeddings of selected skills in the embedding space. Utilizing Principal Component Analysis (PCA) as our method of dimensional reduction (Jolliffe and Cadima, 2016), we were able to visualize the WHIN pre-trained embeddings for various skills. Figure 6 reveals that skills closely related to programming, such as C++ and Python, are grouped together in the embedding space. In contrast, skills associated with healthcare form a distinct cluster. It is noteworthy that these classifications were achieved using only the skill names as initialization. Yet, our WHIN pre-training method successfully distinguishes between skill categories by learning from heterogeneous knowledge, highlighting the capability of WHIN to recognize and differentiate skills across multiple domains. This paper introduces a novel two-stage approach for leveraging professional networks in Person-Job Fit, including the formation of the Workplace Heterogeneous Information Network (WHIN). WHIN encompasses various entities such as members, jobs, skills, companies, and schools, with an emphasis on the professional connections among members. By employing heterogeneous graph pre-training techniques, the approach acquires representations that integrate professional connections and other diverse information for different entities. These representations are subsequently applied to the CSAGNN model, helping to filter out social noise. Experimental results show that professional connections provide valuable job-specific insights. The WHIN pre-training method is also promising for applications like skill completion and professional connection recommendations, opening new research avenues. Furthermore, for large-scale applications of CSAGNN, a key area of future work involves reducing the model\u2019s computational overhead (Lin et al., 2020; Li et al., 2020a, 2021). The integration of professional networks into Person-Job Fit models offers significant potential for enriching recommendations. However, we must recognize and address two vital ethical considerations that may lead to adverse societal implications: Firstly, the core of our approach involves accessing members\u2019 professional connections and corresponding profiles, which raises privacy concerns. In our research, we ensured that the data collection and processing respected privacy by adhering to proper consent mechanisms and limiting access to pertinent information. Future users should follow suit, being mindful of the need for clear and voluntary consent from members and carefully controlling access to connection information. Additionally, connections often exhibit demographic clustering, which could introduce biases favoring certain groups within networks. The models might infer sensitive attributes like race, gender, or age. Future users should conduct audits to ensure demographic equity and prohibit direct utilization of protected class information. In summary, while the enhancement of Person-Job Fit models through professional network data brings advancements, it also introduces risks concerning privacy and fairness. These ethical challenges call for a concerted effort from platforms to resolve, ensuring that the innovations foster an environment that is both respectful of individual rights and free from discriminatory biases."
    },
    {
        "id": 7,
        "title": "Learning of networked spreading models from noisy and incomplete data",
        "corpus": "Recent years have seen a lot of progress in algorithms for learning parameters of spreading dynamics from both full and partial data. Some of the remaining challenges include model selection under the scenarios of unknown network structure, noisy data, missing observations in time, as well as an efficient incorporation of prior information to minimize the number of samples required for an accurate learning. Here, we introduce a universal learning method based on scalable dynamic message-passing technique that addresses these challenges often encountered in real data. The algorithm leverages available prior knowledge on the model and on the data, and reconstructs both network structure and parameters of a spreading model. We show that a linear computational complexity of the method with the key model parameters makes the algorithm scalable to large network instances. Spreading models are routinely used to generate predictions for a plethora of diffusion processes on networks, whereby infectious diseases, opinions, or failures propagate in natural, social, and technological systems [1, 2]. In these models, the nodes typically go from inactive to an active state through interactions with their neighbors on a network, similarly to how an infection is passed from one person to another. When the model structure or parameters are unknown, it is natural to consider the inverse problem of learning of the spreading model from data. Available data usually takes form of reported activation times for nodes in a network in several observed activation cascades. Whether due to a limited observation budget or imperfect reporting, the accessible data is unlikely to be perfect, and may be subject to uncertainty or provide only partial information on the system. This motivates the design of robust methods for selecting a spreading model from incomplete and noisy data. In this paper, we introduce a scalable learning method that addresses the challenges related to imperfect data. Reconstruction of networked spreading models has been addressed by a number of works in recent years. In the context of full and exact observation of nodes\u2019 activation times, [3, 4, 5, 6] showed that maximum-likelihood type approaches succeed to learn the model structure and parameters. The problem becomes significantly more difficult when the system is only partially observed, i.e., only a fraction of nodes report information about their activation times. In this case, the maximum likelihood approach has exponential complexity with respect to the number of unobserved nodes, which warrants learning methods based on the direct problem of predicting the model dynamics [7]. This approach has lead to an efficient algorithm for learning the parameters of the spreading model on a known network, based on minimization of the distance between observed and model-predicted node marginal probabilities [8]. This choice is motivated by the fact that marginal probabilities of nodes\u2019 activation can be estimated in a computationally efficient way using a method known as dynamic message passing (DMP) [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]. DMP is an inference method for spreading processes on networks derived from a classical belief propagation (BP) algorithm [29, 30], and thus showing similar properties to BP [17, 21]. The key result of [8] stated that the model learned using the minimization of distance between marginal probabilities generates better predictions compared to the model with ground-truth parameters when DMP is used as the inference algorithm. This has been shown through an empirical study on a popular spreading model known as Independent Cascade [31, 32], equivalent to a popular epidemic spreading susceptible-infected-recovered (SIR) model [33, 34] with deterministic recovery. Due to a prediction-centric focus, the algorithm proposed in [8] has been referred to as SLICER (Scalable Learning of Independent Cascade Effective Representation). On many sparse network instances, SLICER produces high-quality estimates of model parameters in a time scaling linearly with the system size, even for fraction of hidden nodes up to 25% of the network. However, from the results of [8], it is not immediately obvious what the expected accuracy limits of the method are, i.e., what error can be expected when the number of unobserved nodes reaches very high values. In order to get insight into the performance of SLICER, in section III.1 we run a systematic evaluation of the performance of SLICER for learning of the spreading model parameters on a variety of random graph classes under diverse fractions of unobserved nodes. Partial node-observability of the system is only one of the several ways in which the data may be incomplete or uncertain. Some of the challenges often encountered in real data include model selection under the scenarios of unknown or partially known network structure, noisy data, missing observations in time, or necessity to include prior information in order to minimize the required amount of data. Scenarios that we consider are presented in Figure 1. The first challenge that we address is the lack of information about the network structure. This task has been recently studied in [35], however, the proposed method involves cubic complexity in the number of nodes, making its application to large networks prohibitive. The second challenge deals with incorporation of prior information on the model parameters in the case where the number of available samples is small. For instance, in epidemiological applications, at most a few realizations of the dynamics can be observed, but the spreading is often modeled by a single parameter (transmission probability) [36]. We show how to leverage this prior information and learn the simple graph models from a few observed trajectories. The third challenge is missing data in time, where nodes report their activation times only during limited time windows. This task was analysed in [37], but with an additional assumption that the full probabilistic trace for each node is available. A likelihood-free approach for the missing time case was proposed in [38], together with additional goal of finding the source of spreading. A variant of the scenario of missing timestamps assumes that only the complete final state of the dynamics is available. One of the first results for this variant of the problem assumed that the unknown network is a tree [39], but later this assumption has been relaxed in [40, 41, 42]. In our work, we show that the spreading model can be accuractly recovered with minimal available information in the temporal space, even in the case of partial node-observability. The last challenge that we consider, often related to the reporting procedure, is uncertainty in timestamps, which may be modeled as an additional noise added to the observed data. Similar setting was analysed in [43, 44], but not paired with any other type of missing information such as partial node-wise reporting. As we show in this work, in most of these scenarios, the objective function used in SLICER method is not directly applicable, resulting in biased estimates of the model parameters. Our overreaching goal consists in generalizing SLICER, introducing a universal learning framework capable of addressing all these challenges. Here, we show how to crucially define the objective function in such a way that it incorporates the available prior information, and provides a high-quality reconstruction of the model parameters. As demonstrated below and contrary to the solution of these challenges on a case-by-case basis, this universality presents an advantage in that data uncertainties present in the data and pertaining to different challenges can be treated at the same time. As a demonstration of such a universality, in the end of our study, we illustrate the approach on real-world network instances under a combination of these challenges. In the section, we first pose the learning problem using the Independent Cascade (IC) model as the dynamics of choice. Then, we discuss the dynamic message passing (DMP) inference method that lies at the foundation of our approach. Finally, we explain the details of our learning method. In this paper, we focus on the Independent Cascade model [31, 32]. Define the spreading network as a graph G=(V,E)\ud835\udc3a\ud835\udc49\ud835\udc38G=(V,E)italic_G = ( italic_V , italic_E ), consisting of the set of vertices V={vi}\ud835\udc49subscript\ud835\udc63\ud835\udc56V=\\{v_{i}\\}italic_V = { italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, and the set of edges E={(vi,vj)|vi,vj\u2208V}\ud835\udc38conditional-setsubscript\ud835\udc63\ud835\udc56subscript\ud835\udc63\ud835\udc57subscript\ud835\udc63\ud835\udc56subscript\ud835\udc63\ud835\udc57\ud835\udc49E=\\{(v_{i},v_{j})\\,|\\,v_{i},v_{j}\\in V\\}italic_E = { ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) | italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u2208 italic_V }. Each node visubscript\ud835\udc63\ud835\udc56v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in the IC model can be in one of two states: active and inactive. Assuming node-wise independent initial condition for the states of the nodes at time t=0\ud835\udc610t=0italic_t = 0, which can be either deterministic or stochastic, further dynamics is subject to a single stochastic rule. If node i\ud835\udc56iitalic_i gets activated at any time t\ud835\udc61titalic_t, it has only one chance to activate any of its inactive neighbors j\ud835\udc57jitalic_j at time t+1\ud835\udc611t+1italic_t + 1. This happens independently for each of the inactive neighbor with edge dependent probability \u03b1ijsubscript\ud835\udefc\ud835\udc56\ud835\udc57\\alpha_{ij}italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT: Regardless of activating any of its neighbors, node i\ud835\udc56iitalic_i remains active forever, but it cannot activate any other node in the future beyond t+1\ud835\udc611t+1italic_t + 1. In what follows, we assume that G\ud835\udc3aGitalic_G is undirected and there are no self-loops or multi-links (although this assumption can be relaxed in general). The dynamics continues until a predefined number of steps T\ud835\udc47Titalic_T. A single realisation of this process is called a cascade. Since each node can be activated only once, every cascade c\ud835\udc50citalic_c is fully described by a set of activation times {\u03c4ic}i\u2208Vsubscriptsubscriptsuperscript\ud835\udf0f\ud835\udc50\ud835\udc56\ud835\udc56\ud835\udc49\\{\\tau^{c}_{i}\\}_{i\\in V}{ italic_\u03c4 start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i \u2208 italic_V end_POSTSUBSCRIPT. Additionally, if node i\ud835\udc56iitalic_i does not get activated before or at the specified time T\ud835\udc47Titalic_T in cascade c\ud835\udc50citalic_c, we assign \u03c4ic=*subscriptsuperscript\ud835\udf0f\ud835\udc50\ud835\udc56\\tau^{c}_{i}=*italic_\u03c4 start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = * as its activation time. It means that *** summarizes all future activity of a given node. More details about this notation is given in the Appendix A. In all our experiments, we use a single seed as an initial condition (this setting corresponds to the most popular scenario, but can be relaxed as well). This means that there is only one node activated at the beginning of the spreading process, although it may be a different node for each specific cascade. The number of observed cascades is denoted by M\ud835\udc40Mitalic_M. The problem that we consider in this work is as follows: given a set of observed activation times {\u03c4ic}i\u2208O,c\u2208Csubscriptsubscriptsuperscript\ud835\udf0f\ud835\udc50\ud835\udc56formulae-sequence\ud835\udc56\ud835\udc42\ud835\udc50\ud835\udc36\\{\\tau^{c}_{i}\\}_{i\\in O,c\\in C}{ italic_\u03c4 start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i \u2208 italic_O , italic_c \u2208 italic_C end_POSTSUBSCRIPT where O\u2282V\ud835\udc42\ud835\udc49O\\subset Vitalic_O \u2282 italic_V is the set of observed nodes and C\ud835\udc36Citalic_C is the set of cascades, as well as using additional information on the cascades (e.g., prior knowledge that \u03c4icsubscriptsuperscript\ud835\udf0f\ud835\udc50\ud835\udc56\\tau^{c}_{i}italic_\u03c4 start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are noisy), learn model parameters, i.e., estimate {\u03b1ij}(ij)\u2208V\u00d7Vsubscriptsubscript\ud835\udefc\ud835\udc56\ud835\udc57\ud835\udc56\ud835\udc57\ud835\udc49\ud835\udc49\\{\\alpha_{ij}\\}_{(ij)\\in V\\times V}{ italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT ( italic_i italic_j ) \u2208 italic_V \u00d7 italic_V end_POSTSUBSCRIPT so that they are close to the parameters of the ground-truth model, denoted as {\u03b1ij*}(ij)\u2208Esubscriptsubscriptsuperscript\ud835\udefc\ud835\udc56\ud835\udc57\ud835\udc56\ud835\udc57\ud835\udc38\\{\\alpha^{*}_{ij}\\}_{(ij)\\in E}{ italic_\u03b1 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT ( italic_i italic_j ) \u2208 italic_E end_POSTSUBSCRIPT. In the case of the unknown network, thresholded values of {\u03b1ij}(ij)\u2208V\u00d7Vsubscriptsubscript\ud835\udefc\ud835\udc56\ud835\udc57\ud835\udc56\ud835\udc57\ud835\udc49\ud835\udc49\\{\\alpha_{ij}\\}_{(ij)\\in V\\times V}{ italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT ( italic_i italic_j ) \u2208 italic_V \u00d7 italic_V end_POSTSUBSCRIPT away from small values close to zero define the recovered network structure E^^\ud835\udc38\\widehat{E}over^ start_ARG italic_E end_ARG that can be compared to the ground-truth set of edges E\ud835\udc38Eitalic_E. In what follows, the fraction of hidden nodes |V\\O|/|O|\\\ud835\udc49\ud835\udc42\ud835\udc42|V\\backslash O|/|O|| italic_V \\ italic_O | / | italic_O | is denoted as \u03be\ud835\udf09\\xiitalic_\u03be. We chose the IC model for simplicity reasons. On one hand, it does capture basic properties of many known spreading processes, including a possibility of an early cascade termination. On the other, it simplifies the analytical equations presented in the next subsection, making the proposed approach easier to follow and understand. Our approach can be can easily be generalized to a broad class of more complex spreading models on networks for which DMP equations (explained next) are known. As discussed in the Introduction, presence of partial information warrants learning methods that feature inference algorithms for predicting the model dynamics as a subroutine. One of the key observables that quantify the spread is given by the influence function: the number of expected number of activated nodes at a certain time t\ud835\udc61titalic_t for a given initial condition. For a fixed initial conditions in cascade c\ud835\udc50citalic_c, the influence function is given by the sum of marginal probabilities pic(t)subscriptsuperscript\ud835\udc5d\ud835\udc50\ud835\udc56\ud835\udc61p^{c}_{i}(t)italic_p start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) of activation of each node i\u2208V\ud835\udc56\ud835\udc49i\\in Vitalic_i \u2208 italic_V at at time t\ud835\udc61titalic_t [20]. Prediction of influence function or marginal probabilities is known to be #P-hard [45, 46], and hence approximate methods need to be used. A classical way of estimating the influence function consists in using Monte-Carlo simulations. However, this approach typically requires a large sampling factor to provide a reliable estimate [32, 47, 48, 49, 50, 51]. Aiming at an accelerated approach that saves this sampling factor, we use the inference method known as Dynamic Message Passing [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]. For IC model [21], DMP estimates marginal probabilities of activation in a linear time in both system size and duration of the dynamics, and has the properties of being exact on graphs without loops and asymptotically exact on random graphs, providing an upper-bound of the influence function on general networks. For the IC model, the equations take the following form: where pic(t)subscriptsuperscript\ud835\udc5d\ud835\udc50\ud835\udc56\ud835\udc61p^{c}_{i}(t)italic_p start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) is the marginal probability of node i\ud835\udc56iitalic_i being active at time t\ud835\udc61titalic_t under initial conditions of cascade c\ud835\udc50citalic_c, pi\u2192jc(t)subscriptsuperscript\ud835\udc5d\ud835\udc50\u2192\ud835\udc56\ud835\udc57\ud835\udc61p^{c}_{i\\rightarrow j}(t)italic_p start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT ( italic_t ) is the same probability, but on an auxiliary graph where node j\ud835\udc57jitalic_j was removed, \u2202i\ud835\udc56\\partial i\u2202 italic_i denotes the set of neighbors of node i\ud835\udc56iitalic_i in the graph G\ud835\udc3aGitalic_G, and \u2202j\u2216i\ud835\udc57\ud835\udc56\\partial j\\setminus i\u2202 italic_j \u2216 italic_i denotes the set of neighbors of node j\ud835\udc57jitalic_j in the graph G\ud835\udc3aGitalic_G except i\ud835\udc56iitalic_i. We also denote an initial condition for node i\ud835\udc56iitalic_i in cascade c\ud835\udc50citalic_c as p\u00afic=pic(0)subscriptsuperscript\u00af\ud835\udc5d\ud835\udc50\ud835\udc56subscriptsuperscript\ud835\udc5d\ud835\udc50\ud835\udc560\\bar{p}^{c}_{i}=p^{c}_{i}(0)over\u00af start_ARG italic_p end_ARG start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_p start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 0 ). Note that for known model parameters, the marginals given by DMP depend only on the initial condition p\u00aficsubscriptsuperscript\u00af\ud835\udc5d\ud835\udc50\ud835\udc56\\bar{p}^{c}_{i}over\u00af start_ARG italic_p end_ARG start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Detailed properties of the above equations were studied in [21]. In Appendix A, following the approach of [17], we provide an alternative derivation of DMP equations for IC model that connects them and their properties to the classical belief propagation algorithm [29, 30]. In this section, we explain the details of our method. We start by stating the approach introduced in [8] and known as Scalable Learning of Independent Cascade Effective Representation (SLICER), which will be used as a baseline method in all numerical experiments below. Subsequently, we will show that the objective used in SLICER need to be modified to account for the prior information on the uncertainty in the data, and enhanced with the known constraints on the model parameters \u2013 an approach referred to as SLICER+. Let us focus on the case of perfectly observed information from a subset of visible nodes O\ud835\udc42Oitalic_O [8]. SLICER is based on the minimization of the Kullback\u2013Leibler (KL) divergence between the empirical marginal distributions computed from the data, and the marginals estimated from the model using DMP, on the observed nodes. Marginal distributions depend on non-local model parameters, including those adjacent to hidden nodes, and thus allowing for the reconstruction of the entirety of model parameters under sufficient observations. Minimization of KL distance on marginals on the observed nodes in our case is equivalent to maximizing: where C\ud835\udc36Citalic_C is the set of available cascades, O\ud835\udc42Oitalic_O is the set of visible nodes and \u03bcic(t)subscriptsuperscript\ud835\udf07\ud835\udc50\ud835\udc56\ud835\udc61\\mu^{c}_{i}(t)italic_\u03bc start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) is the marginal probability of node i\ud835\udc56iitalic_i being activated under cascade c\ud835\udc50citalic_c precisely at time t\ud835\udc61titalic_t. These marginal probabilities can be calculated based on the marginal variables pic(t)subscriptsuperscript\ud835\udc5d\ud835\udc50\ud835\udc56\ud835\udc61p^{c}_{i}(t)italic_p start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) used in the DMP equations above: where \ud835\udfd91\\mathds{1}blackboard_1 stands for the indicator function. The cost function (4) has been first proposed in [7] showing an asymptotic consistency of DMP-based recovery using this objective, but providing an inefficient optimization algorithm. The work [8] proposed an efficient algorithm for minimizing (4), which gave rise to the SLICER algorithm. For a given set of parameters \u03b1ijsubscript\ud835\udefc\ud835\udc56\ud835\udc57\\alpha_{ij}italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT, marginal probabilities depend only on the initial condition. Therefore, we can re-write the objective in the following way: where S\ud835\udc46Sitalic_S is a set of all the initial conditions occurring across all the cascades C\ud835\udc36Citalic_C and m\u03c4issuperscript\ud835\udc5asubscriptsuperscript\ud835\udf0f\ud835\udc60\ud835\udc56m^{\\tau^{s}_{i}}italic_m start_POSTSUPERSCRIPT italic_\u03c4 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is the number of times node i\ud835\udc56iitalic_i gets activated at time \u03c4issubscriptsuperscript\ud835\udf0f\ud835\udc60\ud835\udc56\\tau^{s}_{i}italic_\u03c4 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT under initial condition s\ud835\udc60sitalic_s. This equivalent reformulation allows one to reduce the computing cost by evaluating DMP equations only |S|\ud835\udc46|S|| italic_S | times instead of M=|C|\ud835\udc40\ud835\udc36M=|C|italic_M = | italic_C |. In our simulations, where s\ud835\udc60sitalic_s is assumed to be a single node, it means that DMP will not be run more than N\ud835\udc41Nitalic_N times, regardless of the number of available cascades. In the case of a stochastic initial condition of all of the cascades, e.g., when each node is independently initially activated with probability 1N1\ud835\udc41\\frac{1}{N}divide start_ARG 1 end_ARG start_ARG italic_N end_ARG, the size of S\ud835\udc46Sitalic_S is |S|=1\ud835\udc461|S|=1| italic_S | = 1, making the whole computation far less costly. In order to maximize (6) we use a Lagrangian formulation, where the objective function \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O (6) is supplemented with constrains on marginal provided via DMP, where in the absence of any additional prior information on the parameters, \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C are given by DMP equations (2)-(3) reweighted by Lagrange multipliers \u03bbis(t)subscriptsuperscript\ud835\udf06\ud835\udc60\ud835\udc56\ud835\udc61\\lambda^{s}_{i}(t)italic_\u03bb start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) for all nodes i\u2208V\ud835\udc56\ud835\udc49i\\in Vitalic_i \u2208 italic_V and \u03bbi\u2192js(t)subscriptsuperscript\ud835\udf06\ud835\udc60\u2192\ud835\udc56\ud835\udc57\ud835\udc61\\lambda^{s}_{i\\rightarrow j}(t)italic_\u03bb start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT ( italic_t ) for all edges (ij)\u2208E\ud835\udc56\ud835\udc57\ud835\udc38(ij)\\in E( italic_i italic_j ) \u2208 italic_E of graph G\ud835\udc3aGitalic_G for each time t\ud835\udc61titalic_t and for each cascade in the class s\ud835\udc60sitalic_s, see Appendix B for details. The iterative equations constituting SLICER is obtained by differentiating the Lagrangian (7) with respect to all variables: pis(t)i\u2208Vsuperscriptsubscript\ud835\udc5d\ud835\udc56\ud835\udc60subscript\ud835\udc61\ud835\udc56\ud835\udc49{p_{i}^{s}(t)}_{i\\in V}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_t ) start_POSTSUBSCRIPT italic_i \u2208 italic_V end_POSTSUBSCRIPT, pi\u2192js(t)(ij)\u2208V\u00d7Vsuperscriptsubscript\ud835\udc5d\u2192\ud835\udc56\ud835\udc57\ud835\udc60subscript\ud835\udc61\ud835\udc56\ud835\udc57\ud835\udc49\ud835\udc49{p_{i\\to j}^{s}(t)}_{(ij)\\in V\\times V}italic_p start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_t ) start_POSTSUBSCRIPT ( italic_i italic_j ) \u2208 italic_V \u00d7 italic_V end_POSTSUBSCRIPT, \u03b1ij(ij)\u2208V\u00d7Vsubscriptsubscript\ud835\udefc\ud835\udc56\ud835\udc57\ud835\udc56\ud835\udc57\ud835\udc49\ud835\udc49{\\alpha_{ij}}_{(ij)\\in V\\times V}italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUBSCRIPT ( italic_i italic_j ) \u2208 italic_V \u00d7 italic_V end_POSTSUBSCRIPT, \u03bbis(t)i\u2208Vsuperscriptsubscript\ud835\udf06\ud835\udc56\ud835\udc60subscript\ud835\udc61\ud835\udc56\ud835\udc49{\\lambda_{i}^{s}(t)}_{i\\in V}italic_\u03bb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_t ) start_POSTSUBSCRIPT italic_i \u2208 italic_V end_POSTSUBSCRIPT, and \u03bbi\u2192js(t)(ij)\u2208V\u00d7Vsuperscriptsubscript\ud835\udf06\u2192\ud835\udc56\ud835\udc57\ud835\udc60subscript\ud835\udc61\ud835\udc56\ud835\udc57\ud835\udc49\ud835\udc49{\\lambda_{i\\to j}^{s}(t)}_{(ij)\\in V\\times V}italic_\u03bb start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_t ) start_POSTSUBSCRIPT ( italic_i italic_j ) \u2208 italic_V \u00d7 italic_V end_POSTSUBSCRIPT. This results in DMP equations (2)-(3) in the primal space; DMP-like equations on the Lagrange multipliers \u03bbis(t)i\u2208Vsuperscriptsubscript\ud835\udf06\ud835\udc56\ud835\udc60subscript\ud835\udc61\ud835\udc56\ud835\udc49{\\lambda_{i}^{s}(t)}_{i\\in V}italic_\u03bb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_t ) start_POSTSUBSCRIPT italic_i \u2208 italic_V end_POSTSUBSCRIPT, and \u03bbi\u2192js(t)(ij)\u2208V\u00d7Vsuperscriptsubscript\ud835\udf06\u2192\ud835\udc56\ud835\udc57\ud835\udc60subscript\ud835\udc61\ud835\udc56\ud835\udc57\ud835\udc49\ud835\udc49{\\lambda_{i\\to j}^{s}(t)}_{(ij)\\in V\\times V}italic_\u03bb start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_t ) start_POSTSUBSCRIPT ( italic_i italic_j ) \u2208 italic_V \u00d7 italic_V end_POSTSUBSCRIPT in the dual space; and update equations for the parameters \u03b1ij(ij)\u2208V\u00d7Vsubscriptsubscript\ud835\udefc\ud835\udc56\ud835\udc57\ud835\udc56\ud835\udc57\ud835\udc49\ud835\udc49{\\alpha_{ij}}_{(ij)\\in V\\times V}italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUBSCRIPT ( italic_i italic_j ) \u2208 italic_V \u00d7 italic_V end_POSTSUBSCRIPT, see Appendix B. When prior information about the parameters is available, it can be directly incorporated in the term \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C. As an example, the constraint can be greatly simplified if we know that for each (ij)\u2208E\ud835\udc56\ud835\udc57\ud835\udc38(ij)\\in E( italic_i italic_j ) \u2208 italic_E, \u03b1ij=\u03b1subscript\ud835\udefc\ud835\udc56\ud835\udc57\ud835\udefc\\alpha_{ij}=\\alphaitalic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = italic_\u03b1 (the case of so-called simple graphs, representing a popular case in epidemiological models). As we show below in the section III.3, the gradient of the Lagrangian with respect to the model parameter \u03b1\ud835\udefc\\alphaitalic_\u03b1 in this case reads for \u03b1\u22600\ud835\udefc0\\alpha\\neq 0italic_\u03b1 \u2260 0: The gradient in Eq. (32) can be used to learn parameter \u03b1\ud835\udefc\\alphaitalic_\u03b1 using an iterative procedure with \u03b1\u27f5\u03b1+\u03b5\u2202\u2112\u2202\u03b1\u27f5\ud835\udefc\ud835\udefc\ud835\udf00\u2112\ud835\udefc\\alpha\\longleftarrow\\alpha+\\varepsilon\\frac{\\partial\\mathcal{L}}{\\partial\\alpha}italic_\u03b1 \u27f5 italic_\u03b1 + italic_\u03b5 divide start_ARG \u2202 caligraphic_L end_ARG start_ARG \u2202 italic_\u03b1 end_ARG, where \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 is a learning rate. Lagrangian formulation ensures that a single gradient descent step has the worst-case computational complexity O(|E|T|S|)\ud835\udc42\ud835\udc38\ud835\udc47\ud835\udc46O(|E|T|S|)italic_O ( | italic_E | italic_T | italic_S | ), which means it is linear in the system size, cascade length and the number of initial conditions. In a similar manner, under different scenarios of incomplete data considered in Figure 1, the loss function \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O can be appropriately modified to include the prior information A\ud835\udc34Aitalic_A on the type of the uncertainty in the data, as we discuss in the next section below. Collectively, we refer to the algorithm based on modified objective \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O and constraints \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C as to SLICER+. The full derivation of SLICER+ for each of the challenges in Figure 1 is given in the Appendix B. Efficient implementation of the algorithm, taking advantage of the DMP-like equations on the evolution of the Lagrange multiplies in the dual space and assuring the linear complexity of the algorithm, is available in the Appendix C. In this section we present details on the SLICER+ approach designed for dealing with different types of challenges regarding uncertainty and partial observability of data presented in Figure 1. All subsections devoted to particular challenges are supported with simulations on synthetic networks. For this purpose we choose four different network models: 3-regular tree (RT), 3-regular random graph (RR), Erd\u00f6s-R\u00e9nyi graph with average degree equal to 3 (ER) [52] and a scale-free graph generated with Barab\u00e1si-Albert model with average degree equal to 3 (BA) [53]. Except for the tree, all models have the same number of edges, so that they are comparable. The tree network case is studied for the baseline benchmarking purposes given that DMP is exact on trees. In the section III.6, we apply the method to real-world networks, while assuming a simultaneous co-occurence of multiple types of incomplete data. SLICER was already shown in [8] to perform well in a regime where the unobserved part of the system is significant. It was not shown, however, what are the limits of the algorithm in terms of the size of the unobserved part and how it depends on the sample size. Here we answer this question and additionally we show how these limits depend on the network structure. We take all four synthetic network models, generate up until M=106\ud835\udc40superscript106M=10^{6}italic_M = 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT cascades and vary the percentage of the unobserved part from 0% to 95% (with a 5% step). The results of applying SLICER to all these cases are presented in the form of heat maps in Fig. 2. Interestingly, the quality of reconstructed parameters decrease faster for tree graphs than for loopy networks, because tree graphs have significantly lower edge density than other networks, making it not directly comparable. At the same time, single unobserved node affects a tree structure in a more significant way \u2013 single node separates a tree into two disconnected sub-graphs. We observe an interesting behavior from a scale-free graph. Despite a steeper decline in the quality of reconstruction for smaller fractions of the unobserved part \u03be\ud835\udf09\\xiitalic_\u03be, for high values of \u03be\ud835\udf09\\xiitalic_\u03be the reconstruction still correlate with the true solution. If we look at 80\u221290%80percent9080-90\\%80 - 90 % of unobserved nodes for both random regular and Erd\u00f6s-R\u00e9nyi graph, the reconstructed parameters are basically random. Dynamics on scale free graph on the other hand, still does contain information that one is able to exploit, even for unobserved fraction equal to \u03be=90%\ud835\udf09percent90\\xi=90\\%italic_\u03be = 90 %. For lower values of the unobserved fraction \u03be\ud835\udf09\\xiitalic_\u03be, in the interval between 30\u221270%30percent7030-70\\%30 - 70 %, the best parameters\u2019 reconstruction results are obtained for the Erd\u00f6s-R\u00e9nyi graph. Finally, when \u03be\ud835\udf09\\xiitalic_\u03be is below 30%percent3030\\%30 %, the best parameters\u2019 reconstruction is achieved for the regular random structure. These results illustrate that the uncertainty of the obtained reconstruction depends on the particular network structure. In many real-world applications, like epidemic spreading, data on contact network is extremely difficult to gather. Sometimes it is possible to assess a limited number of potential connections, but detailed knowledge about the graph is rarely available. In practice, structure learning is equivalent to learning spreading parameters, where near-zero couplings signal an absence of an edge. Under the structure recovery task, parameter learning needs to be run on larger graph reflecting the prior knowledge on the network structure. The less knowledge one have about the network, the larger the set of edges one needs to consider, up until N(N\u22121)/2\ud835\udc41\ud835\udc4112N(N-1)/2italic_N ( italic_N - 1 ) / 2 potential edges in a fully-connected graph when no knowledge about model structure is provided. When the dynamics of all nodes is available, edges can be recovered using maximum likelihood approach and thresholding. We focus on a more challenging scenario where the network structure is unknown or partially known under the presence of unobserved nodes. One of the main difficulty of this scenario consists in an increased potential for the solution degeneracy. Indeed, already in the case of full structural knowledge, degeneracy may appear for specific arrangements of unobserved nodes. The simplest example is when an unobserved node is a leaf \u2013 a node with a single connection. In this case it is impossible to recover the outgoing spreading parameter corresponding to the leaf edge. There are also more complex situations involving solution degeneracy such as interconnected clusters of hidden nodes, and increased number of unknown connections makes these situations more likely. However, information contained in the cascades makes it possible to discover the structure of the diffusion network with high accuracy even on very loopy graphs, as we show below. Consider a problem of selecting the spreading graph from a set of known super-set of edges. Such a super-set of edges becomes a fully-connected graph in the worst case of no prior information on the diffusion network. SLICER can then be used as the structure discovery algorithm as follows. A gradient descent on the parameters {\u03b1ij}subscript\ud835\udefc\ud835\udc56\ud835\udc57\\{\\alpha_{ij}\\}{ italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT } as a part of SLICER is run on a super-set of edges. If a parameter goes beyond a certain threshold value (which we take as 10\u22128superscript10810^{-8}10 start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT in all experiments in this section), it is declared as zero and the respective edge is removed from the set of candidate edges, thus reducing the network at the next steps of the gradient descent procedure. This online pruning procedure results in a reduced overall computational complexity of the algorithm, compared to an alternative where SLICER is run on the full super-set of edges and the parameters are thresholded only in the end of the procedure. In order to evaluate the effectiveness of our method we use a receiver operating characteristic (ROC) curve [54]. After assessing all potential edges, we build the ROC curve of true and false positives among them and then we compute the surface under such curve. ROC curve surface equal to 1111 represents a perfect reconstruction, while values oscillating around 0.50.50.50.5 suggest that the edge set selection is not better than a random guess. Surprisingly, structure discovery with SLICER using the procedure described above is successful even when the underlying network has many short loops, which strongly affects the accuracy of the DMP approximation, as explained in section II.2. To illustrate this point, consider an adversarial scenario of a regular two-dimensional square lattice containing a large number of short loops, which represents the ground-truth propagation network. Further, consider a super-set of edges by adding diagonal connections to the lattice, as shown in Fig. 3(a). Thus expanded network containing 50%percent5050\\%50 % fake edges (and even more short loops compared to the ground-truth square lattice) is then used as a starting network in the structure learning task. Ground-truth parameters \u03b1ijsubscript\ud835\udefc\ud835\udc56\ud835\udc57\\alpha_{ij}italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT are drawn from a uniform distribution over [0,1]01[0,1][ 0 , 1 ] interval. Despite this adversarial scenario, we are able to perfectly reconstruct the correct network, see Fig. 3, even with 25%percent2525\\%25 % of nodes being unobserved. In Fig. 4, we test structure recovery procedure on four different types of synthetic networks, where the size of the super-set of edges is two times larger than the number of ground-truth edges in the network on which the data has been generated. These results show that an accurate recovery of the network structure is possible even in the presence of a large fraction of hidden nodes. In Section III.6, we further explore structure learning for real-world network instances. In Appendix D, we discuss results for network structure recovery in the case of no available prior information on the topology, where the super-set of edges corresponds to a fully-connected graph. In many applications, some knowledge about model parameters is assumed, which makes it easier to estimate them. Most often it is simply assumed that all the spreading parameters are the same for all edges \u03b1ij=\u03b1\u2200(i,j)\u2208Esubscript\ud835\udefc\ud835\udc56\ud835\udc57\ud835\udefcsubscriptfor-all\ud835\udc56\ud835\udc57\ud835\udc38\\alpha_{ij}=\\alpha\\,\\forall_{(i,j)\\in E}italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = italic_\u03b1 \u2200 start_POSTSUBSCRIPT ( italic_i , italic_j ) \u2208 italic_E end_POSTSUBSCRIPT. Although such setting significantly simplifies the problem, it is a good case study for understanding how the algorithm can take advantage of additional knowledge. The constraint of equal parameters change the DMP equations, which leads to a different form of the SLICER+ instantiation for this case. In this new setting the constraints become a single parameter function: which requires recomputing the Lagrangian derivatives. Following the steps described in the Appendix B.2 we arrive at: In the end, not only do we reduce the memory usage of the algorithm, but most importantly, we reduce the amount of data needed to obtain desirable level of error on parameters. We apply the modified procedure to synthetic data generated with different network types. As shown in Fig. 5 even with 90%percent9090\\%90 % unobserved nodes and regardless of the network structure, a small number of cascades is needed to obtain remarkable accuracy. Ten cascades are on average enough to get error below 0.10.10.10.1 and it does not depend strongly on the size of the unobserved part. Assuming different transmission probabilities for each edge in the reconstruction, shown with a gray dashed line as a benchmark, yields results which are orders of magnitude worse. Moreover, as shown in Fig. 2, different transmission probabilities combined with unobserved fraction above 60%percent6060\\%60 % yields almost random outcome, even with large number of cascades. Not being able to observe the whole system, as a result of a limited budget or other challenges related to gathering data, is a fair limitation for many practical applications. Another realistic situation arises when, for similar reasons, one is not able to record the data for extended periods of time. Partial observability of the system, should be considered in both spatial and temporal spaces. Here we show how to modify the objective so that it correctly accounts for the unobserved time periods, resulting in a SLICER+ instantiation for the scenario of missing observations in times. Under this scenario, the state of the network is not observed at each time step, but only at a subset of observation times. Therefore, although we do not generally observe activation time of a node, we still know that activation happened somewhere in the last unobserved period preceding the first time we noticed a given node to be active. As a result there are two cases: (i) node activation is observed and we denote the activation time as \u03c4isubscript\ud835\udf0f\ud835\udc56\\tau_{i}italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT; (ii) node activation happens during an unobserved interval denoted as [\u03c4^i\u2212\u0394\u03c4^i,\u03c4^i]subscript^\ud835\udf0f\ud835\udc56\u0394subscript^\ud835\udf0f\ud835\udc56subscript^\ud835\udf0f\ud835\udc56[\\hat{\\tau}_{i}-\\Delta\\hat{\\tau}_{i},\\hat{\\tau}_{i}][ over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - roman_\u0394 over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]. Observed snapshots of the initial and of the final states of the spreading dynamics only represents a particular example of such a missing information in time. New objective will depend on known activation times and activation intervals: This leads to a variant of the SLICER+ algorithm for missing temporal observations, presented in Appendix B.3. Results in Fig. 6(b-e) are presented in the case where only initial and final states of the dynamics are observed, similarly to the case considered in [41] (where the case of full spatial observations \u03be=0\ud835\udf090\\xi=0italic_\u03be = 0 has been treated). Given the effectiveness of the method in the setting of unobserved time-periods, we explore an even more complex case, where we combine both unobserved times and unobserved nodes, a realistic setting, which, to our best knowledge, was not addressed in the literature before. Results presented in Fig. 6(b-e) are of similar quality as the ones obtained in [8] when there are no missing observations in time, despite lack of knowledge on the exact activation times, which illustrates the robustness of SLICER+ with respect to missing observations in time. Unobserved time periods can be naively treated with SLICER, by simply assuming that if a given node was activated in such a period, it is regarded as an unobserved node. This, however, does not take advantage of all the available information. As it is shown in Fig. 6(f-h), this leads to a one-sided bias, which quantitatively depends on the network structure and the fraction of unobserved timestamps. When the unobserved periods are treated correctly, as done in SLICER+, the bias disappears. Another realistic scenario in data collection relates to the fact that apart from not being able to monitor everything, the collected data is not necessary perfect. This may be a result of omission, deliberate actions or simply imperfect gathering procedure. If, for example, a population is tested for a certain virus, the testing dates are most likely not the dates when the infection occurred. On top of that, the lab results may be delayed, dates may be mistaken and tests can produce both false positives and false negatives. All of this builds up into an unpredictable noise, which cannot be removed in a deterministic matter. It has to be accounted for in the modelling process. In our case of learning spreading parameters, the data is a set of activation times. In this context, we model noisy observations as deviations around the correct activation times. We assume that what is observed is a sum of the original activation time and a noise coming from a certain discrete distribution. In general it is described as follows: where \u03c4\ud835\udf0f\\tauitalic_\u03c4 is the observed activation time, \u03c4*superscript\ud835\udf0f\\tau^{*}italic_\u03c4 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT is the real activation time, and K\ud835\udc3eKitalic_K is a constant related to the support of the noise distribution. The presence of noise in timestamps has no effect on the DMP equations and the constraints, but will affect the objective function, resulting in incorporation of the prior on the noise in the SLICER+ formulation. The objective function will now depend on both activation times and the noise distribution: Exact form of the objective for noisy data and detailed derivation of SLICER+ for this case is given in the Appendix B.4. As in the previous sections, we test the instantiation of SLICER+ for the case of noisy observations with different network structures. In the numerical experiments we use noise described by the following distribution for K=1\ud835\udc3e1K=1italic_K = 1: For simplicity, in the numerical experiments in this section, we assume that the noise distribution is known, but the probabilities \u03c0ksubscript\ud835\udf0b\ud835\udc58\\pi_{k}italic_\u03c0 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT characterizing the noise distribution can be treated as parameters and learned from the data. Notice that a naive application of SLICER to the case of noisy observations could lead to infinite objective function, since the realization of noisy timestamps could be impossible, showing inconsistency with the dynamic rules of the IC model. This could happen, for instance, if a node is observed to be activated before any of its neighbor is activated. Application of SLICER is still possible by neglecting these conflicting cases leading to an infinite value of the objective function. We use such an approach as a benchmark to SLICER+ that accounts for the possibility of noisy observations. Fig. 7(b-e) shows that the use of SLICER quickly leads to a significant error gap, while consideration of noise implemented in SLICER+ brings the algorithm\u2019s performance to a level similar to a noiseless situation. Looking more closely at the particular results at Fig. 7(f-h), we see that disregarding the presence of noise gives results that are correlated with the true solution, but there is a systematic bias, which depends on the network structure. The solution given by the noise-corrected algorithm, on the other hand, aligns perfectly with the ground-truth. Previously we used different network structures to show how they affect the results of the learning procedure for different scenarios. Real-world networks typically combine several characteristics of different random models [55]. In order to truly test the effectiveness of our learning framework we use two real-world social networks together with multiple types of noisy and incomplete spreading data. The first network is the Zachary Karate Club [56] with N=34\ud835\udc4134N=34italic_N = 34 nodes, |E|=78\ud835\udc3878|E|=78| italic_E | = 78 edges and multiple short loops, small enough to serve as an explicit illustration of the quaility of structure learning under partial and noisy observations. The second network is a Facebook snapshot from [57] and previously analysed in [58], with N=2888\ud835\udc412888N=2888italic_N = 2888 nodes and |E|=2981\ud835\udc382981|E|=2981| italic_E | = 2981 edges. In Figure 8, we present results on learning of the structure of the network from a combination of missing information: partially reporting nodes and noisy activation times. We assume that no prior information on the topology is known to the algorithm. Due to many short loops, learning spreading parameters in the Zachary Club network using SLICER+ is not an easy task. As shown in Fig. 8(c), 10%percent1010\\%10 % of unobserved nodes leads to a low error in parameter recovery of 5%percent55\\%5 % error on average. With 25%percent2525\\%25 % of unobserved nodes, this error grows above 10%percent1010\\%10 %. However, the structure recovery is performed with a greater accuracy, despite no assumed knowledge on the edge set. Obviously, no algorithm can produce perfect reconstruction for high fractions of non-reporting nodes and unobserved edges due to a degeneracy in the space of solutions. An explicit structure learning example produced for \u03be=10%\ud835\udf09percent10\\xi=10\\%italic_\u03be = 10 % unobserved nodes and M=1000\ud835\udc401000M=1000italic_M = 1000 cascades of length T=5\ud835\udc475T=5italic_T = 5 is shown in Fig. 8(a). To eliminate possibility of bias due to a choice of ground-truth \u03b1ijsubscript\ud835\udefc\ud835\udc56\ud835\udc57\\alpha_{ij}italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT, we use the test case with all \u03b1ij=0.5subscript\ud835\udefc\ud835\udc56\ud835\udc570.5\\alpha_{ij}=0.5italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 0.5. The algorithm however does not have access to this information during the learning procedure. Although the parameter reconstruction quality could be slightly improved by increasing the number of cascades, one can already see that the reconstruction on the observed part is perfect. Incorrect classification appears only in the vicinity of the unobserved nodes. Specifically, the problem is more pronounced for the two connected unobserved nodes, whereas the result is much better for an isolated unobserved node. We now test structure recovery with SLICER+ on the larger Facebook network with the following setting: the observed timestamps are noisy, and we start with a superset of 5962596259625962 edges, twice as large as the ground-truth set of 2981298129812981 edges. The Facebook snapshot network is a sparse graph with multiple hubs. Hubs tend to increase the number of triangles, but the clustering of this network is on average lower than for the Zachary Club. This is consistent with our observation that the reconstruction error in spreading parameters is significantly smaller, even when 25%percent2525\\%25 % of nodes are unobserved, as shown in Fig. 9. Furthermore, inclusion of the knowledge on the super-set of 5962596259625962 potential edges allows to nearly perfectly reconstruct the network structure in the partial observation regime, assuming enough data is available. Literature on learning of spreading models from data is rich, but not much attention so far was given to the problem of learning from incomplete and uncertain information. Our work proposes a flexible method that addresses this problem. SLICER+ deals with multiple settings related to incomplete and noisy data, that include partial observations in both spatial and temporal dimensions, noisy timestamps, unknown network structure, and combinations of these settings. For unobserved times, we show that a proper formulation of the algorithm is able to learn model parameters from final and initial states only, assuming that enough data available. This holds even in the presence of unobserved nodes. We further show that uncertainty in provided activation times leads to significant bias, when not properly accounted for. Although the structure learning task under unobserved nodes may generate degenerate solutions, we empirically showed that additional knowledge can significantly increase the quality of reconstruction using several representative synthetic networks, as well as two popular real-world networks. For real-world instances specifically, we used a setting where different types of incomplete and uncertain data settings are combined. We found that our results are quite robust regardless of the synthetic network type. As an illustration, we considered quality of reconstruction as a function of the size of the set of unobserved nodes on a variety of random graph families. Our framework can be further developed in several future directions. In the setting of noisy observations, we assumed for simplicity that the noise distribution is known. However, the noise distribution probabilities \u03c0ksubscript\ud835\udf0b\ud835\udc58\\pi_{k}italic_\u03c0 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT can be treated as additional parameters and learned from the data. For the structure learning task, we considered reconstruction from a super-set of edges. It would be interesting to test the impact of other graph topological prior information on learning, such as details on density or degree distribution. Such a setting would be relevant in the setting where surveillance and data gathering procedures can be controlled. Under the scenario where the observations are too sparse and the degeneracy in the reconstruction is unavoidable and in applications focused on downstream prediction tasks, it could be useful to study the properties of the learned models as effective representations [8]. Finally, in the future, it would be useful to extend our framework to other dynamic models, including models with reversible dynamics. Full implementation of all algorithms studied in this work is available at [59]. Authors acknowledge support from the Laboratory Directed Research and Development program of Los Alamos National Laboratory under project numbers 20240245ER and 20240198ER, and from U.S. DOE/SC Advanced Scientific Computing Research Program. The DMP equations presented in the paper are quite intuitive. In case of simple models, such as IC model, they could be derived by identifying the correct dynamical variables to use in the equations, as it was done here. They are also intrinsically connected to a more general framework of Belief Propagation (BP) equations, which are well described in detail in the literature [30, 29]. In the dynamic setting, the BP equations on time trajectories have been studied by Kanoria and Montanari in [11]. Here, for completeness, we show how DMP equations for IC model can be obtained directly from BP formulation. Denote \u03c3\u2192i=(\u03c3i0,\u2026,\u03c3iT)subscript\u2192\ud835\udf0e\ud835\udc56superscriptsubscript\ud835\udf0e\ud835\udc560\u2026superscriptsubscript\ud835\udf0e\ud835\udc56\ud835\udc47\\vec{\\sigma}_{i}=(\\sigma_{i}^{0},\\dots,\\sigma_{i}^{T})over\u2192 start_ARG italic_\u03c3 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , \u2026 , italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) the time trajectory of node i\ud835\udc56iitalic_i variable, and wi(\u03c3it+1|\u03c3it,{\u03c3jt}j\u2208\u2202i)subscript\ud835\udc64\ud835\udc56conditionalsuperscriptsubscript\ud835\udf0e\ud835\udc56\ud835\udc611superscriptsubscript\ud835\udf0e\ud835\udc56\ud835\udc61subscriptsuperscriptsubscript\ud835\udf0e\ud835\udc57\ud835\udc61\ud835\udc57\ud835\udc56w_{i}\\left(\\sigma_{i}^{t+1}|\\,\\sigma_{i}^{t},\\,\\{\\sigma_{j}^{t}\\}_{j\\in% \\partial i}\\right)italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT | italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , { italic_\u03c3 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_j \u2208 \u2202 italic_i end_POSTSUBSCRIPT ) the local probability of node i\ud835\udc56iitalic_i transitioning to state \u03c3it+1superscriptsubscript\ud835\udf0e\ud835\udc56\ud835\udc611\\sigma_{i}^{t+1}italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT at time t+1\ud835\udc611t+1italic_t + 1, given its previous state and the states of its neighborhood at time t\ud835\udc61titalic_t, and P({\u03c3i0}i\u2208V)\ud835\udc43subscriptsuperscriptsubscript\ud835\udf0e\ud835\udc560\ud835\udc56\ud835\udc49P(\\{\\sigma_{i}^{0}\\}_{i\\in V})italic_P ( { italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i \u2208 italic_V end_POSTSUBSCRIPT ) is the probability of the initial state. Additionally, let us introduce new type of message \u03bci\u2192j(\u03c3\u2192i||\u03c3\u2192j)\\mu^{i\\rightarrow j}(\\vec{\\sigma}_{i}||\\vec{\\sigma}_{j})italic_\u03bc start_POSTSUPERSCRIPT italic_i \u2192 italic_j end_POSTSUPERSCRIPT ( over\u2192 start_ARG italic_\u03c3 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | | over\u2192 start_ARG italic_\u03c3 end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ), which represents the probability of node i\ud835\udc56iitalic_i variable having the trajectory \u03c3\u2192isubscript\u2192\ud835\udf0e\ud835\udc56\\vec{\\sigma}_{i}over\u2192 start_ARG italic_\u03c3 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT on an auxiliary graph, where the trajectory of node j\ud835\udc57jitalic_j is fixed to \u03c3\u2192jsubscript\u2192\ud835\udf0e\ud835\udc57\\vec{\\sigma}_{j}over\u2192 start_ARG italic_\u03c3 end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. Our starting point is the dynamic belief propagation (DBP) equation on time trajectories [11]: where \u03bcti\u2192j(\u03c3\u2192i||\u03c3\u2192j)\\mu^{i\\rightarrow j}_{t}(\\vec{\\sigma}_{i}||\\vec{\\sigma}_{j})italic_\u03bc start_POSTSUPERSCRIPT italic_i \u2192 italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( over\u2192 start_ARG italic_\u03c3 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | | over\u2192 start_ARG italic_\u03c3 end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) is a message probability for trajectories up until time t\ud835\udc61titalic_t. A direct use of the above equations is still impractical due to their exponential complexity over cascade length T\ud835\udc47Titalic_T. This is where the unidirectional character of the dynamics come in handy and leads to simplifications. In the case of the IC model, vector \u03c3\u2192isubscript\u2192\ud835\udf0e\ud835\udc56\\vec{\\sigma}_{i}over\u2192 start_ARG italic_\u03c3 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT can be fully described by a single number \u03c4isubscript\ud835\udf0f\ud835\udc56\\tau_{i}italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which represents the activation time. This allows us to rewrite the DBP equations as follows: where \u03bcTi\u2192j(\u03c4i||\u03c4j)\\mu^{i\\rightarrow j}_{T}(\\tau_{i}||\\tau_{j})italic_\u03bc start_POSTSUPERSCRIPT italic_i \u2192 italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | | italic_\u03c4 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) and \u03bcTi(\u03c4i)subscriptsuperscript\ud835\udf07\ud835\udc56\ud835\udc47subscript\ud835\udf0f\ud835\udc56\\mu^{i}_{T}(\\tau_{i})italic_\u03bc start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) correspond the same messages and marginals as before, but parameterized with activation times \u03c4isubscript\ud835\udf0f\ud835\udc56\\tau_{i}italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Additionally, we denote W\u00afi(\u03c4i;{\u03c4k}k\u2208\u2202i)=P({\u03c3i0}i\u2208V)\u22c5Wi(\u03c4i;{\u03c4k}k\u2208\u2202i)subscript\u00af\ud835\udc4a\ud835\udc56subscript\ud835\udf0f\ud835\udc56subscriptsubscript\ud835\udf0f\ud835\udc58\ud835\udc58\ud835\udc56\u22c5\ud835\udc43subscriptsuperscriptsubscript\ud835\udf0e\ud835\udc560\ud835\udc56\ud835\udc49subscript\ud835\udc4a\ud835\udc56subscript\ud835\udf0f\ud835\udc56subscriptsubscript\ud835\udf0f\ud835\udc58\ud835\udc58\ud835\udc56\\bar{W}_{i}(\\tau_{i};\\{\\tau_{k}\\}_{k\\in\\partial i})=P(\\{\\sigma_{i}^{0}\\}_{i\\in V% })\\cdot W_{i}(\\tau_{i};\\{\\tau_{k}\\}_{k\\in\\partial i})over\u00af start_ARG italic_W end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; { italic_\u03c4 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k \u2208 \u2202 italic_i end_POSTSUBSCRIPT ) = italic_P ( { italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i \u2208 italic_V end_POSTSUBSCRIPT ) \u22c5 italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; { italic_\u03c4 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k \u2208 \u2202 italic_i end_POSTSUBSCRIPT ) for simplicity. Before we move further we want to highlight some observations, which will be used later. First, all the information required to compute the probability of activation at certain time \u03c4isubscript\ud835\udf0f\ud835\udc56\\tau_{i}italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is available at that time. Second, if the activation time \u03c4isubscript\ud835\udf0f\ud835\udc56\\tau_{i}italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT of the node i\ud835\udc56iitalic_i is earlier then the activation time \u03c4ksubscript\ud835\udf0f\ud835\udc58\\tau_{k}italic_\u03c4 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT of its neighbor k\ud835\udc58kitalic_k, than the message from i\ud835\udc56iitalic_i to k\ud835\udc58kitalic_k does not depend on \u03c4ksubscript\ud835\udf0f\ud835\udc58\\tau_{k}italic_\u03c4 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. If we combine this we the previous equation, we can write: where \u221e\\infty\u221e describes the state where a given node was activated at any time later than the cutoff time T\ud835\udc47Titalic_T (the time subscript). Finally, the incoming messages in equations above are independent of node i\ud835\udc56iitalic_i activation time \u03c4isubscript\ud835\udf0f\ud835\udc56\\tau_{i}italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and always behave as \u03c4isubscript\ud835\udf0f\ud835\udc56\\tau_{i}italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is later than their cutoff time: This is a direct consequence of the previous observations. Let us now define new type of marginals and messages: where PSi(t)superscriptsubscript\ud835\udc43\ud835\udc46\ud835\udc56\ud835\udc61P_{S}^{i}(t)italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( italic_t ) is the probability that node i\ud835\udc56iitalic_i was not activated until time t\ud835\udc61titalic_t and PSi\u2192j(t)superscriptsubscript\ud835\udc43\ud835\udc46\u2192\ud835\udc56\ud835\udc57\ud835\udc61P_{S}^{i\\rightarrow j}(t)italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i \u2192 italic_j end_POSTSUPERSCRIPT ( italic_t ) is the same probability, but on an auxiliary graph where node j\ud835\udc57jitalic_j does not exist. Symbol *** stands for all the trajectories with the activation time \u03c4isubscript\ud835\udf0f\ud835\udc56\\tau_{i}italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT being after the time horizon T\ud835\udc47Titalic_T. It means that when we sum over \u03c4i>bsubscript\ud835\udf0f\ud835\udc56\ud835\udc4f\\tau_{i}>bitalic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > italic_b, in practice we sum over \u03c4i\u2208{b+1,b+2,\u2026,T\u22121,T,*}subscript\ud835\udf0f\ud835\udc56\ud835\udc4f1\ud835\udc4f2\u2026\ud835\udc471\ud835\udc47\\tau_{i}\\in\\{b+1,b+2,\\dots,T-1,T,*\\}italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 { italic_b + 1 , italic_b + 2 , \u2026 , italic_T - 1 , italic_T , * }. It should be noted that in all equations below we use the convention that when \u03c4i=*subscript\ud835\udf0f\ud835\udc56\\tau_{i}=*italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = *, \u03c4i\u22121=Tsubscript\ud835\udf0f\ud835\udc561\ud835\udc47\\tau_{i}-1=Titalic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - 1 = italic_T. Now, we use these new objects to compute the term W\u00afi(\u2026)subscript\u00af\ud835\udc4a\ud835\udc56\u2026\\bar{W}_{i}(\\dots)over\u00af start_ARG italic_W end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( \u2026 ), which describes the dynamics of the process. In the case of IC model it has the following form: We can now plug it all together and get: which is equivalent to the DMP equations (3) from the main text, where pi\u2192j(t)=1\u2212PSi\u2192j(t)subscript\ud835\udc5d\u2192\ud835\udc56\ud835\udc57\ud835\udc611superscriptsubscript\ud835\udc43\ud835\udc46\u2192\ud835\udc56\ud835\udc57\ud835\udc61p_{i\\rightarrow j}(t)=1-P_{S}^{i\\rightarrow j}(t)italic_p start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT ( italic_t ) = 1 - italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i \u2192 italic_j end_POSTSUPERSCRIPT ( italic_t ) and assuming some initial condition p\u00afi\u2192j=1\u2212PSi\u2192j(0)subscript\u00af\ud835\udc5d\u2192\ud835\udc56\ud835\udc571superscriptsubscript\ud835\udc43\ud835\udc46\u2192\ud835\udc56\ud835\udc570\\bar{p}_{i\\rightarrow j}=1-P_{S}^{i\\rightarrow j}(0)over\u00af start_ARG italic_p end_ARG start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT = 1 - italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i \u2192 italic_j end_POSTSUPERSCRIPT ( 0 ). Readers interested in derivation of DMP from DBP for different dynamic models should refer to [11, 17, 23]. Below we present the details of Lagrangian formulation used in the optimisation scheme of each of the different case of missing or incomplete data. We show step-by-step how to compute all the Lagrange multipliers and the gradient step used to update the parameters. The SLICER algorithm [8] was developed to deal with the scenario of missing information on nodes exclusively. In this setting the objective takes the following form: The constraints are the same as in the main text, but lets remind them for reader\u2019s convenience. The sum of the objective and the constraints constitutes the Lagrangian. Lagrange multipliers can now be found by differentiating the Lagrangian: and equating the derivatives to zero. Finally, derivatives over parameters \u03b1ijsubscript\ud835\udefc\ud835\udc56\ud835\udc57\\alpha_{ij}italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT read which can further be simplified for \u03b1ij\u22600subscript\ud835\udefc\ud835\udc56\ud835\udc570\\alpha_{ij}\\neq 0italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT \u2260 0: The above can be directly used in the forward\u2013backward propagation procedure described in the main text. In the case of simple graphs, where \u03b1ij=\u03b1\u2200(i,j)\u2208Esubscript\ud835\udefc\ud835\udc56\ud835\udc57\ud835\udefcsubscriptfor-all\ud835\udc56\ud835\udc57\ud835\udc38\\alpha_{ij}=\\alpha\\,\\forall_{(i,j)\\in E}italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = italic_\u03b1 \u2200 start_POSTSUBSCRIPT ( italic_i , italic_j ) \u2208 italic_E end_POSTSUBSCRIPT, the objective is the same as for the original SLICER, but the constraints simplify to: As a result, we need to update the derivatives over messages: while the derivatives over marginals remain in the same form as before. Then we can rewrite the derivatives over parameters, which in this case reduce to only a single equation: where by E\u2032superscript\ud835\udc38\u2032E^{\\prime}italic_E start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT we denote the set of edges such that (i,j)\ud835\udc56\ud835\udc57(i,j)( italic_i , italic_j ) and (j,i)\ud835\udc57\ud835\udc56(j,i)( italic_j , italic_i ) are counted as separate elements. The above equation can, assuming that \u03b1>0\ud835\udefc0\\alpha>0italic_\u03b1 > 0, be simplified to: Now we can repeat the whole procedure, but the update step is in only one dimension. In this setting part of the activation times are known precisely, while others are known to be in certain time intervals. Based on that we modify the objective in a following way. The only change is that if the node was activated during an unobserved time interval, the marginal probability of being infected precisely at \u03c4isubscript\ud835\udf0f\ud835\udc56\\tau_{i}italic_\u03c4 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is replaced by a marginal probability of being infected inside the interval [\u03c4^i\u2212\u0394\u03c4^i,\u03c4^i]subscript^\ud835\udf0f\ud835\udc56\u0394subscript^\ud835\udf0f\ud835\udc56subscript^\ud835\udf0f\ud835\udc56[\\hat{\\tau}_{i}-\\Delta\\hat{\\tau}_{i},\\hat{\\tau}_{i}][ over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - roman_\u0394 over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]. The constraints are the same as in the original SLICER. The above modification affects only the computation of the Lagrange derivative over marginal probabilities. This requires updating the values of Lagrange multipliers \u03bbis(t)subscriptsuperscript\ud835\udf06\ud835\udc60\ud835\udc56\ud835\udc61\\lambda^{s}_{i}(t)italic_\u03bb start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ). The rest of the procedure remains the same. Taking into account the noisy activation times results with an objective function of the following form: Updating the objective function affects the learning procedure. However, the only element of the optimisation scheme, which require changes is the computation of \u03bbis(t)superscriptsubscript\ud835\udf06\ud835\udc56\ud835\udc60\ud835\udc61\\lambda_{i}^{s}(t)italic_\u03bb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_t ). All the further computations remain the same as before. It should be noted that the described change in the learning procedure does also affect its complexity. In the worst case scenario where K=T\ud835\udc3e\ud835\udc47K=Titalic_K = italic_T the complexity of the algorithm becomes max[O(|E||S|T),O(N|S|T2)]\ud835\udc42\ud835\udc38\ud835\udc46\ud835\udc47\ud835\udc42\ud835\udc41\ud835\udc46superscript\ud835\udc472\\max[O(|E||S|T),O(N|S|T^{2})]roman_max [ italic_O ( | italic_E | | italic_S | italic_T ) , italic_O ( italic_N | italic_S | italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ] in comparison with the complexity O(|E||S|T)\ud835\udc42\ud835\udc38\ud835\udc46\ud835\udc47O(|E||S|T)italic_O ( | italic_E | | italic_S | italic_T ) of the original algorithm. Direct implementation of Eq. (2) and (3) suffers from evaluating the products over neighbors, in particular, this is repeated for every message rooted in a given node. As a result the complexity of the algorithm depends on the network\u2019s degree distribution, instead of the number of edges. Networks with broad degree distributions, which are common in real-world applications, are specifically affected by this problem. It is possible to overcome this by rewriting the DMP equations in the following form: where the marginals equations are the same, but the messages are computed using the pre-computed marginals. Now the complexity is independent of the degree distribution and truly linear in the number of edges, assuming that \u03b1ij\u22c5pi\u2192jc(t\u22121)\u22601\u22c5subscript\ud835\udefc\ud835\udc56\ud835\udc57subscriptsuperscript\ud835\udc5d\ud835\udc50\u2192\ud835\udc56\ud835\udc57\ud835\udc6111\\alpha_{ij}\\cdot p^{c}_{i\\rightarrow j}(t-1)\\neq 1italic_\u03b1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT \u22c5 italic_p start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT ( italic_t - 1 ) \u2260 1. The latter is, however, a reasonable assumption, which is unlikely to be broken, in which case the original equation can still be used. Similar problem arises from Eq. (30), which after equating left hand side to zero leads to: Although the equation have a different form, they are dual to the DMP one. Similarly to the latter, direct implementation leads to degree distribution dependence. Here instead of a product over neighbors we have a sum over neighbors and similarly as before, this can be prevented. To do so, let us introduce which can be further simplified as Now we can write: This way, knowing that \u03bbi\u2192js(T)=0\u2200i,jsubscriptsuperscript\ud835\udf06\ud835\udc60\u2192\ud835\udc56\ud835\udc57\ud835\udc470subscriptfor-all\ud835\udc56\ud835\udc57\\lambda^{s}_{i\\rightarrow j}(T)=0\\,\\,\\forall_{i,j}italic_\u03bb start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT ( italic_T ) = 0 \u2200 start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT, we reduced the complexity to being linear in the number of edges, same way as we did for messages. Additionally, in the directed case where (i,j)\u2208E\ud835\udc56\ud835\udc57\ud835\udc38(i,j)\\in E( italic_i , italic_j ) \u2208 italic_E, but (j,i)\u2209E\ud835\udc57\ud835\udc56\ud835\udc38(j,i)\\notin E( italic_j , italic_i ) \u2209 italic_E, one needs to rewrite Eq. (46) into: The above equations, used in all our experiments, are implemented in Julia [60] and available at [59]. Here, we present results on structure learning in the presence of unobserved nodes. We reproduce the experiment presented in Fig. 4, but in the most challenging setting where no information on the network topology is available, and the super-set of edges that we initially consider corresponds to a fully-connected graph. The results are given in Fig. 10, and are similar for all four synthetic network types.The edge reconstruction is always better than random guess, but we observe reduction of reconstruction quality with the increase of the number of unobserved nodes. This is expected due to the appearance of degeneracy where several models are consistent with the observation data. As a result SLICER produces a graph, which is compatible with the observed data and as such can be used as an effective model for prediction [8], even if it is not the true graph. Moreover, have in mind that our method recovers the network simultaneously with learning model parameters."
    },
    {
        "id": 8,
        "title": "Resource-Limited Automated Ki67 Index Estimation in Breast Cancer",
        "corpus": "The prediction of tumor progression and chemotherapy response has been recently tackled exploiting Tumor Infiltrating Lymphocytes (TILs) and the nuclear protein Ki67 as prognostic factors. Recently, deep neural networks (DNNs) have been shown to achieve top results in estimating Ki67 expression and simultaneous determination of intratumoral TILs score in breast cancer cells. However, in the last ten years the extraordinary progress induced by deep models proliferated at least as much as their resource demand. The exorbitant computational costs required to query (and in some cases also to store) a deep model represent a strong limitation in resource-limited contexts, like that of IoT-based applications to support healthcare personnel. To this end, we propose a resource consumption-aware DNN for the effective estimate of the percentage of Ki67-positive cells in breast cancer screenings. Our approach reduced up to 75%percent7575\\%75 % and 89%percent8989\\%89 % the usage of memory and disk space respectively, up to 1.5\u00d71.5\\times1.5 \u00d7 the energy consumption, and preserved or improved the overall accuracy of a benchmark state-of-the-art solution. Encouraged by such positive results, we developed and structured the adopted framework so as to allow its general purpose usage, along with a public software repository to support its usage. The nuclear protein Ki67 has been introduced as a proliferative marker to be used along with Tumor Infiltrating Lymphocytes (TILs) as a feature effectively driving the prediction of tumor progression and chemotherapy response (Taneja et al., 2010; Mao et al., 2016). Ki67 estimation in surgical pathology is a specific example of a task in which the result of the evaluation bears a significant clinical consequence in many cancer types. Indeed, the pathology practice considers different positivity cut-offs to discriminate between lesions with different overall prognosis or therapy response (Tao et al., 2017; Pena and Andrade-Filho, 2009). For this reason, an high accuracy in Ki67 estimation is essential to avoid over or under-grading of the scrutinized sample. In breast pathology this task may occupy a major role in the routine of pathologists given the high number of surgical samples produced daily in both referral and peripheral pathology centres, as a consequence of the effectiveness of the woman health screening policies in different countries. These tasks require the segmentation and visual count of cells by pathologists, which is a relatively time-consuming task subject to high intra- and inter-operator variability, related to the experience of each professional (Chung et al., 2016; Gomes et al., 2014). Furthermore, in the evaluation of all these features it should also be considered the presence of a different degree of intra-tumoral heterogeneity, depending on the cancer histotype and the type of tissue sample (Zilenaite et al., 2020). Thus, the development of automatic cell count approaches enables a faster and more reliable diagnosis/prognosis. Indeed, even if the evaluation of a single Ki67 slide may be a relatively fast operation, the in-series estimation of a large number of cancer samples is time consuming and may benefit of automated tools able to provide and integrate such data in the histopathology report, independently of the computational resources available. This latter aspect is also of critical importance, as it should be considered that even though screening programs are commonly employed at least in western countries, not every facility may dispose of enough computational resources to run Deep Learning (DL) software (Harbeck et al., 2019; Park et al., 2013). Along with the problem of Ki67 estimation, also the identification of TIL-score is relevant in breast cancer because it has a prognostic role, as a component of the immune system fighting tumor progression (Stanton and Disis, 2016). TIL estimation from histological images present challenges similar to those related to Ki67 (i.e. the task easily becomes time-consuming and operator-dependent), thus also this activity can take advantage from the application of DL. The successful results of deep convolutional models are not surprising, since these models proved to be one of the most effective and flexible tools to tackle healthcare problems (Bengio et al., 2017; Egger et al., 2022). These methods are currently applied to analyse a wide spectrum of medical data types, such as (I) clinical images, (II) biosignals, (III) high-dimensional omics data, and (IV) Electronic Health Records, achieving state-of-the-art performance on several tasks, including the recognition of mitoses, nodal metastases, or the measurement of prognostic markers (Hanna et al., 2021; Klein and Duda, 2021; Xu et al., 2021; Lee et al., 2021). The application of DL models is also pervasive in the analysis of breast cancer histopathological images, where it is effective to tackle different tasks: image classification (Debelee et al., 2020; Aljuaid et al., 2022; Meirelles et al., 2022), cell contours and nuclei detection (Mridha et al., 2021) and Ki67-index estimation. Regarding breast cancer image classification, state-of-the-art methods are based on convolutional neural networks. Nawaz et al. (Nawaz et al., 2018) proposed the use of the convolutional architecture DenseNet to perform multi-class classification of histopathological images from the BreakHis dataset (Spanhol et al., 2015), reaching an accuracy of 95.4%. Xie et al. (Xie et al., 2019) used the Inception_V3 and Inception_ResNet_V2 networks to classify breast cancer histopathological images through transfer learning, outperforming many previously proposed architectures (AlexNet, CSDCNN, LeNet). Zhu et al. (Zhu et al., 2017) used a fully-CNN exploiting convolution and deconvolution layers to directly regress a density map showing the position of cells. Interestingly, they evaluated the model also on histopathological breast lymph node images. Xue et al. (Xue et al., 2016) use AlexNet and ResNet with Euclidean loss to predict the number of cells in image patches. The use of DL for cell counting is so well-established in literature that an ImageJ plugin (Falk et al., 2019) now enables the use of U-Net models to perform cell detection on generic biomedical image data. Finally, the estimation of Ki67-score from immunohistochemical images is another task where DL is largely adopted. To this end, Saha et al. (Saha et al., 2017) initially proposed to use a Gamma mixture model with Expectation-Maximization to identify seed points and patch selection, which are fed to a CNN model having a custom decision layer based on decision trees. The method achieved 91% of F-score. Zhang et al. (Zhang et al., 2018a) explored the use of CNN for Ki67 image classification, where a probability heatmap can be obtained from repositioning the classified patches to their original location. The ratio of tumor cells in the heatmap can be used as an indicator for Ki67 expression. Moreover, they explored the use of single shot multibox detectors to detect Ki67 positive and negative cells. Fulawka et al. (Fulawka et al., 2022) applied a CNN DenseNet model with fuzzy interpretation to obtain a binary mask, which is then used to segment breast cancer cells and compute Ki67 index. Negahbani et al. (Negahbani et al., 2021) introduced a pipeline for cell classification and detection of both Ki67 and TILs that exploits a U-Net architecture with novel residual dilated inception modules (see section 2.2 for further details). However, the resources required to run DL applications are not trivial, being these models typically overparameterized (Allen-Zhu et al., 2019), and therefore requiring a high amount of computational resources. We refer here to the size of the learned DL models, and not to the ever-growing size of the datasets to be processed (Li et al., 2021), or the size of the single images, as these issues have been addressed in literature (see, e.g., (Hirahara et al., 2021) and the use of tiles/patches (Berman et al., 2021)). Just to state some examples, depending on their specific implementation, the space needed to store a CNN trained for image classification purposes, such as the previously mentioned DenseNet, ResNet, or AlexNet models, varies from tens to hundreds of megabytes; this requirement jumps up to several tens gigabytes if we consider modern Large Language Models (e.g., 11 billions of parameters for some variants of the T5 Text-To-Text Transformer Model (Raffel et al., 2020), meaning around 44444444 terabytes of RAM when using 4444 bytes per parameter). To face the issue related to the excessive resource usage, recent works focused on specifically designing deep models for mobile devices (Zhang et al., 2018b), which however, despite their effectiveness, is applicable to novel architectures but not to the existing ones. Indeed, most available DNNs have been pre-trained (often using a more than remarkable amount of resources) with the aim of deploying them onto standard computing facilities. Such aspect can easily hamper the adoption of DL-based medical software, especially on devices with limited resources, like smartphones or IoT-devices (Garc\u00eda et al., 2019; Liu et al., 2015; Abdellatif et al., 2020). Further, this is even more relevant in developing countries, characterized by poor economic conditions\u2014mainly in the rural areas\u2014and often even by shortage of physicians, which would highly benefit from AI systems to support clinical decisions. Low-resource DL models can improve the exploitation and applicability of the basic wearable networked devices available, and foster a speed-up toward revolutionary changes in the healthcare systems of these countries (Guo and Li, 2018). On the other side, cloud-based solutions might be used for hosting large DL models in order to query them online. However, apart privacy and security issues induced by sensible data, it would also require an active Internet connection, which might be a problem in poor rural areas. Moreover, Internet and cloud-based solutions have a cost which has to be taken into account, in relation with the (scarce) available budget. In such a context, this study proposes an automated AI method for cell classification and detection of Ki67 and TILs, with a particular attention to the resource usage. We show our solution to be competitive with the state-of-the-art solution, PathoNet (Negahbani et al., 2021), a CNN shown to be top-performing in classifying breast cancer images belonging to three classes Ki67- immunopositive, Ki67- immunonegative and tumor infiltrating lymphocytes, while reducing its RAM and disk requirements up to 4\u00d74\\times4 \u00d7 and 9\u00d79\\times9 \u00d7, respectively, along with its energy consumption. Moreover, once verified the effectiveness of our methodology, we have done a further step towards the realization of a more general framework to reduce the resource demand of existing pre-trained DL models. In this sense, PathoNet can be considered as a use case of such a methodology, relatively \u2018toy\u2019 for its memory size of around 13MB13\ud835\udc40\ud835\udc3513MB13 italic_M italic_B, but we will argument in the discussion at the end of the paper how recent researches already proposed DNNs for the same problem requiring several hundreds of megabytes of RAM. Summarizing, we provide a Python module to 1) first let the existing pre-trained model to undergo lossy compression of its layers, and then lossless storage of the compressed layers; 2) to run in main memory the model in the resource-saver format and efficiently querying it directly in such a format, without need to reconvert it to the original uncompressed format. The paper is organized as follow: Sect. 2 illustrates the used data, model and overall methodology. The obtained results are described in Sect. 3 and discussed in Sect. 4. Some concluding remarks end the paper. This section depicts the application of the proposed framework to the considered case study. More precisely, Sect. 2.1 describes the used dataset, while Sect. 2.2 illustrates the framework adopted to learn the low-resources model, as well as how to run queries in the compressed storage format. We requested to the authors the breast cancer invasive ductal carcinoma dataset (SHIDC-B-Ki-67-V1.0)111https://github.com/SHIDCenter/PathoNet, which is already divided in a train and a test set (containing 1656165616561656 and 700700700700 images, respectively). Each image is associated with a JSON file containing the nuclei coordinates (position on X and Y axes of the cell center) and the ground truth label for each nucleus (where 1=Ki67-immunopositive, 2=Ki67-immunonegative and 3=TIL). From the provided training set, we created a validation set randomly selecting 20%percent2020\\%20 % of the labeled images. We checked that the obtained train (composed by the remaining 80%percent8080\\%80 % of the images) and validation set had an average number of cells per image (i.e. \u201cavg./IMG\u201d) similar to the values published in Table 2 of the reference paper (Negahbani et al., 2021). The obtained values of avg./IMG are presented in Table 1. The train set undergoes data augmentation by flipping images w.r.t. X and Y axes and applying rotations, as done in the reference paper. Most memory and disk requirements of a convolutional neural network (CNN) are due to the storage of weight connections for each layer in the model. Thus, the compression of an existing CNN mainly maps to the problem of finding out a succinct approximation \ud835\udc7e\ud835\udc7e\\boldsymbol{W}bold_italic_W of a given matrix/tensor \ud835\udc7eosuperscript\ud835\udc7eo\\boldsymbol{W}^{\\mathrm{o}}bold_italic_W start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT representing the learned connection weights of a network layer. The compression ratio is defined as the ratio \u03c8\ud835\udf13\\psiitalic_\u03c8 between the sizes of the uncompressed and compressed matrix, that is \u03c8=size(\ud835\udc7eo)/size(\ud835\udc7e)\ud835\udf13sizesuperscript\ud835\udc7eosize\ud835\udc7e\\psi=\\mathrm{size}(\\boldsymbol{W}^{\\mathrm{o}})/\\mathrm{size}(\\boldsymbol{W})italic_\u03c8 = roman_size ( bold_italic_W start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT ) / roman_size ( bold_italic_W ), where size(x)size\ud835\udc65\\mathrm{size}(x)roman_size ( italic_x ) is the memory size of x\ud835\udc65xitalic_x. In general, uppercase boldface symbols will denote matrices, and the corresponding lowercase letters will refer to matrix entries (e.g., wosuperscript\ud835\udc64ow^{\\mathrm{o}}italic_w start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT will be an entry of \ud835\udc7eosuperscript\ud835\udc7eo\\boldsymbol{W}^{\\mathrm{o}}bold_italic_W start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT). Vectors\u2014precisely, row vectors\u2014will be rendered using italic boldface (e.g., \ud835\udc99\ud835\udc99\\boldsymbol{x}bold_italic_x). The first phase of the proposed methodology concerns the reuse of an existing pre-trained model, as common nowadays given the large number of publicly available pre-trained models developed for various applications. To show the power and flexibility of the proposed approach we thereby compress the pre-trained PathoNet network, retrieved from the same repository holding the previously mentioned data. PathoNet is a CNN proposed for the accurate detection and count of tumoral cells, appropriately stained for Ki67 and TILs, from biopsy images of malignant breast tumors. The cells present in these images have been labelled as Ki67 positive tumor cells, Ki67 negative tumor cells, and lymphocytes infiltrating the cancer area. The model is designed to detect and classify cells according to these three classes (Negahbani et al., 2021). To deal with the size variability of cells from image to image, PathoNet is mostly composed of dilated inception layers. On the one hand, this solves the problem of choosing a fixed kernel size by using different kernel sizes in one module; on the other one, it enlarges the network structure without suffering from the vanishing gradient problem when increasing the number of kernels. In particular, residual dilated inception modules (RDIMs) are used in the encoder and decoder part of the network. Each of these modules consists of two parallel parts, the first composed by two stacked convolutional layers with kernel size 3333, and the second built by stacking two 3\u00d73333\\times 33 \u00d7 3 dilated convolutional layers with dilation 4. To reduce the number of parameters, and consequently the possibility of overfitting, the outputs of these two parts are not concatenated but summed up. Overall, PathoNet utilizes a U-Net-like structure (Ronneberger et al., 2015), where most convolutional layers are replaced by RDIMs. The structure of the network is the following: the input layer is initially processed using two convolutional layers, in turn stacked over four encoder RDIMs and four decoder RDIMs; the latter are followed by three 1\u00d71111\\times 11 \u00d7 1 convolutional layers with linear activation function, used to produce the three-channel output of the model (see (Negahbani et al., 2021), Figure 5, for a visual representation). This step is crucial to achieve good compression ratios while not affecting the model accuracy. Among the vast plethora of compression techniques proposed in literature (see, e.g. (Deng et al., 2020) for a survey), we designed and exploited the most suitable techniques for the structure of PathoNet. We did not consider weight or structural pruning, since the model network structure is already tuned by the authors, with a well-conceived and precise organization of the individual blocks. Weight pruning, for instance, is rewarded when attaining high sparsity levels (>0.5absent0.5>0.5> 0.5), which in turn allow the usage of compressed formats such as CSC. However, an excessive pruning of convolutional layers can highly deteriorate the predictive capabilities of the model (Marin\u00f2 et al., 2023). Additionally, our aim is also to not slower the model execution, along with obtaining its space reduction. Accordingly, we operated a weight quantization of convolutional layers, consisting in building the matrix \ud835\udc7e\ud835\udc7e\\boldsymbol{W}bold_italic_W using a limited number of distinct weights, each represented using less bits than each entry in \ud835\udc7eosuperscript\ud835\udc7eo\\boldsymbol{W}^{\\mathrm{o}}bold_italic_W start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT. The idea is to cast connection weights into categories and substituting all weights in a category with a representative. This approach, named weight sharing (WS), allows to save room when combined with the Index Map storage format, which consists in replacing weights with their category/representative index, at the cost of storing separately the vector of representative weights. The advantage of this format is that it only adds one level memory access and almost preserves the dot time efficiency of the original model. It is worth pointing out that in case the model to compress contains also other types of layers, e.g., fully-connected layers, specific and more suitable lossy compression and lossless storage approaches can be leveraged, exploiting for instance pruning+quantization compression and address map storage (Marin\u00f2 et al., 2021, 2021). The strategy used to group weights and to select representatives distinguishes the four state-of-the-art quantization algorithms considered in this study and described in the following. Clustering-based WS (CWS). This strategy groups weights into k\ud835\udc58kitalic_k clusters via the k\ud835\udc58kitalic_k-means algorithm (McQueen, 1967), and uses the resulting centroids {c1,\u2026,ck}subscript\ud835\udc501\u2026subscript\ud835\udc50\ud835\udc58\\{c_{1},\\dots,c_{k}\\}{ italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } as representatives to replace the weights in the corresponding cluster (Han et al., 2016). Probabilistic WS (PWS). This technique is based on the Probabilistic Quantization method (Marin\u00f2 et al., 2021), in which a randomized algorithm transforms each weight wo\u2208\ud835\udc7eosuperscript\ud835\udc64osuperscript\ud835\udc7eow^{\\mathrm{o}}\\in\\boldsymbol{W}^{\\mathrm{o}}italic_w start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT \u2208 bold_italic_W start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT in one of k\ud835\udc58kitalic_k distinct representatives c1,\u2026,cksubscript\ud835\udc501\u2026subscript\ud835\udc50\ud835\udc58c_{1},\\ldots,c_{k}italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. A nice feature of this method consists in the fact that the obtained \ud835\udc7e\ud835\udc7e\\boldsymbol{W}bold_italic_W can be seen as the value of an unbiased estimator for \ud835\udc7eosuperscript\ud835\udc7eo\\boldsymbol{W}^{\\mathrm{o}}bold_italic_W start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT (see (Marin\u00f2 et al., 2021) for details). Uniform Quantization (UQ). In this scheme, which achieved top compression performance in recent applications to CNNs compression (Choi et al., 2020), representatives are selected by uniformly partitioning the entire weight domain into k\ud835\udc58kitalic_k subintervals222Note that the actual number of subintervals k\ud835\udc58kitalic_k can be lower than the input value due to the internal selection of the \u03b4\ud835\udeff\\deltaitalic_\u03b4 hyperparameter of the method (see (Choi et al., 2020) for further details).. Such a selection has been proven to yield an output entropy which is asymptotically smaller than that of any other quantizer, regardless of the source statistics, when the source density function is sufficiently smoothed (Gish and Pierce, 1968). Entropy Constrained Scalar Quantization (ECSQ): This is a technique leveraging an iterative optimization algorithm to determine the optimal number of groups. It is driven by the joint optimization of the expected value for the quantization distortion (a measure of the distance between \ud835\udc7eosuperscript\ud835\udc7eo\\boldsymbol{W}^{\\mathrm{o}}bold_italic_W start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT and \ud835\udc7e\ud835\udc7e\\boldsymbol{W}bold_italic_W) and the entropy of the resulting discrete distribution for representative weights (Chou et al., 1989). A retraining phase is finally applied after quantization, ensuring that the updated weights always assume values in the set of representatives (Han et al., 2016). The third phase of our framework is the design of a suitable room-saver format for the model, tailored for the compression schemes used in the previous step, and able to perform the model execution without re-expanding the network. As mentioned above, an efficient solution to exploit the quantized tensors is represented by the Index Map (IM) format (Han et al., 2016). Representatives are stored in a vector \ud835\udc84={c1,\u2026,ck}\ud835\udc84subscript\ud835\udc501\u2026subscript\ud835\udc50\ud835\udc58\\boldsymbol{c}=\\{c_{1},\\ldots,c_{k}\\}bold_italic_c = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }, whose indices are entries of a new matrix/tensor \ud835\udc74\ud835\udc74\\boldsymbol{M}bold_italic_M. Thus, if wo\u2208\ud835\udc7eosuperscript\ud835\udc64osuperscript\ud835\udc7eow^{\\mathrm{o}}\\in\\boldsymbol{W}^{\\mathrm{o}}italic_w start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT \u2208 bold_italic_W start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT is associated with centroid, say, c2subscript\ud835\udc502c_{2}italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, then the corresponding entry in \ud835\udc74\ud835\udc74\\boldsymbol{M}bold_italic_M is set to 2222. When \ud835\udc7eosuperscript\ud835\udc7eo\\boldsymbol{W}^{\\mathrm{o}}bold_italic_W start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT (and accordingly \ud835\udc74\ud835\udc74\\boldsymbol{M}bold_italic_M) has dimension n\u00d7m\ud835\udc5b\ud835\udc5an\\times mitalic_n \u00d7 italic_m, denoted by b\ud835\udc4fbitalic_b and b\u00af\u00af\ud835\udc4f\\bar{b}over\u00af start_ARG italic_b end_ARG the number of bits used to store one entry of \ud835\udc7eosuperscript\ud835\udc7eo\\boldsymbol{W}^{\\mathrm{o}}bold_italic_W start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT and of \ud835\udc74\ud835\udc74\\boldsymbol{M}bold_italic_M, respectively, the compression ratio obtained is: For instance, when k\u2264256\ud835\udc58256k\\leq 256italic_k \u2264 256, b\u00af=8\u00af\ud835\udc4f8\\bar{b}=8over\u00af start_ARG italic_b end_ARG = 8 is enough to represent 28=256superscript282562^{8}=2562 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT = 256 different indices, and assuming a typical FP32 format for \ud835\udc7eosuperscript\ud835\udc7eo\\boldsymbol{W}^{\\mathrm{o}}bold_italic_W start_POSTSUPERSCRIPT roman_o end_POSTSUPERSCRIPT (b=32\ud835\udc4f32b=32italic_b = 32), the compression ratio would be \u03c8\u22484\ud835\udf134\\psi\\approx 4italic_\u03c8 \u2248 4. We remark that this format does not induce any information loss, while needing only one additional memory access to retrieve a given weight. The final step of our framework is the computation of matrix/tensor products directly in the compressed format used at previous step. Without loss of generality, we can assume that the layer weights are represented by a matrix. To compute the output \ud835\udc90=\ud835\udc99\u22c5\ud835\udc7e\ud835\udc90\u22c5\ud835\udc99\ud835\udc7e\\boldsymbol{o}=\\boldsymbol{x}\\cdot\\boldsymbol{W}bold_italic_o = bold_italic_x \u22c5 bold_italic_W of a given compressed layer with weight matrix \ud835\udc7e\ud835\udc7e\\boldsymbol{W}bold_italic_W on the input \ud835\udc99\ud835\udc99\\boldsymbol{x}bold_italic_x, that is oi=\u2211jxjwjisubscript\ud835\udc5c\ud835\udc56subscript\ud835\udc57subscript\ud835\udc65\ud835\udc57subscript\ud835\udc64\ud835\udc57\ud835\udc56o_{i}=\\sum_{j}x_{j}w_{ji}italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_j italic_i end_POSTSUBSCRIPT using IM format, we perform oi=\u2211jxjcmjisubscript\ud835\udc5c\ud835\udc56subscript\ud835\udc57subscript\ud835\udc65\ud835\udc57subscript\ud835\udc50subscript\ud835\udc5a\ud835\udc57\ud835\udc56o_{i}=\\sum_{j}x_{j}c_{m_{ji}}italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_j italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT. From a memory consumption standpoint, this does not need to expand the compressed matrix, and it still keeps the model memory footprint \u03c8\ud835\udf13\\psiitalic_\u03c8 times smaller than the original one. The PathoNet source code was implemented in Python 3, using Tensorflow and Keras. Our compression techniques and the retraining procedures have been implemented in the same programming environment. Our implementation, available on GitHub333https://github.com/GliozzoJ/pathonet_compression, allows: (i) to perform the compression and retraining of PathoNet with different quantization techniques; (ii) to compute the compression ratio (relative to the memory space); (iii) to evaluate the compressed model space on disk and the prediction time ratio compared to the uncompressed version of PathoNet. The repository also contains a Jupyter notebook allowing the replication of our results via direct execution of the compressed models relying on IM representation and another notebook to estimate energy consumption. We tested the four considered quantization approaches (i.e., CWS, PWS, UQ, and ECSQ; see Sect. 2.2) on the breast cancer dataset provided by the authors of PathoNet. As mentioned in Sect. 2.1, we executed an holdout procedure in which the training set is exploited to retrain the model after quantization and a validation set is used to tune a set of hyperparameters by means of grid search. Indeed, the approaches adopted to compress the network present some hyperparameters that influence the obtained performance. One of them is the number k\ud835\udc58kitalic_k of groups, which has a direct effect on the final size of the compressed model. Moreover, since we have to retrain the network after the quantization step, all the classical hyperparameters related to the training of a neural network (e.g., learning rate, batch size, patience, etc.) play an important role. In particular, we have considered the cumulative learning rate clr\ud835\udc50\ud835\udc59\ud835\udc5fclritalic_c italic_l italic_r and the number of groups k\ud835\udc58kitalic_k, since they impacted more on the model accuracy. Once the best model is selected (see Sect. 2.1), the corresponding hyperparameters are used to train the network on the train and validation sets, and the generalization performance of the compressed model is assessed on the test set. The successful application of a compression strategy should lead to a model that (I) retains similar generalization performance w.r.t. the uncompressed model, (II) leads to a reduction in terms of space occupancy, and (III) keeps a reasonable execution time when used to make inferences on the test set. In particular, the generalization performances are evaluated in terms of F1-score, RMSE (Root Mean Squared Error) and aggregated cut-off accuracy for TILs and Ki67 (as defined in the reference paper). Moreover, two additional metrics are used: Compression ratio: the ratio of the memory size needed by the uncompressed over the compressed model (cfr. Sect. 2.2); Time ratio: the ratio between evaluation times on the test set of the uncompressed over the compressed model. The train and validation sets (Section 2.1) are used to perform the tuning of hyperparameters, i.e., the cumulative learning rate clr\ud835\udc50\ud835\udc59\ud835\udc5fclritalic_c italic_l italic_r for the fine tuning of weights after quantization (Section 2.2), and number of clusters k\ud835\udc58kitalic_k, by means of grid search. Obviously, the validation set is not augmented in this phase. The best combination of hyperparameters is the one that gives the lowest RMSE on the validation set. Then the best model is retrained on the complete augmented training set. As outlined in (Marin\u00f2 et al., 2021), the cumulative learning rate needs to be smaller than the learning rate used to train the original model: accordingly, the grid for clr\ud835\udc50\ud835\udc59\ud835\udc5fclritalic_c italic_l italic_r has been set to [0.001,0.0001,0.00001,0.000001]0.0010.00010.000010.000001[0.001,0.0001,0.00001,\\linebreak 0.000001][ 0.001 , 0.0001 , 0.00001 , 0.000001 ]. The number of groups k\ud835\udc58kitalic_k has been chosen in [256,1024,4096]25610244096[256,1024,4096][ 256 , 1024 , 4096 ] for all compression methods, with the first choice ensuring using 1 byte for each index, and the other 2 choices ensuring lower compression but potentially higher performance. Note that this bidimensional grid yields 12121212 combinations for each method, for a total of 48484848 experiments; this prevented us from using more refined grids. After choosing the best couple, a final retraining using all the augmented training set produces the compressed model. The cumulative learning rate and the number of groups selected at the end of tuning are showed in Table 2. The network configuration was kept as in the original PathoNet model, whenever possible, and all the other experimental details are reported in our public repository444https://github.com/GliozzoJ/pathonet_compression. The generalization performance, in terms of F1-score for the three classes, RMSE and aggregated cut-off accuracy for Ki67-index and TIL-score is presented in Table 3 and compared to the same metrics computed on the original uncompressed PathoNet model 555The results reported for the uncompressed network in Table 3 are different from the ones showed in the reference paper (Negahbani et al., 2021). We contacted the authors of PathoNet and they agreed with the correctness of the F1-score results using dataset SHIDC-B-Ki-67-V1.0. Moreover, we implemented the function to compute the RMSE and cut-off accuracy, which is now part of the PathoNet package available on GitHub (https://github.com/SHIDCenter/PathoNet/blob/master/evaluation.py).. As we can see from the results of this first set of experiments (rows 2\u20135 in Table 3), the compressed networks achieve comparable performance w.r.t. the original PathoNet model in terms of F1-score for the classes Ki67 immunopositive and immunonegative. Interestingly, the compression methods UQ and ECSQ obtain a significant improvement in F1-score for the TIL class (3%percent33\\%3 % for UQ and slightly lower for ECSQ). Considering RMSE, the performances of compressed networks are slightly worse for the Ki67-index, but they achieve always better results for the TIL-score. Moreover, all the compressed networks match the uncompressed ones for the Ki67-index cut-off accuracy while always consistently improving the corresponding metric for TIL-score. In particular, the improvement in cut-off accuracy for TIL-score ranges from 4.4%percent4.44.4\\%4.4 % to 13.1%percent13.113.1\\%13.1 %, depending on the applied quantization approach. Overall, all the compressed models almost halve the space occupancy in RAM while bringing a slow down during execution of less than 20%percent2020\\%20 %. As shown in Table 2, the grid search process led to the selection of an high number of groups k\ud835\udc58kitalic_k, which was equal to 4096 in most cases. The performance of the compressed models is competitive with the original PathoNet model, and some metrics are often better (especially the ones related to TILs). This behaviour is expected, since an higher number of representatives gives better chances to preserve the network structure. On the other hand, it is interesting to evaluate if a lower number of representatives can lead to competitive results while significantly reducing the space occupancy in main memory. From a practical standpoint, a user could have only limited computational resources available and willing to still use a compressed CNN even at the expenses of a marginal decrease in generalization performance. To test this situation, we executed again the experiments performed in the previous section but avoiding the model selection process. In particular, the number of groups is fixed to 256256256256 for all quantization methods and the cumulative learning rate as the best value selected by means of grid search in the previous set of experiments (see Table 2). The other hyperparameters remained unchanged. The results are showed in Table 3 (rows 6\u20139). Quite surprisingly, compressed PathoNet models in this setting tend to preserve or even improve their performance when using more representatives (see, e.g., the CWS method), which has the appreciated benefit that the space compression is still increased, namely to \u22484\u00d7\\approx 4\\times\u2248 4 \u00d7 the original uncompressed PathoNet model. An exception is the PWS method, which shows a lower ability to select informative representatives when their number is limited, confirming the results in (Marin\u00f2 et al., 2023). On the other side, CWS method exhibited an overall higher stability and effectiveness in choosing the representative weights, as confirmed by the the fact that it performed best during model selection when not using the maximum k\ud835\udc58kitalic_k allowed (Table 2). In this research, we focused on the problem of the automatic computation of Ki67 and TIL indices in breast cancer, with the primary goal of at least preserve the top performance obtained for this task in the literature, along with a special attention to limit the resulting model resource usage, to not hamper its practical applicability. We showed how to obtain a CNN exhibiting performance competitive with PathoNet (reference CNN for the problem), while yielding a model much more resource-cautious, through a novel compression framework. Our solution is around 4\u00d74\\times4 \u00d7 smaller in terms of memory footprint and 9\u00d79\\times9 \u00d7 in terms of disk size (Figure 1), with reference to PathoNet, while still performing the same or better. This is at first of clinical interest, being Ki67 pivotal in the definition of patients\u2019 treatment and for the evaluation of their prognosis (Dowsett et al., 2011). Similarly, the number of TILs show a positive correlation with patients survival and therapy response. In fact at least in certain subtypes of breast cancer, a higher number of TILs indicates a higher activation of tumor-suppressing adaptive immunity and an increased rate of response after adjuvant anthracycline-based chemotherapy (Salgado et al., 2015). For example, in lung cancer, the evaluation of PDL-1 immunohistochemical positivity became critical after the demonstration of the efficacy of anti-PDL1 drug Pembrolizumab (Gandhi et al., 2018). The inclusion of patients in immunochemotherapy protocols with Pembrolizumab monotherapy or its combinations passes through PDL1 positive cell count, since inter-observer variability can easily shift the therapeutic plan due to the low amount of positive cells needed to reach the TPS cut-off (Cooper et al., 2017). Indeed, attempts to use DNN for TPS computation in lung cancer already exist in literature (Wang et al., 2021). Moreover, given the growing knowledge about disease molecular therapy targets and interest in Precision Medicine, a future increase in the number of routinely analyzed immunohistochemical predictive and prognostic markers becomes a reasonable educated guess (Fassan et al., 2020). In this context, the application of ML-based models on prognostic/predictive immunohistochemical panels will shorten the time required for their evaluation, and increase the overall accuracy of the tests. The employment of such models will likely reduce the costs and time related to the evaluation of each biomarker, therefore mitigating the pathologist workload; reducing the resource demand of such computational models will thereby still favour their applicability. Secondly, apart the relevance of cell count operation emphasized so far, our work induces a second benefit related to the model compression, in terms of both RAM and disk occupancy reduction. In general, the disk space required to store a CNN is lower than the amount of RAM needed to load and query the model, due to optimized formats available to serialize models on disk (e.g., the disk serialization provided by the lzma Python module666https://docs.python.org/3/library/lzma.html). The disk storage reduction (up to 9\u00d79\\times9 \u00d7, UQ quantization method), represents an advantage in situations in which the serialized representation of models is used to share the latter among several actors in a distributed framework. This happens notably in the federated learning setting (Li et al., 2020), characterized by a privacy-preserving communication loop in which edge computing devices train \u201cpartial\u201d machine learning models on locally acquired data and send them to a centralized server that merges them and shares with all devices the resulting \u201cglobal\u201d model. Having the possibility to send compressed models back and forth would lead in this case to a valuable saving of network bandwidth, which is the bottleneck resource. Motivated by the promising results obtained, we have made our compression methodology ready and general enough to be applied in other domains with similar needs. In this sense, PathoNet can be seen as a \u2018toy example\u2019 for stating the usefulness of our methodology, being its memory footprint slightly less than 13 MB, taking into account 4 bytes for each model parameter as a floating point number. Nevertheless, for the computation of Ki67-index current researches have also proposed much bigger models: see e.g. (Fulawka et al., 2022), where an ensemble of three DenseNet121 models is used to provide a final prediction, which roughly requires 320 MB of RAM. Although in small scale, we give an idea of how much the compression of PathoNet impacts on the energy consumption. We leveraged the Python package codecarbon and obtained 0.0005730.0005730.0005730.000573 kWh used for querying the original model on 500500500500 images, and 0.0003800.0003800.0003800.000380 kWh to query the compressed one (i.e. UQ, k=256\ud835\udc58256k=256italic_k = 256).777Energy consumption was computed on Linux-6.2.6-76060206-generic-x86_64-with-glibc2.35, CPU Intel(R) Core(TM) i7-9750HF 2.60GHz, GPU NVIDIA GeForce RTX 2060. Our repository provides a dedicated Jupyter notebook to specifically compare the energy consumption (see Sect. 2), in addition to the code able to reproduce all the experiments proposed in this study, and to the scripts allowing to automatically load serialized saved models, deserialize them, and run their execution. [Disk compression of tested approaches.]Uniform Quantization using 256 groups allows to obtain a model 9 times smaller with respect to the original Pathonet model on disk. Clustering-based weight sharing , Probabilistic weight sharing and Entropy Constrained Scalar Quantization using 256 groups lead to a 4 times reduction in disk usage. Finally, the methods considering a tuned value for the number of groups achieve a reduction in disk usage of 4 times for Uniform Quantization and Clustering-based weight sharing, while Probabilistic weight sharing and Entropy Constrained Scalar Quantization obtains a reduction of 2 times. This study introduces an automated DL approach for the Ki67 - immunonegative, Ki67 - immunopositive, and TILs cell detection on stained images, with a specific attention to the resulting model resource demand. Our framework exhibits performance competitive with state-of-the-art models for the estimation of Ki67 and TIL indices in breast cancer. Further, it successfully tackles the problem of reducing model size and computational resource need, to enhance its applicability in low-resources context, and to limit the energy consumption. With reference to a state-of-the-art top-performing solution, we obtained a model up to 4\u00d74\\times4 \u00d7 and 9\u00d79\\times9 \u00d7 smaller in terms of RAM and disk space respectively, while reducing the energy consumption of around 1.5\u00d71.5\\times1.5 \u00d7, and substantially preserving classification accuracy. Once favorably demonstrated the effectiveness of our approach in the estimation of Ki67 and TIL scores, we have done a further step to settle a general framework for compressing pre-trained DL models, that coupled with a publicly available repository, allows to extend our approach to other problems and deep models. This study serves thereby as a potential reference for non-expert users who need to downsize existing AI tools based on deep or convolutional neural networks, avoiding building compact DNNs from scratch, mainly for problems where the training has very high costs. Our solution allows to extend existing models to contexts where sufficiently powerful hardware is not available or where devices have inherently few computational resources. Our pipeline can handle both fully-connected and convolutional layer in compression step, and it is not limited to a given storage format in the final compressed layer representation step. The main limitation of the overall framework lies in its applicability to only deep models (which however cover the majority of application domains), and among them, to feed-forward architectures, excluding for instance recurrent neural networks (RNNs), just to state an example. A potential future development of this study would indeed extend the present solution to also support such models."
    },
    {
        "id": 9,
        "title": "Distributional Reinforcement Learning-based Energy Arbitrage Strategies in Imbalance Settlement Mechanism",
        "corpus": "Growth in the penetration of renewable energy sources makes supply more uncertain and leads to an increase in the system imbalance. This trend, together with the single imbalance pricing, opens an opportunity for balance responsible parties (BRPs) to perform energy arbitrage in the imbalance settlement mechanism. To this end, we propose a battery control framework based on distributional reinforcement learning (DRL). Our proposed control framework takes a risk-sensitive perspective, allowing BRPs to adjust their risk preferences: we aim to optimize a weighted sum of the arbitrage profit and a risk measure while constraining the daily number of cycles for the battery. We assess the performance of our proposed control framework using the Belgian imbalance prices of 2022 and compare two state-of-the-art RL methods, deep Q learning and soft actor-critic. Results reveal that the distributional soft actor-critic method can outperform other methods. Moreover, we note that our fully risk-averse agent appropriately learns to hedge against the risk related to the unknown imbalance price by (dis)charging the battery only when the agent is more certain about the price. Keywords Battery energy storage systems (BESS) \u22c5\u22c5\\cdot\u22c5 Distributional soft actor-critic (DSAC) \u22c5\u22c5\\cdot\u22c5 Imbalance settlement mechanism \u22c5\u22c5\\cdot\u22c5 Reinforcement learning (RL) \u22c5\u22c5\\cdot\u22c5 Risk-sensitive energy arbitrage Climate change has been a motivation for transitioning toward a decarbonized electricity grid on both the supply and the demand side. The European Commission aims to reach carbon neutrality by 2050 [1]. To achieve this target, the penetration of renewable energy sources (RES) needs to dramatically increase. The International Renewable Energy Agency\u2019s report of 2023 states that the total power capacity of RES in the world grew from 1.57 TW in 2013 to 3.37 TW in 2022 [2]. However, this trend makes electricity generation more uncertain due to the dependence of RES production on weather conditions. Consequently, the increase in the share of RES leads to an increase in the mismatch between generation and consumption. Given this potentially increasing mismatch between production and consumption, transmission system operators (TSOs) are facing challenges in maintaining the balance of the grid. Following the liberalization of the European electricity system, the balancing responsibility of TSOs has been outsourced to balance responsible parties (BRPs) [3]. Each unbalanced BRP is penalized by an imbalance price at the end of each imbalance settlement period. According to the electricity balancing guideline (EBGL), published by the European network of transmission system operators for electricity (ENTSO-E), the main objective of the imbalance settlement mechanism is to make sure that BRPs support the system balance in an efficient way and to stimulate market participants in restoring the system balance [4]. Also, EBGL states that a single imbalance pricing method should be used to calculate the imbalance cost: the settlement price should be the same for both negative and positive imbalances. Such a single imbalance pricing encourages BRPs to deviate from their day-ahead nomination to help the TSO with balancing the grid and to reduce their cost. The wide usage of RES in addition to the single imbalance pricing provides an opportunity for BRPs to reduce their cost using an arbitrage strategy in the imbalance settlement mechanism. For this purpose, recently battery energy storage systems (BESS) have attracted the attention of BRPs due to their fast response time [5], high efficiency [6], and significant decreases in cost of recent battery technology [7]. Energy arbitrage in the imbalance settlement mechanism is challenging because of high uncertainties in imbalance price and near real-time decision-making. Due to these mentioned challenges, as well as the recent change in the imbalance pricing methodology, few research works have been conducted on the arbitrage in the imbalance settlement mechanism [8, 3, 9, 10, 11, 12, 13]. Although most of the cited studies have formulated control strategies for BESS using model-based optimization methods (such as stochastic optimization and robust optimization), we argue that model-based optimization methods are not the most appropriate for obtaining an arbitrage strategy. Although it is possible, to formulate the energy arbitrage problem as a nonlinear programming problem, because of the nonconvex nature of such nonlinear problems, there is no efficient way to find the optimal solution for them [14]. Hence, linearization techniques (such as piecewise linear approximation) are applied to transform a nonlinear problem into a linear or mixed-integer convex problem. However, applying these techniques may result in an intractable optimization problem or an inaccurate approximation of the problem. Moreover, these model-based methods need a (probabilistic) forecaster for future imbalance prices to address uncertainty in future prices. In stochastic optimization, such uncertainties can be handled by generating a set of scenarios. Yet, as imbalance prices are highly uncertain, a large number of scenarios are required to correctly reflect the imbalance price distribution, which increases the computational burden to the extent that the problem may become computationally intractable. On the other hand, although robust optimization does not need as many scenarios [15], its obtained solution might be a very conservative strategy and not necessarily the most economical one [16]. Given the above challenges, few research works have focused on risk management in the arbitrage problem in the imbalance settlement mechanism. Generally, market participants have different risk preferences. For example, BRPs have more conservative arbitrage strategies in the imbalance settlement mechanism because of highly volatile imbalance prices. In other words, BRPs assign higher weights to scenarios with lower revenues and deviate from risk-neutral decision-making. Thus, to provide a more practical solution, a risk-averse perspective needs to be considered in the arbitrage strategy, while the previous studies have ignored risk management. Moreover, a battery\u2019s lifetime mainly depends on its charging/discharging operations. Frequently switching between charging and discharging can significantly reduce the battery cycle life and thus decrease the net profit due to an increased operational cost of the BESS. We note that existing works have not investigated the impact of the battery cycle life on the arbitrage strategy. In summary, shortcomings and weaknesses in previous studies of arbitrage strategies are that: (i) adopt model-based methods (which are complex to solve) (ii) do not consider a risk-sensitive perspective; and (iii) neglect battery cycle life constraints. To address these shortcomings (further elaborated in Section 2), in this paper, we propose a distributional reinforcement learning (DRL)-based control framework for a risk-sensitive energy arbitrage strategy in the imbalance settlement mechanism for BESS. The proposed control framework (Section 3) aims to maximize the arbitrage profit as well as a risk measure by constraining the daily number of cycles for the battery. We believe DRL methods are proper methods for risk management, since they learn the complete probability distribution of random returns instead of the expected return. The proposed control framework can be tuned according to the risk preference of BRPs from a fully risk-averse perspective to a fully risk-seeking one. In this paper, we start from two state-of-the-art reinforcement leaning (RL) methods, i.e., deep Q learning (DQN), as a value-based method, and soft actor-critic (SAC), as a policy gradient method. We extend these vanilla DQN and SAC methods with a distributional perspective (i.e., DDQN, DSAC) and a risk-aware component in the loss function (Section 4). The performance of the proposed control framework is evaluated on the Belgian imbalance prices of 2022 (Sections 5 and 6). Overall, our contributions in this paper are that we propose a DRL-based control framework for a BESS while considering a constraint on the daily number of cycles; that achieves a risk-sensitive arbitrage strategy with a tunable risk tolerance by optimizing a weighted sum of the arbitrage revenue and a risk measure in the imbalance settlement mechanism; for which we compare the performance of value-based and policy gradient RL methods in a highly uncertain trading market. Energy arbitrage is a technique to achieve financial profits by purchasing energy when the price is cheap and selling it when the price is expensive [17]. Energy arbitrage can be performed within one electricity market (e.g., day-ahead market [18] or intra-day market [19]) to take advantage of varying prices at different hours. Moreover, energy arbitrage strategies between several electricity markets have been developed to benefit from a price difference between two or more electricity markets, for instance, energy arbitrage between day-ahead and intra-day markets [20], day-ahead and real-time markets [12], day-ahead market and imbalance settlement mechanism [21], or day-ahead, intra-day, and real-time markets [22]. The recent change in the imbalance price calculation [4] and an increase in imbalance prices have opened up a new arbitrage opportunity in electricity markets. Figure 1 demonstrates the rise in Belgian imbalance prices in recent years. However, only few studies have been conducted on energy arbitrage in the imbalance settlement mechanism, due to the high risk involved in this arbitrage. In [8], a novel real-time stochastic multi-period management strategy for a virtual power plant was proposed to maximize the profit of a virtual power plant as well as minimize the operational grid cost. It solves a sequential stochastic optimization problem to manage the participation of a BESS in the real-time market. The authors in [3] first implement a new tailored encoder-decoder architecture to generate improved probabilistic forecasts of the future system imbalance. Then, they solve a bi-level robust optimization problem to maximize the revenue from the participation of a BESS in the imbalance settlement. The authors in [9] introduce a novel stochastic model predictive control (MPC) approach to optimize the revenue of BESS in the imbalance settlement mechanism by taking into account battery degradation costs and risk aversion. More specifically, an attention-based recurrent neural network is used to predict the system imbalance and imbalance price. In [10], first, the maximum potential profit from the real-time market is obtained using a linear optimization program with the assumption of perfect foresight for future prices. Then, a shrinking-horizon control algorithm is developed to obtain the energy arbitrage strategy for BESS in the real-time market by considering forecast errors in future real-time prices. Reference [11] proposes a stochastic model to maximize the energy arbitrage revenue of BESS under uncertainty in day-ahead and real-time markets. A hybrid stochastic-robust optimization method is proposed in [12] to maximize the revenue of BESS participants in day-ahead and real-time markets. The day-ahead market problem is solved by stochastic optimization, while the bidding and offering strategy in the real-time market is determined by robust optimization. The authors in [13] proposes control strategies for seasonal thermal energy storage systems to interact with day-ahead and imbalance markets: MPC-based and RL-based controllers are developed for each market interaction to compare the performance of these two controllers in the different electricity markets. Thus, most previous research works have applied model-based optimization methods to solve the arbitrage problem [8, 3, 9, 10, 11, 12]. Nonetheless, the main disadvantage of these methods is that they require linearization techniques to approximate the nonlinear problem as a linear (or mixed-integer) convex problem that can result in an inaccurate approximation. Due to partially known model parameters and uncertainties of the real electricity market, the real market is simplified into a convex market model, resulting in an inaccurate approximation of the real market dynamics [23]. Furthermore, stochastic optimization can be time-consuming, while robust optimization leads to overly conservative strategies. Another limitation of the mentioned studies is that only one of them [9] proposes a risk-sensitive arbitrage strategy. Moreover, the previous research works [8, 3, 10, 11, 12] ignore the effect of a battery\u2019s lifetime on the arbitrage strategy. To avoid problems of model-based optimization methods, RL methods can be used. Recently, RL, as a model-free method, has attracted researchers\u2019 attention due to its remarkable performance in solving complex sequential decision-making problems such as playing games, robotic control, and autonomous driving. RL can learn a (near-)optimal policy for a stochastic nonlinear environment by directly interacting with the environment [24]. In RL, there is no special hypothesis regarding the reward function; it can be linear or nonlinear. In contrast to model-based optimization methods, model-free RL methods do not need prior knowledge or an explicit model of the environment. The agent, by interacting with the environment, captures uncertainties and estimates system dynamics. Another advantage of RL methods is that after training the RL agent, its learned policy can be directly used in a new test setting without requiring solving any optimization problem. Therefore, RL methods are efficient tools for real-time control [25]. RL methods have been successfully applied to many power system problems such as the smart charging of EVs [26, 27], demand response [28], frequency control [29], etc. Generally, model-free RL methods can be classified into two categories: value-based methods (e.g., Q-learning, fitted Q-iteration (FQI), DQN, etc.) and policy gradient methods (e.g., actor-critic, deep deterministic policy gradient (DDPG), SAC, etc.) [30]. In value-based methods, the Q (or V) function is learned (estimated) and the action is chosen based on the learned Q (or V) function as to maximize it. The authors in [31] proposed the DQN method, which combines RL with deep learning. A trained agent using DQN reached human-level performance on many Atari video games. On the other hand, policy gradient methods directly learn the policy. In [32], the SAC method has been proposed as an off-policy actor-critic method. In SAC, the policy is learned by an actor network and the Q function is estimated by a critic network. The actor aims to maximize the expected reward as well as the entropy of the actor, to encourage the agent to explore the environment more. In this paper, we will use the DQN (as a state-of-the-art method in value-based methods) and SAC (as a state-of-the-art method in policy gradient methods) methods to solve the arbitrage problem. In this section, the imbalance settlement mechanism is explained in detail (Section 3.1) and the Markov decision process (MDP) formulation of the energy arbitrage problem in the imbalance settlement mechanism is provided (Sections 3.2 and 3.3). BRPs are responsible for continuously balancing their individual demand and supply. But sometimes BRPs deviate from their traded consumption and generation due to uncertainties in the grid. The total imbalance volume of all BRPs in a single control area is called the total system imbalance [33]. Positive and negative values of the system imbalance indicate the excess and shortage of the generation, respectively. A TSO corrects the system imbalance in real-time by activating reserve capacities offered in the balancing market [34]. A TSO charges BRPs for their imbalance at a price specific to the imbalance settlement period (15 min in most European markets). This mechanism is known as imbalance settlement. The imbalance price is dependent on the reserve volume activated by the TSO [35]. In each imbalance settlement period, the negative imbalance price is equal to the highest activated upward reserve offer (marginal incremental price), and the positive imbalance price is determined by the lowest activated downward reserve offer (marginal decremental price) [36]. Three main imbalance pricing methodologies are used in various countries: (1) dual pricing; (2) two-price settlement; and (3) single pricing [36]. In the dual pricing method, the imbalance price is different for positive and negative imbalances. BRPs penalize for negative and positive imbalances using the marginal incremental price (MIP) and marginal decremental price (MDP), respectively. This pricing method motivates BRPs to keep the balance within their own portfolio without being concerned about the total system imbalance. The main drawback of this method is that there is no incentive for BRPs to deviate from their nomination to restore the grid. For instance, if the total system imbalance is positive and there is a BRP that can reduce this imbalance, then this BRP is not incentivized, but even penalized for deviating from its day-ahead nomination. In the two-price settlement method, similar to the dual pricing method, different imbalance prices are considered for each imbalance direction. The difference with the dual pricing method is that if the imbalance direction of BRPs is opposite to the total system imbalance direction, the imbalance price is the same as the day-ahead price. Although in this pricing method, BRPs do not face penalties due to their deviation for helping TSO with restoring the grid, the imbalance price is not attractive to create a portfolio imbalance for supporting the grid (typically, day ahead prices are imbalance prices). In the single pricing method, the imbalance price is the same for both imbalance directions and depends on the total system imbalance. This pricing method provides an opportunity for BRPs to reduce their cost by supporting the grid. For instance, if the total imbalance price is negative and a BRP creates a positive imbalance, the BRP will receive an MIP (imbalance price) which is usually higher than the day-ahead price. In some countries, e.g., Germany, despite using the single pricing method, arbitrage in the imbalance settlement mechanism is prohibited and market players are expected to trade honestly in the markets [37]. Nonetheless, the arbitrage in the imbalance settlement mechanism is a win-win situation for both BRPs and TSOs. On the one hand, BRPs can profit from the arbitrage and indirectly reduce the total system imbalance. On the other hand, this decrease in the total system imbalance results in a lower imbalance price since the TSO does not need to activate more expensive reserve offers. As mentioned earlier, ENTSO-E aims to harmonize the imbalance settlement mechanism in Europe by implementing the single pricing method for calculating the imbalance price with a 15 minute imbalance settlement period. For this reason, the focus of this paper is on the single pricing method. The Belgian imbalance settlement mechanism is a good case study for this research work because since the beginning of 2020, it adopts the single pricing method with a 15 minute settlement period [33]. The energy arbitrage problem can be formulated as an MDP. An MDP provides a mathematical framework for stochastic sequential decision-making problems and is modeled by a tuple (\ud835\udcae,\ud835\udc9c,\u211b,\ud835\udcab,\u03b3)\ud835\udcae\ud835\udc9c\u211b\ud835\udcab\ud835\udefe(\\mathcal{S},\\mathcal{A},\\mathcal{R},\\mathcal{P},\\gamma)( caligraphic_S , caligraphic_A , caligraphic_R , caligraphic_P , italic_\u03b3 ), where \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S is the state space, \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A is the (discrete) action space, \u211b:\ud835\udcae\u00d7\ud835\udc9c\u2192\u211d:\u211b\u2192\ud835\udcae\ud835\udc9c\u211d\\mathcal{R}:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}caligraphic_R : caligraphic_S \u00d7 caligraphic_A \u2192 blackboard_R represents the immediate reward function, \ud835\udcab:\ud835\udcae\u00d7\ud835\udcae\u00d7\ud835\udc9c\u2192[0,1]:\ud835\udcab\u2192\ud835\udcae\ud835\udcae\ud835\udc9c01\\mathcal{P}:\\mathcal{S}\\times\\mathcal{S}\\times\\mathcal{A}\\rightarrow[0,1]caligraphic_P : caligraphic_S \u00d7 caligraphic_S \u00d7 caligraphic_A \u2192 [ 0 , 1 ] denotes the unknown state transition probability distribution, and \u03b3\u2208(0,1]\ud835\udefe01\\gamma\\in(0,1]italic_\u03b3 \u2208 ( 0 , 1 ] is the discount factor [38]. At each time step t\ud835\udc61titalic_t, the agent observes the environment state st\u2208\ud835\udcaesubscript\ud835\udc60\ud835\udc61\ud835\udcaes_{t}\\in\\mathcal{S}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 caligraphic_S and takes an action at\u2208\ud835\udc9csubscript\ud835\udc4e\ud835\udc61\ud835\udc9ca_{t}\\in\\mathcal{A}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 caligraphic_A based on the current state. As a consequence of the taken action, the agent receives a reward value \u211b(st,at)\u211bsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61\\mathcal{R}(s_{t},a_{t})caligraphic_R ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) and moves to a new state st+1\u2208\ud835\udcaesubscript\ud835\udc60t+1\ud835\udcaes_{\\text{t+1}}\\in\\mathcal{S}italic_s start_POSTSUBSCRIPT t+1 end_POSTSUBSCRIPT \u2208 caligraphic_S with the probability determined by the state transition probability distribution \ud835\udcab(st+1|st,at)\ud835\udcabconditionalsubscript\ud835\udc60t+1subscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61\\mathcal{P}(s_{\\text{t+1}}|s_{t},a_{t})caligraphic_P ( italic_s start_POSTSUBSCRIPT t+1 end_POSTSUBSCRIPT | italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ). In the energy arbitrage problem, the agent is a decision maker who decides about the charging/discharging of BESS at each time step. The environment is the external context with which the agent interacts (electricity markets, grid, etc.). We define the MDP formulation of the energy arbitrage problem in the imbalance settlement mechanism without cycle constraints as follows: State: The state at each time step is expressed as where Tqhsubscript\ud835\udc47qhT_{\\textrm{qh}}italic_T start_POSTSUBSCRIPT qh end_POSTSUBSCRIPT represents the minute of the quarter hour, qh is the quarter hour of the day, mo is the month of the year, SOCtsubscriptSOC\ud835\udc61\\textrm{SOC}_{t}SOC start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the SoC of BESS at time t\ud835\udc61titalic_t, and \u03c0^timbsubscriptsuperscript^\ud835\udf0bimb\ud835\udc61\\hat{\\pi}^{\\textrm{imb}}_{t}over^ start_ARG italic_\u03c0 end_ARG start_POSTSUPERSCRIPT imb end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the forecasted imbalance price of the current quarter-hour. We used a forecast of the imbalance price because the real imbalance price of the quarter hour is only calculated once the quarter hour is over. Action: We consider a discrete action space with 3 possible actions, as follows: where Pmaxsubscript\ud835\udc43maxP_{\\text{max}}italic_P start_POSTSUBSCRIPT max end_POSTSUBSCRIPT is the maximum (dis-)charging power of the BESS. The action atsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT represents a decision on the charging/discharging power at time t\ud835\udc61titalic_t. Reward: The objective of the agent is to maximize the revenue by buying energy when the imbalance price is low and selling it when the imbalance price is high. Hence, the reward function to be maximized is the negative of the energy cost, defined as follows where \u03c0timbsubscriptsuperscript\ud835\udf0bimb\ud835\udc61\\pi^{\\text{imb}}_{t}italic_\u03c0 start_POSTSUPERSCRIPT imb end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the real imbalance price of the quarter hour in which t\ud835\udc61titalic_t lies. State transition function: In the MDP framework, system dynamics are described by a state transition probability function \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P. This probability function is unknown in the energy arbitrage problem because of uncertainties in the imbalance price. The agent strives to estimate the state probability distribution through interactions with the environment. However, the state transition for SOCtsubscriptSOC\ud835\udc61\\textrm{SOC}_{t}SOC start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is controlled by atsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and can be explicitly formulated as below. where CBESSsubscript\ud835\udc36BESSC_{\\text{BESS}}italic_C start_POSTSUBSCRIPT BESS end_POSTSUBSCRIPT is the maximum capacity of the BESS, and \u03b7chasubscript\ud835\udf02cha\\eta_{\\text{cha}}italic_\u03b7 start_POSTSUBSCRIPT cha end_POSTSUBSCRIPT and \u03b7dissubscript\ud835\udf02dis\\eta_{\\text{dis}}italic_\u03b7 start_POSTSUBSCRIPT dis end_POSTSUBSCRIPT, denote the charging and discharging efficiency of the BESS, respectively. Frequent charging/discharging cycles cause an extra cost because they expedite the degradation of BESS. Modeling the aging of BESS is crucial as it indicates a capital loss of BESS investment costs [39]. Due to the dependence of battery lifetime on its operational strategy, the lifetime of a BESS plays an important role in the financial evaluation of the energy arbitrage strategy. Usually, the lifetime of a BESS is determined by the number of complete charge-discharge cycles before its nominal capacity becomes lower than a certain level of its initial rated capacity [40]. Thus, we constrain the daily number of cycles, since it aligns with the designed lifetime and guarantee provided by manufacturers [41]. The MDP formulation with cycle constraint consideration is described next. State: The state is given by where ntcycsuperscriptsubscript\ud835\udc5b\ud835\udc61cycn_{t}^{\\text{cyc}}italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT cyc end_POSTSUPERSCRIPT is the daily consumed number of cycles, calculated using (7). Action: Similar to the MDP formulation without cycle constraints, the action space is discrete with 3 possible actions. The action is determined as follows where nmaxcycsuperscriptsubscript\ud835\udc5bmaxcycn_{\\text{max}}^{\\text{cyc}}italic_n start_POSTSUBSCRIPT max end_POSTSUBSCRIPT start_POSTSUPERSCRIPT cyc end_POSTSUPERSCRIPT is the maximum allowed daily number of cycles and B(.)B(.)italic_B ( . ) is a backup controller to ensure the daily cycle constraint. The backup controller is used to override the agent action (utsubscript\ud835\udc62\ud835\udc61u_{t}italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) when the agent wants to discharge the battery and the daily number of cycles exceeds the maximum allowed value. Reward: The reward function definition is the same as that of the MDP formulation without cycle constraint. State transition function: Also the state transition function is the same as that of the MDP formulation without cycle constraint. In this paper, RL methods are used to solve the arbitrage problem formulated as an MDP and find an arbitrage strategy in the imbalance settlement mechanism. The goal in RL is to learn a policy that maximizes the expected long-term reward. Next we detail the two RL methods adopted in this paper, i.e., DQN and SAC. Subsequently, we introduce the distributional perspective on RL and the risk-sensitive RL framework. Classical tabular RL methods, e.g., Q-learning, suffer from an issue known as the curse of dimensionality. Since these methods can only be applied to problems with discrete state space, they can not be used for problems with high-dimensional or continuous state space. In addition, these methods usually need handcrafted state representations [30]. To overcome these limitations, the DQN method uses a deep neural network as a function approximator to estimate the Q-value function parametrized by \u03b8\ud835\udf03\\thetaitalic_\u03b8. The Q-value function Q\u03b8(st,at)subscript\ud835\udc44\ud835\udf03subscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61Q_{\\theta}(s_{t},a_{t})italic_Q start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is learned by minimizing the following loss function: The first benefit of DQN is its stability in learning. In [31], two techniques are used to stabilize the learning process. First, the target Q function Q\u03b8\u2032(st,at)subscript\ud835\udc44superscript\ud835\udf03\u2032subscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61Q_{\\text{$\\theta^{\\prime}$}}(s_{t},a_{t})italic_Q start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is used to calculate next state-action values in Eq. 10. Parameters of the target Q function (\u03b8\u2032superscript\ud835\udf03\u2032\\theta^{\\prime}italic_\u03b8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT) are periodically updated with the most recent \u03b8\ud835\udf03\\thetaitalic_\u03b8. Second, agent past transitions are stored in an experience replay buffer (\ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D) and for training Q\u03b8subscript\ud835\udc44\ud835\udf03Q_{\\theta}italic_Q start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT, mini-batches of experiences are sampled from \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D. Another benefit of the DQN method is that this method is an off-policy method. The key advantage of off-policy methods is their capacity to learn from historical data since using the current experiences as the training set can easily overfit the policy because the training samples are not independent [42]. In an off-policy setting, a policy learned by the agent is different from a behavior policy used for collecting historical data. Using past transitions for training can significantly improve sample efficiency. Value-based methods have some limitations. The application of these methods is limited to problems with a discrete and low-dimensional action space. Also, these methods learn a deterministic policy, which means for a given state, an action taken by the agent is always the same. Thus, keeping a balance between exploration and exploitation in value-based methods is challenging. Policy gradient methods solve these limitations by learning a policy network that outputs the probability of taking actions in each state. From the existing policy gradient methods, we use SAC because of its superior sample efficiency and stability. In this off-policy method, the policy is learned by an actor network \u03c0\u03d5subscript\ud835\udf0bitalic-\u03d5\\pi_{\\phi}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT and the Q function is approximated by a critic network Q\u03b8subscript\ud835\udc44\ud835\udf03Q_{\\theta}italic_Q start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT. The objective of the actor is to maximize the expected reward as well as maximize the entropy of the actor to encourage the agent to further explore the environment. The loss function of the actor network (J\u03c0subscript\ud835\udc3d\ud835\udf0bJ_{\\pi}italic_J start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT) is given by The critic network estimates the soft Q-value function. The loss function of the critic network (LQsubscript\ud835\udc3f\ud835\udc44L_{Q}italic_L start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT) is formulated as follows: In Eq. 13, ytsubscript\ud835\udc66\ud835\udc61y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is an estimated soft-Q value that is calculated by a modified Bellman equation (the so-called soft Bellman equation). Similar to the DQN method, the target Q function is used to calculate ytsubscript\ud835\udc66\ud835\udc61y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. After each update of Q\u03b8subscript\ud835\udc44\ud835\udf03Q_{\\theta}italic_Q start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT, the parameters of Q\u03b8\u2032subscript\ud835\udc44superscript\ud835\udf03\u2032Q_{\\text{$\\theta^{\\prime}$}}italic_Q start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT are updated according to Eq. 14 with \u03c4\u226a1much-less-than\ud835\udf0f1\\tau\\ll 1italic_\u03c4 \u226a 1 to slowly track the learned network [43]. A distributional perspective on RL was first introduced in [44]. In DRL methods, the probability distribution over returns is estimated rather than a point estimate of the mean. DRL methods offer several advantages, including more stable learning [44], mitigating Q-value overestimation [45], and providing a framework for risk-sensitive learning [46]. In the vanilla DQN method, the core idea is to estimate the Q-value function Q\u03b8subscript\ud835\udc44\ud835\udf03Q_{\\theta}italic_Q start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT. Going beyond the vanilla DQN method, the distributional DQN (DDQN) method learns the probability distribution of returns (\ud835\udcb5\u03b8subscript\ud835\udcb5\ud835\udf03\\mathcal{Z}_{\\theta}caligraphic_Z start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT) using the distributional Bellman equation as follows [44]: where \ud835\udcb5\ud835\udcb5\\mathcal{Z}caligraphic_Z is the distribution of returns, A=\ud835\udc37B\ud835\udc34\ud835\udc37\ud835\udc35A\\overset{D}{=}Bitalic_A overitalic_D start_ARG = end_ARG italic_B denotes that two random variables A\ud835\udc34Aitalic_A and B\ud835\udc35Bitalic_B have an equal probability distribution, and \ud835\udcaf\ud835\udcb5\u03b8\ud835\udcafsubscript\ud835\udcb5\ud835\udf03\\mathcal{T}\\mathcal{Z}_{\\theta}caligraphic_T caligraphic_Z start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT indicates the probability distribution of \ud835\udcafZ\ud835\udcaf\ud835\udc4d\\mathcal{T}Zcaligraphic_T italic_Z. The distribution of returns can be modeled as a categorical distribution as below. In Eq. 17, Vminsubscript\ud835\udc49minV_{\\text{min}}italic_V start_POSTSUBSCRIPT min end_POSTSUBSCRIPT and Vmaxsubscript\ud835\udc49maxV_{\\text{max}}italic_V start_POSTSUBSCRIPT max end_POSTSUBSCRIPT are the maximum and minimum values of random returns, respectively, and N\ud835\udc41Nitalic_N is the number of bins. In distributional SAC (DSAC), the critic network learns the probability distribution of soft returns. The loss function of the critic network in DSAC is similar to Eq. 15, but the calculation of \ud835\udcafZ(st,at)\ud835\udcaf\ud835\udc4dsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61\\mathcal{T}Z(s_{t},a_{t})caligraphic_T italic_Z ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) differs as follows: Since the expectation of Z(st,at)\ud835\udc4dsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61Z(s_{t},a_{t})italic_Z ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) over \ud835\udcb5\u03b8subscript\ud835\udcb5\ud835\udf03\\mathcal{Z}_{\\theta}caligraphic_Z start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT is equal to Q(st,at)\ud835\udc44subscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61Q(s_{t},a_{t})italic_Q ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), the loss function of the actor network is modified as below. By approximating the probability distribution of returns, DRL presents a possibility for learning a risk-averse policy. In a risk-neutral RL framework, the agent in each state takes an action that aims to maximize the expected return (Q value). On the other hand, in the risk-sensitive RL framework, the agent takes an action with the lowest associated risk. The main risk in the arbitrage problem is related to forecasted imbalance prices. The greater the inaccuracy in predicted prices, the higher the associated risk of taking the wrong action. Risk measures can be used to assess the level of risk associated with a distribution of returns [47]. The loss function of the actor network in the risk-sensitive DSAC can be formulated as follows: where \u03a8[.]\\Psi[.]roman_\u03a8 [ . ] represents a risk measure function and \u03b2\ud835\udefd\\betaitalic_\u03b2 is a parameter that controls the trade-off between the expectation value and risk. \u03b2=0\ud835\udefd0\\beta=0italic_\u03b2 = 0 represents the risk-neutral attitude of the agent. As \u03b2\ud835\udefd\\betaitalic_\u03b2 increases, the agent becomes more risk-averse. In this paper, value-at-risk (VaR) is applied as the risk measure function: where \u03c1\u2208(0,1]\ud835\udf0c01\\rho\\in(0,1]italic_\u03c1 \u2208 ( 0 , 1 ] is a confidence level. We will set \u03c1=0.1\ud835\udf0c0.1\\rho=0.1italic_\u03c1 = 0.1 in this paper. We will evaluate the performance of the proposed control framework, is explained in Sections 3, LABEL: and 4, for the energy arbitrage problem. Figure 2 shows the overview of the proposed control framework, which we test on the Belgian imbalance in 2022 extracted from [48]. As mentioned in Section 3.1, Elia publishes two imbalance prices: 15-minute-based and 1-minute-based prices. The reference price for the imbalance settlement of BRPs is the 15-minute-based price which is the real imbalance price calculated at the end of the quarter-hour period. The 1-minute-based prices, on the other hand, are calculated based on non-validated data, based on the instantaneous system imbalance and prices of cumulative activated regulation volumes on a minute basis. These 1-minute-based prices are published to provide additional information to BRPs [49]. We use these non-validated prices as forecasted imbalance prices of the corresponding quarter-hour period. Since the granularity of the forecasted imbalance prices is one minute, the RL agent takes an action every minute. In this work, the day-ahead schedule for the battery is set to zero which means that the battery does not trade in the day-ahead market. However, future work will extend our proposed control framework for arbitrage in both the day-ahead market and imbalance settlement. To train and validate the proposed control framework, the imbalance price dataset is split as follows: the first 20 days of each month are considered as the training set, the 21st day to the 25th day of each month are considered as the validation set, and the remaining days of each month are used as the test set. The considered BESS has a power rating of 1 MW and a maximum capacity of 2 MWh with a round-trip efficiency of 0.9 for both charging and discharging. Since the maximum allowed annual number of cycles for the BESS is 400, the maximum daily number of cycles is set to 1.1. The RL methods are trained with 50 0005000050\\,00050 000 episodes and each episode constitutes a simple day. The hyperparameters used for the methods are listed in Table 1. The proposed control framework is implemented in Python using the PyTorch package. We design experiments to answer the following questions: Q1: What is the learned arbitrage strategy when there is no limit on the daily number of cycles? Q2: How does a daily number of cycles affect the learned arbitrage strategy? Q3: What is the effect of the risk-averse perspective on the learned arbitrage strategy? The learning process of the RL methods for the risk-neutral scenario, without considering the cycle constraint, is illustrated in Fig. 3. The performance of the trained RL methods on the test set is indicated in Table 2. Results show that the DRL methods outperform the standard RL methods. The reason behind this is that estimating the probability distribution of returns, rather than the expectation of returns, can provide a more stable training target. Also, the DRL methods can mitigate instability in the Bellman optimality operator by learning probability distribution of returns. The DDQN method increases the average daily profit by 17% compared to the DQN method. DSAC improves the proportional reward (defined as the ratio of average daily profit to average daily number of cycles) by 2.1% compared to SAC. The comparison between the performance of the distributional and vanilla DQN, and SAC, indicates that the distributional perspective can enhance DQN results to a greater extent. The reason is that the SAC method mitigates instability in the Bellman optimality operator by using an actor network instead of the max operator in the Bellman equation. Therefore, the improvement in the DSAC results is mainly due to stable training target for the critic network. However, the distributional perspective can boost the performance of the vanilla DQN by both providing stable training targets and mitigating instability in the Bellman optimality operator. Results also highlight the superiority of SAC over DQN. This is because SAC can mitigate Q-value overestimations in DQN by replacing the max operator (Eq. 10) with the expectation operator (Eq. 13) in the Bellman equation. To analyze and study the learned policy of the four RL methods, the policy heatmaps are illustrated in Fig. 4. Since SoC and forecasted imbalance price are the two most determinative features for the agent, we show the learned policy with respect to these two input features, which are also informative to interpret the policy. Figure 4 shows that the SAC and DSAC methods can learn a more meaningful and smooth policy compared to the DQN and DDQN methods. For DQN and DDQN, the Q-value function overestimates the value of rarely seen states and out-of-distribution (OOD) actions in these rare sates due to the max operator and the reliance of the estimated Q values on inputs from the same distribution as its training set. This overestimation results in policies that choose OOD actions. According to Fig. 5, the forecasted imbalance price rarely goes beyond 850 \u20ac/ MWh (the probability is 1%). It means that the DQN and DDQN methods overestimate Q values for this area and take OOD actions. Figures 4, LABEL: and 5 reveal some correlation between the learned policy by DSAC and the price distribution. The agent always charges the BESS when the price is within the lower 7% quantile (lower than \u2212--60 \u20ac/ MWh), regardless of the SoC level. The agent never takes the charging action for the 25% highest prices (prices higher than 380 \u20ac/ MWh), even if the BESS is empty. The BESS is always discharged when the price lies in the upper 5% quantile (higher than 640 \u20ac/ MWh). For the median price (roughly 220 \u20ac/ MWh), the BESS is discharged if the SoC is higher than 60%, does nothing when the SoC is between 60% and 50%, and is charged if the SoC is lower than 50%. Generally, the agent learns a milder slope boundary for the discharge action. If the BESS with a low SoC level is discharged, the agent needs to quickly recharge the BESS to make sure it can still make money. This quick recharging increases the risk of charging at a higher price. Therefore, by decreasing the SoC, the area of idle action becomes larger. Figure 6 shows the learning process of the RL methods for the risk-neutral scenario when the limitation is applied to the daily number of cycles. Similar to the previous scenario, the DSAC method surpasses other methods by converging to a higher reward with a fewer number of cycles. According to Table 2, although the average daily profit of the DSAC method is less than that of the SAC method, the DSAC method earns this profit by consuming fewer number of cycles. In other words, the DSAC method achieves a 14.6% improvement in the proportional reward per cycle compared to the SAC method. Furthermore, the SAC and DSAC methods converge faster than the DQN and DDQN methods due to their efficient exploration. Since in DQN and DDQN the learned policy is deterministic, the \u03f5italic-\u03f5\\epsilonitalic_\u03f5-greedy exploration technique needs to be used. On the other hand, the SAC and DSAC methods learn a stochastic policy and use the learned probabilities for exploration. Thus, instead of always considering a fixed exploration probability of \u03f5italic-\u03f5\\epsilonitalic_\u03f5 for all states, the probability of exploration depends on the current state. For a given state, when the probability of one action is close to 1, the agent almost always exploits and hardly explores. Conversely, when probabilities of all actions are close to each other, the agent most of the time explores to find the best action for that state. Consequently, the SAC and DSAC methods are more data efficient than the DQN and DDQN methods. The learned policy of DSAC when considering the cycle constraint is illustrated in Fig. 7. Note that the displayed policy is a projection of the learned policy, as the learned policy depends on more than two features and thus is more complicated than the figures shown. The logic behind the learned policy with and without the cycle constraint consideration, which is charging at cheap prices and discharging at expensive prices, is nearly identical. The main difference between these learned policies is in the size of the idle action area. Adding the cycle constraint makes the agent more conservative and increases the idle action area. Moreover, by limiting the number of cycles, the agent recharges the BESS less frequently due to reduced discharging. As a result, in this scenario, the agent recharges the BESS at cheaper prices compared to the previous scenario. To show the performance of the learned DSAC agents in a real-life case, the learned agents are tested using data from March 31, 2022. As Fig. 8 shows, there is one major peak in the imbalance price from 11:00 to 13:15 and one major valley from 13:30 to 17:00 on this day. Both agents properly respond to these prices: the agent without the cycle constraint reacts to roughly all fluctuations in the imbalance price, even small ones (such as the price fluctuation between 4:30 and 6:00, or between 20 and 21:30). However, another agent mostly focuses on more significant fluctuations to limit the number of charging cycles. To answer Q3, we train the DSAC agent without the cycle constraint consideration for varying \u03b2\ud835\udefd\\betaitalic_\u03b2 values. Results in Table 3 shows that the risk-averse agent with \u03b2=3\ud835\udefd3\\beta=3italic_\u03b2 = 3 experiences a 54.8% reduction in the average daily profit compared to the risk-neutral agent, but given that it avoids risky behavior, we note a higher profit per cycle. Figure 9 illustrates the difference between the learned critic network for the fully risk-averse and risk-neutral agents. The learned critic network for the fully risk-averse agent is narrower due to applying the risk measure function (VaR) instead of the expectation. Also the VaR values align with this observation: VaR values for the risk-neutral and fully risk-averse critic networks are equal to \u2212--589.2\u20ac and \u2212--240.5\u20ac, respectively. The probability distribution of the hourly profit for test data is shown in Fig. 10. Based on Fig. 10, the risk-averse agent successfully hedges against the uncertainty in the imbalance price and mitigates the tail of the hourly profit distribution.111Note that both the left- and right-tails are reduced, although from the risk perspective especially the lower (negative) return values should be avoided. The VaR value of each distribution is provided in Table 3. Figure 11 shows the learned risk-averse policy when \u03b2=3\ud835\udefd3\\beta=3italic_\u03b2 = 3. Compared to Fig. 4, we note that the idle area gets significantly larger: the agent does not discharge the battery when the SoC is low. In this way, the agent makes sure that the battery has always enough energy to inject into the grid when the price is high. Moreover, there is an observable change in the charge threshold that can be justified by Fig. 12. The charge threshold for the risk-neutral agent ranges between 0 and 400 \u20ac/ MWh. However, Fig. 12 indicates that within this range, the actual price is significantly uncertain and the chance of charging battery at a price larger than the forecasted value is high. To mitigate this risk, the risk-averse agent learns a lower charge threshold. The risk-averse agent charges the battery at cheaper prices to minimize the risk of charging at a high price resulting from inaccurate price predictions. In this paper, a DRL-based control framework for BESS was proposed to obtain energy arbitrage strategies in the imbalance settlement mechanism. In the proposed control framework, in addition to considering a constraint on the daily number of cycles, the degree of risk taking in the learned arbitrage strategy can be adjusted based on the risk preference of BRPs. To evaluate the performance of the proposed control framework, two state-of-the-art RL methods, i.e., DQN and SAC, and their distributional variants have been implemented. The results for the Belgian imbalance price in 2022 showed that the DSAC method outperforms other methods (i.e., both the non-distributional baselines as well as DDQN) in all experiments. DSAC improves the average daily profit in the experiment without cycle constraint by 53.1% and in the experiment with cycle constraint by 43.9%, respectively, compared to the (worst performing) DQN method. The dominance of SAC over DQN in terms of data efficiency and mitigating Q-value overestimation, stem from replacing the max operator in the Bellman equation with the expectation operator. Moreover, the distributional methods exhibit better performance than the standard RL methods because they estimate the full probability distribution of returns rather than the expectation of returns, and they resolve instability in the Bellman optimality operator. In a first experiment, without considering cycle constraints, we noted that the DSAC agent learned a smooth and rational policy: it learned to charge the battery when the price is very cheap (within the lower 7% quantile), discharge when the price is very expensive (within the upper 5% quantile), and take the action based on the SoC for prices in between. In a second experiment, including the cycle constraints, the cycle-aware arbitrage strategy expectedly showed a larger \u2018idle\u2019 action area compared to the case without cycle constraints, effectively leading to a lower number of cycles used. The trained cycle-aware agent tended to respond only to major peaks and valleys in the imbalance price due to the limited number of cycles, while the cycle-unaware agent reacted to almost all fluctuations in the imbalance price. Our study of risk-sensitive agents showed that the risk-averse arbitrage strategies make the distribution of hourly profit narrower and mitigate the tail of the distribution. Indeed, the risk-averse agent charges the battery at lower prices to mitigate the risk associated with inaccurate price forecasts and avoid incurring higher charging costs. Finally, we note that in this paper, the day-ahead schedule for the battery was set to zero. In future research, the proposed control framework will be generalized by taking into account energy arbitrage between the day-ahead market and the imbalance settlement mechanism. Studying the effect of considering a continuous action space instead of a discrete one forms another next step to take. The FQI method [50] is another widely used value-based method. In [13], FQI is used to obtain a 15-minute-based arbitrage strategy in the imbalance settlement mechanism. In this section, a small experiment is carried out to compare the performance of the DQN and FQI methods. In this experiment, the methods are trained on the first nine days of February and evaluated on February 10, 2022. The architecture of the neural network used in the FQI method is the same as that of the DQN method. The experience replay buffer size, number of iterations, and number of episodes are 16 3841638416\\,38416 384, 400, and 500, respectively. In accordance with Fig. 13, both methods perform almost similarly. However, the run time of the FQI method is roughly 5 times greater than that of the DQN method and even gets worse by increasing the experience replay size and the number of episodes. The reason for the longer run time for FQI is its number of iterations: in each episode, the Q network is trained for the mentioned number of iterations. Thus, the FQI method is inappropriate for obtaining the arbitrage strategy."
    },
    {
        "id": 10,
        "title": "1002 km Twin-Field Quantum Key Distribution with Finite-Key Analysis",
        "corpus": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on. Authors: achieve the best HTML results from your LaTeX submissions by selecting from this list of supported packages. Quantum key distribution (QKD) holds the potential to establish secure keys over long distances. The distance of point-to-point QKD secure key distribution is primarily impeded by the transmission loss inherent to the channel. In the quest to realize a large-scale quantum network, increasing the QKD distance under current technology is of great research interest. Here we adopt the 3-intensity sending-or-not-sending twin-field QKD (TF-QKD) protocol with the actively-odd-parity-pairing method. The experiment demonstrates the feasibility of secure QKD over a 1002 km fibre channel considering the finite size effect. The secure key rate is 3.11\u00d710\u2212123.11superscript10123.11\\times 10^{-12}3.11 \u00d7 10 start_POSTSUPERSCRIPT - 12 end_POSTSUPERSCRIPT per pulse at this distance. Furthermore, by optimizing parameters for shorter fiber distances, we conducted performance tests on key distribution for fiber lengths ranging from 202 km to 505 km. Notably, the secure key rate for the 202 km, the normal distance between major cities, reached 111.74 kbps. Quantum key distribution (QKD) Bennett and Brassard (1984); Ekert (1991); Gisin et al. (2002); Scarani et al. (2009); Gisin (2015); Xu et al. (2020a); Pirandola et al. (2020) ensures secure key distribution using the principles of quantum mechanics. An active research frontier in practical quantum cryptography is exploring the distribution distance achievable with the present technology. The main challenge in extending the distribution distance of QKD is the channel loss that occurs when transmitting single-photon level quantum signals. Unlike classical communication, quantum signals cannot be amplified, posing a significant hurdle Wootters and Zurek (1982). The exponentially decreased transmission of the optical fibre channel results in a vanishing secure key rate at long distances. Importantly, the limited number of detected signals over long distances also constrains the secure key rate due to the finite-key effect. In the quest for achieving long-distance QKD, twin-field QKD (TF-QKD) is proposed Lucamarini et al. (2018) recently. TF-QKD may achieve a secure key rate in spirit similar to that of a single-repeater QKD scenario, significantly increasing the ultimate distance. Within a few years, TF-QKD has already been experimentally demonstrated in lab Minder et al. (2019); Wang et al. (2019); Liu et al. (2019); Zhong et al. (2019); Fang et al. (2020); Chen et al. (2020); Liu et al. (2021); Chen et al. (2021, 2022); Pittaluga et al. (2021); Wang et al. (2022); Liu et al. (2023) through up to 1002 km spooled fibre Liu et al. (2023), and in the field test over 511 km deployed fibre across metropolitans Chen et al. (2021). However, the previous 1002 km distribution distance Liu et al. (2023) was achieved under an asymptotic assumption. The longest distribution distance considering the finite size effect was reported to be 952 km Liu et al. (2023). The finite-size effect has to be taken into account in a practical QKD system since there are only a finite number of pulses. This consideration allows us to quantify the security level, i.e., the security coefficient; and apply the composable security framework. In this work, we demonstrate TF-QKD over 1002 km fibre channel considering the finite size effect. We adopt the sending-or-not-sending (SNS) protocol Wang et al. (2018) with advanced 3-intensity decoy-state method Hu et al. (2022) and the actively-odd-parity-pairing (AOPP) Xu et al. (2020b); Jiang et al. (2020) to improve the distribution distance. The ultra-low-noise superconducting nanowire single-photon detectors (SNSPDs) and dual-band phase estimation method are developed to suppress the system noise, thus achieving long distribution distance. Furthermore, the system is optimized for the normal distance between major cities of a fibre distance of 202 km. A secure key rate of 111.74 kbps is achieved at this distance, better than any reported results to our best knowledge. In this work, we adopt the 3-intensity SNS protocol developed by Wang et al. Wang et al. (2018). By placing the error correction process ahead of the decoy-state analysis process Hu et al. (2022), we can utilize all the heralded time windows for decoy-state analysis, resulting in an enhanced key rate. Furthermore, we also apply the AOPP Xu et al. (2020b) method to reduce the bit-flip error rate. The source parameters are symmetric for Alice and Bob: there are three sources on each side which are the vacuum source v\ud835\udc63vitalic_v, the decoy source x\ud835\udc65xitalic_x, and the signal source y\ud835\udc66yitalic_y with intensities \u03bcv=0,\u03bcx,\u03bcysubscript\ud835\udf07\ud835\udc630subscript\ud835\udf07\ud835\udc65subscript\ud835\udf07\ud835\udc66\\mu_{v}=0,\\mu_{x},\\mu_{y}italic_\u03bc start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = 0 , italic_\u03bc start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_\u03bc start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT and probabilities p0,px,pysubscript\ud835\udc5d0subscript\ud835\udc5d\ud835\udc65subscript\ud835\udc5d\ud835\udc66p_{0},p_{x},p_{y}italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT respectively. In each time window, Alice (Bob) randomly prepares and sends out a pulse from the three candidate sources to Charlie who is assumed to measure the interference result of the incoming pulse pair and announce the measurement results to Alice and Bob. In this work, the raw keys in the time windows, while Alice and Bob choose the sources v\ud835\udc63vitalic_v or y\ud835\udc66yitalic_y, are used to extract the secure keys. After Alice and Bob send N\ud835\udc41Nitalic_N pulse pairs to Charlie, and Charlie announces all measurement results, Alice and Bob distill the secure keys according to the following formula Jiang et al. (2020, 2021); Hu et al. (2022): where R\ud835\udc45Ritalic_R is the key rate of per sending-out pulse pair; n1subscript\ud835\udc5b1n_{1}italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is the lower bound of the number of survived untagged bits after AOPP and e1phsuperscriptsubscript\ud835\udc521\ud835\udc5d\u210ee_{1}^{ph}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p italic_h end_POSTSUPERSCRIPT is the upper bound of the phase-flip error rate of those survived untagged bits after AOPP; ntsubscript\ud835\udc5b\ud835\udc61n_{t}italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the number of survived bits after AOPP and Etsubscript\ud835\udc38\ud835\udc61E_{t}italic_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the corresponding bit-flip error rate in those survived bits; f\ud835\udc53fitalic_f is the error correction inefficiency which is set to f=1.16\ud835\udc531.16f=1.16italic_f = 1.16; H(x)=\u2212xlog2x\u2212(1\u2212x)log2(1\u2212x)\ud835\udc3b\ud835\udc65\ud835\udc65subscript2\ud835\udc651\ud835\udc65subscript21\ud835\udc65H(x)=-x\\log_{2}x-(1-x)\\log_{2}(1-x)italic_H ( italic_x ) = - italic_x roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_x - ( 1 - italic_x ) roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 - italic_x ) is the Shannon entropy. And Rtailsubscript\ud835\udc45tailR_{\\mbox{tail}}italic_R start_POSTSUBSCRIPT tail end_POSTSUBSCRIPT is where \u03b5corsubscript\ud835\udf00\ud835\udc50\ud835\udc5c\ud835\udc5f\\varepsilon_{cor}italic_\u03b5 start_POSTSUBSCRIPT italic_c italic_o italic_r end_POSTSUBSCRIPT is the failure probability of error correction, \u03b5PAsubscript\ud835\udf00\ud835\udc43\ud835\udc34\\varepsilon_{PA}italic_\u03b5 start_POSTSUBSCRIPT italic_P italic_A end_POSTSUBSCRIPT is the failure probability of privacy amplification, \u03b5^^\ud835\udf00\\hat{\\varepsilon}over^ start_ARG italic_\u03b5 end_ARG is the coefficient while using the chain rules of smooth min- and max- entropy Vitanov et al. (2013), and 2log2(nvy+nyv)2subscript2subscript\ud835\udc5b\ud835\udc63\ud835\udc66subscript\ud835\udc5b\ud835\udc66\ud835\udc632\\log_{2}(n_{vy}+n_{yv})2 roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_n start_POSTSUBSCRIPT italic_v italic_y end_POSTSUBSCRIPT + italic_n start_POSTSUBSCRIPT italic_y italic_v end_POSTSUBSCRIPT ) is the extra cost of the advanced decoy state analysis Hu et al. (2022) (nvysubscript\ud835\udc5b\ud835\udc63\ud835\udc66n_{vy}italic_n start_POSTSUBSCRIPT italic_v italic_y end_POSTSUBSCRIPT is the number of raw keys while Alice chooses the source v\ud835\udc63vitalic_v and Bob chooses the source y\ud835\udc66yitalic_y, and the definition of nyvsubscript\ud835\udc5b\ud835\udc66\ud835\udc63n_{yv}italic_n start_POSTSUBSCRIPT italic_y italic_v end_POSTSUBSCRIPT is similar with that of nvysubscript\ud835\udc5b\ud835\udc63\ud835\udc66n_{vy}italic_n start_POSTSUBSCRIPT italic_v italic_y end_POSTSUBSCRIPT ). The experimental setup is shown in Fig. 1. The seed lasers (\u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT=1548.51 nm and \u03bb2subscript\ud835\udf062\\lambda_{2}italic_\u03bb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT=1550.12 nm) are frequency stabilized using the Pound-Drever-Hall (PDH) technique Pound (1946); Drever et al. (1983); Clivati et al. (2022) with an ultra-stable cavity serving as the reference. The light is then sent to Alice\u2019s and Bob\u2019s stations through 450 km single-mode fibres respectively, incorporating 4 erbium-doped fibre amplifiers (EDFAs) in each path to stabilize the intensity. At Alice\u2019s (Bob\u2019s) station, the \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT light from Charlie is modulated to a 400 ns pulse in each 1 \u03bc\ud835\udf07\\muitalic_\u03bcs period, functioning as the \u201cstrong phase reference\u201d. The \u03bb2subscript\ud835\udf062\\lambda_{2}italic_\u03bb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT light serves as the frequency reference for the optical phase-locked loop (OPLL). The locally produced \u03bb2subscript\ud835\udf062\\lambda_{2}italic_\u03bb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT laser is locked to the frequency reference from Charlie, and then modulated to \u201cdim phase reference\u201d and \u201cquantum signals\u201d. As shown in Fig. 2, the intensity of the 40 ms \u201cdim phase reference\u201d is generally higher than the 60 ms \u201cquantum signals\u201d light. In each 1 \u03bc\ud835\udf07\\muitalic_\u03bcs period, the pulse train is modulated to the same random pattern in the first 400 ns for Alice and Bob, serving as the \u201cdim phase reference\u201d. In the remaining 600 ns, the pulse train is modulated to generate random quantum signals based on their respective local random numbers. Note that only the detections occurring within the first 400 ns in each 1 \u03bc\ud835\udf07\\muitalic_\u03bcs period, and within the first 40 ms in the 100 ms period, are utilized as the \u201ddim phase reference\u201d. Similarly, the detections in the last 600 ns of each 1 \u03bc\ud835\udf07\\muitalic_\u03bcs period, and the last 60 ms of the 100 ms period, are used as the \u201dquantum signals\u201d. For both the \u201cstrong phase reference\u201d and \u201cdim phase reference\u201d, the relative phases between Alice and Bob are set to four relative phases \u03b4AB={0,\u03c0/2,\u03c0,3\u03c0/2}subscript\ud835\udeff\ud835\udc34\ud835\udc350\ud835\udf0b2\ud835\udf0b3\ud835\udf0b2\\delta_{AB}=\\{0,\\pi/2,\\pi,3\\pi/2\\}italic_\u03b4 start_POSTSUBSCRIPT italic_A italic_B end_POSTSUBSCRIPT = { 0 , italic_\u03c0 / 2 , italic_\u03c0 , 3 italic_\u03c0 / 2 } within a 1 \u03bc\ud835\udf07\\muitalic_\u03bcs period. We implemented the 3-intensity SNS-TF-QKD protocol in the experiment. The \u03bb2subscript\ud835\udf062\\lambda_{2}italic_\u03bb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT quantum signals are modulated to 3 intensities with 16 different phases using intensity modulators (IMs) and phase modulators. The IMs are stabilized at Alice\u2019s (Bob\u2019s) station to ensure the quantum signals\u2019 intensities are stable. The \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u201cstrong phase reference\u201d is then filtered and combined with the quantum signals. An electronic polarization controller is installed within the secure zone to control the polarization drift. A dispersion compensation module is employed to pre-compensate for the chromatic dispersion of the fibre channel. The signals are then attenuated to the predetermined intensities and subsequently transmitted to Charlie via the quantum channels. The polarization of different wavelengths may evolute differently. At Charlie\u2019s measurement station, a polarization feedback algorithm is utilized to optimize the \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT detections to between 75 kHz and 300 kHz, while minimizing the \u03bb2subscript\ud835\udf062\\lambda_{2}italic_\u03bb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. The relative delay between Alice\u2019s and Bob\u2019s signals is monitored and compensated using the rising edges of the \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT pulses. The light from Alice and Bob is interfered at the beam splitter and subsequently demultiplexed to \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and \u03bb2subscript\ud835\udf062\\lambda_{2}italic_\u03bb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT wavelengths. This light is then filtered by DWDMs, measured with SNSPDs, and recorded with a Time Tagger. The recorded signals are categorized into the \u201cstrong phase reference\u201d, \u201cdim phase reference\u201d and the quantum signal for subsequent data processing. The ultra-low loss fibre is utilized to minimize channel loss. The fibre is manufactured with \u201cpure silica core\u201d technology to reduce the doped Ge in the core and with decreased fictive temperature. The average attenuation of the fibres is measured to be less than 0.157 dB/km. The ultra-low dark count SNSPDs are developed to reduce detection-related noise. The noise suppression includes stages of filtering. The long-wavelength (>>>2 \u03bc\ud835\udf07\\muitalic_\u03bcm) filtering is achieved using the 28 mm diameter fibre coils at the 40 K cold plate. Narrowband wavelength filtering is carried out utilizing a cryogenic bandpass filter (BPF) with a 5 nm bandwidth and an 85% transmittance at 2.2 K cold plate Zhang et al. (2018). The dark count rate is measured to be as low as 0.02 Hz. Additionally, the detection efficiency is optimized to be around 60% with a distributed Bragg reflector (DBR) based optical cavity Zhang et al. (2017). The time-multiplexed dual-band stabilization method is employed to reduce the re-Rayleigh scattering noise induced in previously reported time-multiplexed phase estimation procedures. With dual-band stabilization, the wavelength of the \u201cstrong phase reference\u201d of \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is different from the quantum signal of \u03bb2subscript\ud835\udf062\\lambda_{2}italic_\u03bb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. The induced re-Rayleigh scattering is filtered with DWDMs. Furthermore, circulators are implemented in Charlie to eliminate noise resulting from the SNSPDs. Additionally, the \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u201dstrong phase reference\u201d is time-multiplexed with the quantum signal, effectively mitigating disturbance caused by spontaneous Raman scattering noise induced by the strong phase reference light. The combination of wavelength- and time-multiplexing ensures that the noise introduced by the strong phase reference signal is less than 0.01 Hz. Moreover, the intensity of the weak \u201ddim phase reference\u201d signal, which is also time-multiplexed with the quantum signal, remains low enough to not generate perceptible noise. The data post-processing-based phase estimation method Liu et al. (2023) is adopted. First, the phase drift of the \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT wavelength is estimated using the \u201cstrong phase reference\u201d. Then, the wavelength difference is taken into account to estimate the phase of the \u03bb2subscript\ud835\udf062\\lambda_{2}italic_\u03bb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT light, with the accumulated phase drift of the \u201cstrong phase reference\u201d. Lastly, the initial phase difference \u03d5s(0)\u2212\u03d5r(0)subscriptitalic-\u03d5\ud835\udc600subscriptitalic-\u03d5\ud835\udc5f0\\phi_{s}(0)-\\phi_{r}(0)italic_\u03d5 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( 0 ) - italic_\u03d5 start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( 0 ) is computed using the phase difference between the \u201cdim phase reference\u201d and the \u201cstrong phase reference\u201d. In the experiment, this phase difference is computed and refreshed every 500 ms, to circumvent any accumulation of errors stemming from inaccurate wavelength settings, high-order residual phase errors, and errors in phase estimation. Result.\u2014 We first test the performance of 1002 km fibres (the \u201c+++\u201d-shape points in Fig. 3). The fibre distances between Alice-Charlie and Bob-Charlie are measured to be 500 km and 502 km. The decoy intensities are optimized as \u03bcx=0.08,\u03bcy=0.445formulae-sequencesubscript\ud835\udf07\ud835\udc650.08subscript\ud835\udf07\ud835\udc660.445\\mu_{x}=0.08,\\ \\mu_{y}=0.445italic_\u03bc start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = 0.08 , italic_\u03bc start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT = 0.445, with the time ratios pvac=0.52,px=0.28,py=0.20formulae-sequencesubscript\ud835\udc5d\ud835\udc63\ud835\udc4e\ud835\udc500.52formulae-sequencesubscript\ud835\udc5d\ud835\udc650.28subscript\ud835\udc5d\ud835\udc660.20p_{vac}=0.52,\\ p_{x}=0.28,\\ p_{y}=0.20italic_p start_POSTSUBSCRIPT italic_v italic_a italic_c end_POSTSUBSCRIPT = 0.52 , italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = 0.28 , italic_p start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT = 0.20 (Parameter #1). The finite size effect Jiang et al. (2021) is taken into consideration for all the experimental tests, considering composable security under any coherent attack Jiang et al. (2019, 2021). The error correction inefficiency is set to f=1.16\ud835\udc531.16f=1.16italic_f = 1.16 in the calculation; the failure probability of Chernoff bound in finite-size estimation is set to \u03b5=10\u221210\ud835\udf00superscript1010\\varepsilon=10^{-10}italic_\u03b5 = 10 start_POSTSUPERSCRIPT - 10 end_POSTSUPERSCRIPT; the failure probability of the error correction, and the privacy amplification is set to \u03b5cor=\u03b5PA=10\u221210subscript\ud835\udf00\ud835\udc50\ud835\udc5c\ud835\udc5fsubscript\ud835\udf00\ud835\udc43\ud835\udc34superscript1010\\varepsilon_{cor}=\\varepsilon_{PA}=10^{-10}italic_\u03b5 start_POSTSUBSCRIPT italic_c italic_o italic_r end_POSTSUBSCRIPT = italic_\u03b5 start_POSTSUBSCRIPT italic_P italic_A end_POSTSUBSCRIPT = 10 start_POSTSUPERSCRIPT - 10 end_POSTSUPERSCRIPT; the coefficient of the chain rules of smooth min- and max- entropies is set to \u03b5^=10\u221210^\ud835\udf00superscript1010\\hat{\\varepsilon}=10^{-10}over^ start_ARG italic_\u03b5 end_ARG = 10 start_POSTSUPERSCRIPT - 10 end_POSTSUPERSCRIPT. The system frequency is set to 1 GHz, with the signal pulse width set to 120 ps. The quantum signals are sent in the last 600 ns of the 1 \u03bc\ud835\udf07\\muitalic_\u03bcs period where the \u201cstrong phase reference\u201d is switched off. The quantum signals are time-multiplexed with the \u201cdim phase reference\u201d in the last 60 ms of the 100 ms period. The detections near the strong light are also dropped to avoid potential noises. As a result, the effective signal frequency is 351 MHz for the long-distance scenario. The total noises in the \u03bb2subscript\ud835\udf062\\lambda_{2}italic_\u03bb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are measured to be 0.019 Hz and 0.035 Hz in the working conditions. We attribute the noise mainly contributed by the SNSPD dark count and the spontaneous Raman scattering noise induced by the \u03bb1subscript\ud835\udf061\\lambda_{1}italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT light. The SNSPD detection efficiencies are measured to be 60% and 55%. The additional optical losses in Charlie are measured to be around 1.4 dB. In data processing, a 200 ps window is defined to filter out noises, with an efficiency of about 65%. Owing to the significant optical attenuation experienced over long-distance fibre, it is necessary to send a larger number of quantum signals in order to generate secure keys considering the finite size effect. A total of 1.00\u00d710151.00superscript10151.00\\times 10^{15}1.00 \u00d7 10 start_POSTSUPERSCRIPT 15 end_POSTSUPERSCRIPT quantum signal pulses are dispatched, resulting in 9.81\u00d71059.81superscript1059.81\\times 10^{5}9.81 \u00d7 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT valid detections that fall within the effective window. The quantum bit error rate (QBER) in Z basis is measured to be 9.44\u00d710\u221239.44superscript1039.44\\times 10^{-3}9.44 \u00d7 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT after AOPP; the QBER in X basis is measured to be 4.20%. The final secure key is 3.11\u00d710\u2212123.11superscript10123.11\\times 10^{-12}3.11 \u00d7 10 start_POSTSUPERSCRIPT - 12 end_POSTSUPERSCRIPT, which equates to 0.0011 bps considering the effective signal frequency. A total of 3112 bits of final secure keys are accumulated during the test. The detailed experimental results are summarized in Tab. 2 and Fig. 3. Next, we test the performance of fibre distances between 202 km and 505 km (the \u201c\u00d7\\times\u00d7\u201d-shape points in Fig. 3). The intensities of the decoy states are optimized for short distances as \u03bcx=0.05,\u03bcy=0.482formulae-sequencesubscript\ud835\udf07\ud835\udc650.05subscript\ud835\udf07\ud835\udc660.482\\mu_{x}=0.05,\\ \\mu_{y}=0.482italic_\u03bc start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = 0.05 , italic_\u03bc start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT = 0.482, with the time ratios pvac=0.68,px=0.04,py=0.28formulae-sequencesubscript\ud835\udc5d\ud835\udc63\ud835\udc4e\ud835\udc500.68formulae-sequencesubscript\ud835\udc5d\ud835\udc650.04subscript\ud835\udc5d\ud835\udc660.28p_{vac}=0.68,\\ p_{x}=0.04,\\ p_{y}=0.28italic_p start_POSTSUBSCRIPT italic_v italic_a italic_c end_POSTSUBSCRIPT = 0.68 , italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = 0.04 , italic_p start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT = 0.28 (Parameter #2). The \u201cstrong phase reference\u201d period is reduced to 100 ns in the 1 \u03bcs\ud835\udf07\ud835\udc60\\mu sitalic_\u03bc italic_s signal period; the intensity of the \u201cdim phase reference\u201d is set to the same as the quantum signal through the 100 ms period. As a result, the effective signal frequency is increased to 900 MHz. Besides, we used SNSPDs with >>>80% detection efficiency and a relatively higher dark count rate of about 10 Hz. The time window in data processing is set to 500 ps, yielding an almost unity efficiency. A total of 3.24\u00d710123.24superscript10123.24\\times 10^{12}3.24 \u00d7 10 start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT quantum signal pulses are sent for each distance, which equals to one hour of experimental time. The secure key rate is measured to be 1.24\u00d710\u221241.24superscript1041.24\\times 10^{-4}1.24 \u00d7 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT, 2.60\u00d710\u221252.60superscript1052.60\\times 10^{-5}2.60 \u00d7 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT, 3.11\u00d710\u221263.11superscript1063.11\\times 10^{-6}3.11 \u00d7 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT, and 3.76\u00d710\u221273.76superscript1073.76\\times 10^{-7}3.76 \u00d7 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT which corresponds to 111.74 kbps, 23.44 kbps, 2.80 kbps, and 338 bps for the 202 km, 303 km, 404 km, and 505 km fibre distances, respectively. The secure key rates exceed the absolute PLOB bound Pirandola et al. (2017) for the tests with the fibre distances equal to or longer than 404 km, where the PLOB bound is calculated as \u2212log2(1\u2212\u03b7)subscript21\ud835\udf02-\\log_{2}(1-\\eta)- roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 - italic_\u03b7 ) with the optical and detection efficiency in Charlie set to \u03b7opt=100%subscript\ud835\udf02\ud835\udc5c\ud835\udc5d\ud835\udc61percent100\\eta_{opt}=100\\%italic_\u03b7 start_POSTSUBSCRIPT italic_o italic_p italic_t end_POSTSUBSCRIPT = 100 %. In conclusion, we have demonstrated the first experiment of SNS-TF-QKD over a remarkable distance of 1002 km, while considering the finite size effect. The result has been made possible by employing several key components, including the ultra-low-loss fiber, ultra-low-noise SNSPD, dual-band phase stabilization method, and moderate data size. The achieved secure key rates over fiber distances ranging from 202 km to 505 km were highly practical, indicating the potential for supporting a wide range of applications. In addition to improving the performance, TF-QKD is also expected to be implemented in chip-scale systems in the future, based on recent advancements in chip-scale systems implementing BB84 and MDI-QKD protocols Paraiso et al. (2021); Zhu et al. (2022); Wei et al. (2020); Cao et al. (2020). Y.L. and W.-J.Z. contributed equally. The experimental results are summarized in Tab. 2. In the table, we denote N\ud835\udc41Nitalic_N as the total number of signal pulses, ntsubscript\ud835\udc5b\ud835\udc61n_{t}italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT(After AOPP) as the remaining pairs after AOPP, n1subscript\ud835\udc5b1n_{1}italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(Before AOPP) and n1subscript\ud835\udc5b1n_{1}italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(After AOPP) as the number of the untagged bits before and after AOPP; e1phsuperscriptsubscript\ud835\udc521\ud835\udc5d\u210ee_{1}^{ph}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p italic_h end_POSTSUPERSCRIPT(Before AOPP) and e1phsuperscriptsubscript\ud835\udc521\ud835\udc5d\u210ee_{1}^{ph}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p italic_h end_POSTSUPERSCRIPT(After AOPP) as the phase-flip error rate before and after AOPP, Etsubscript\ud835\udc38\ud835\udc61E_{t}italic_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT(Before AOPP) and Etsubscript\ud835\udc38\ud835\udc61E_{t}italic_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT(After AOPP) as the bit-flip error rate before and after the bit error rejection by active odd parity pairing (AOPP). With all the parameters in the table, the final key rate per pulse and in one second is calculated as R\ud835\udc45Ritalic_R (per pulse) and R\ud835\udc45Ritalic_R (bps), Exsubscript\ud835\udc38\ud835\udc65E_{x}italic_E start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT as the phase-flip error rate of the sifted bits. We note that the ultra-low QBER E(After AOPP) allows us to use a practical error correction inefficiency f=1.16\ud835\udc531.16f=1.16italic_f = 1.16 in calculating the secure key rate. In the following rows, we list the numbers of pulses Alice and Bob sent in different decoy states, labelled as \u201cSent-AB\u201d, where \u201cA\u201d (\u201cB\u201d) is \u201c0\u201d, \u201c1\u201d, or \u201c2\u201d, indicating the intensity Alice (Bob) has chosen within \u201cvacuum\u201d, \u201c\u03bcxsubscript\ud835\udf07\ud835\udc65\\mu_{x}italic_\u03bc start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT\u201d, or \u201c\u03bcysubscript\ud835\udf07\ud835\udc66\\mu_{y}italic_\u03bc start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT\u201d. With the same rule, the numbers of detections are listed as \u201cDetected-AB\u201d. The total valid detections reported by Charlie is denoted as \u201cDetected-Valid-ch\u201d, where \u201cch\u201d can be \u201cDet1\u201d or \u201cDet2\u201d indicating the responsive detector of the recorded counts. The valid events falls in the preset Ds angle range is denoted as \u201cDetected-11-Ds\u201d, the numbers of correct detections in this range is denoted as \u201cCorrect-11-Ds\u201d."
    },
    {
        "id": 11,
        "title": "Li-Yau Estimates for a Nonlinear Parabolic Equation on Finsler Manifolds",
        "corpus": "Abstract. In this paper, we explore the positive solutions to the Finslerian nonlinear equation which is related to Ricci solitons and serves as the Euler-Lagrange equation to the Finslerian log-energy functional. We then obtain the global gradient estimate of its positive solution on a compact Finsler metric measure space with the weighted Ricci curvature bounded below. Furthermore, using a new comparison theorem developed by the first author, we also establish a local gradient estimate on a non-compact forward complete Finsler metric measure spaces with the mixed weighted Ricci curvature bounded below, as well as finite bounds of misalignment and some non-Riemannian curvatures. Lastly, we prove the Harnack inequalities and a Liouville-type theorem of such solutions. Mathematics Subject Classification: 35K55, 53C60, 58J35 Keywords: Finslerian nonlinear equation, Mixed weighted Ricci curvature, Gradient estimate, Metric measure space Log-Sobolev inequalities were first introduced and studied by L. Gross [7] for Gaussian probability measure. More recently, S. Ohta [13] and S. Yin [23] proved that in the Finsler measure space (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ), for any f\u2208C0\u221e(M)\ud835\udc53subscriptsuperscript\ud835\udc360\ud835\udc40f\\in C^{\\infty}_{0}(M)italic_f \u2208 italic_C start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_M ) such that \u222bMf2\ud835\udc51\u03bc=Vold\u03bc(M)subscript\ud835\udc40superscript\ud835\udc532differential-d\ud835\udf07subscriptVol\ud835\udc51\ud835\udf07\ud835\udc40\\int_{M}f^{2}d\\mu=\\operatorname{Vol}_{d\\mu}(M)\u222b start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT italic_f start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d italic_\u03bc = roman_Vol start_POSTSUBSCRIPT italic_d italic_\u03bc end_POSTSUBSCRIPT ( italic_M ), where C\ud835\udc36Citalic_C is a constant that depends on different curvature conditions. In order to find the sharp constant in (1.1), as F. R. K. Chung and S.-T. Yau [4] did in the Riemannian case, we can also define the Finslerian log-Sobolev constant by Assume that u\ud835\udc62uitalic_u is a minimizer of the functional in (1.2), then by the variational calculation, u\ud835\udc62uitalic_u satisfies the Euler-Lagrange equation that which inspires us to consider this type of nonlinear parabolic equation where a\ud835\udc4eaitalic_a and b\ud835\udc4fbitalic_b are two real constants, \u0394\u2207usuperscript\u0394\u2207\ud835\udc62\\Delta^{\\nabla u}roman_\u0394 start_POSTSUPERSCRIPT \u2207 italic_u end_POSTSUPERSCRIPT is nonlinear Finslerian Laplacian. Another motivation is to understand the Ricci soliton defined by R. Hamilton [8], which is a Riemannian manifold (M,g)\ud835\udc40\ud835\udc54(M,g)( italic_M , italic_g ) with a smooth function f\u2208C\u221e(M)\ud835\udc53superscript\ud835\udc36\ud835\udc40f\\in C^{\\infty}(M)italic_f \u2208 italic_C start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT ( italic_M ), such that for some constant c\u2208\u211d\ud835\udc50\u211dc\\in\\mathbb{R}italic_c \u2208 blackboard_R, it satisfies where \u22072superscript\u22072\\nabla^{2}\u2207 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and RicRic\\operatorname{Ric}roman_Ric are the Hessian tensor and Ricci tensor of the metric g\ud835\udc54gitalic_g, respectively. The Finslerian Ricci solitons have been investigated by H. Zhu [24] and some other results of rigidity and characterizations can be seen in [25, 3]. Setting u=ef\ud835\udc62superscript\ud835\udc52\ud835\udc53u=e^{f}italic_u = italic_e start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT in (1.5), one can deduce that where A0subscript\ud835\udc340A_{0}italic_A start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is a constant (see [11]). For this kind of elliptic equation, L. Ma [11] established a Li-Yau estimate on a complete Riemannian manifold with Ricci curvature bounded below by a negative constant. Afterwards, Y. Yang [22] generalized the estimate to the parabolic equation \u2202tu=\u0394u+aulogu+busubscript\ud835\udc61\ud835\udc62\u0394\ud835\udc62\ud835\udc4e\ud835\udc62\ud835\udc62\ud835\udc4f\ud835\udc62\\partial_{t}u=\\Delta u+au\\log u+bu\u2202 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_u = roman_\u0394 italic_u + italic_a italic_u roman_log italic_u + italic_b italic_u. Then G. Huang and B. Ma [9] also considered the weighted Laplacian \u0394f=\u0394\u2212\u2207f\u2207subscript\u0394\ud835\udc53\u0394\u2207\ud835\udc53\u2207\\Delta_{f}=\\Delta-\\nabla f\\nablaroman_\u0394 start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = roman_\u0394 - \u2207 italic_f \u2207, and replaced curvature condition with the N\ud835\udc41Nitalic_N-Bakry-Emery Ricci tensor. Apart from Li-Yau esitimates, some other types of gradient estimates like Souplet-Zhang esitimates have been obtained on this nonlinear equation by X. Fu [6] and N. Dung [5]. Although numerous studies have been carried out on gradient estimates for equations on Riemannian manifolds, there are not many results on Finsler manifolds yet. S. Ohta and K.-T. Sturm have introduced the weighted Ricci curvature on a Finsler manifold as a generalization of Bakry-Emery Ricci curvature in Riemannian geometry (cf. [14] and [12]) and proved a Finslerian version of Bochner\u2013Weitzenb\u00f6ck formula (cf. [15]), which they applied to derive Li-Yau estimate for heat equation on a compact Finsler manifold. Following their serial work, we can generalize the results in [11], [22] and [9] to the nonlinear parabolic equation (1.4) on Finsler manifold. The first main result in compact Finslerian case is the following: Let (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ) be an n\ud835\udc5bnitalic_n-dimensional compact Finsler manifold . For some integer N\u2208(n,\u221e)\ud835\udc41\ud835\udc5bN\\in(n,\\infty)italic_N \u2208 ( italic_n , \u221e ), suppose the weighted Ricci curvature RicN\u2265\u2212K\ud835\udc45\ud835\udc56superscript\ud835\udc50\ud835\udc41\ud835\udc3eRic^{N}\\geq-Kitalic_R italic_i italic_c start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2265 - italic_K where K\ud835\udc3eKitalic_K is a nonnegative constant. If u(x,t)\ud835\udc62\ud835\udc65\ud835\udc61u(x,t)italic_u ( italic_x , italic_t ) is a positive smooth global solution to equation (1.4) on M\u00d7[0,+\u221e)\ud835\udc400M\\times[0,+\\infty)italic_M \u00d7 [ 0 , + \u221e ) where a\u22600\ud835\udc4e0a\\neq 0italic_a \u2260 0, let f(x,t)=logu(x,t)\ud835\udc53\ud835\udc65\ud835\udc61\ud835\udc62\ud835\udc65\ud835\udc61f(x,t)=\\log u(x,t)italic_f ( italic_x , italic_t ) = roman_log italic_u ( italic_x , italic_t ) and \u03b1=max{a/2,\u2212a/4}\ud835\udefc\ud835\udc4e2\ud835\udc4e4\\alpha=\\max\\{a/2,-a/4\\}italic_\u03b1 = roman_max { italic_a / 2 , - italic_a / 4 }. Then, for any \u03b2>1\ud835\udefd1\\beta>1italic_\u03b2 > 1, x\u2208M\ud835\udc65\ud835\udc40x\\in Mitalic_x \u2208 italic_M and t>0\ud835\udc610t>0italic_t > 0, we have In particular, as a\u21920\u2192\ud835\udc4e0a\\to 0italic_a \u2192 0 and b=0\ud835\udc4f0b=0italic_b = 0 , the equation (1.4) simplifies to a common heat equation on a Finsler manifold. In this special case, the gradient estimate (1.7) would be the same as the result by Ohta and Sturm (cf. Theorem 4.4 in [15]). It is remarkable that S. Ohta [12] showed that the lower curvature bound (RicN\u2a7e\u2212KsuperscriptRic\ud835\udc41\ud835\udc3e\\operatorname{Ric}^{N}\\geqslant-Kroman_Ric start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2a7e - italic_K) is equivalent to the curvature-dimension condition CD(\u2212K,N)\ud835\udc36\ud835\udc37\ud835\udc3e\ud835\udc41CD(-K,N)italic_C italic_D ( - italic_K , italic_N ) defined by Lott-Villani-Sturm [10, 18, 19]. Further challenge, however, arose in the case of complete but non-compact Finsler manifold. Although Q. Xia [21] gave a gradient estimate on non-compact case, that method is feasible only for Finslerian heat equation. Due to the latest progress in comparison theorem made in [16], we have demonstrated the following gradient estimate. Let (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ) be a forward complete n\ud835\udc5bnitalic_n-dimensional Finsler metric measure space without boundary. Denote by Bp(2R)subscript\ud835\udc35\ud835\udc5d2\ud835\udc45B_{p}(2R)italic_B start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( 2 italic_R ) the forward geodesic ball centering p\u2208M\ud835\udc5d\ud835\udc40p\\in Mitalic_p \u2208 italic_M with radius 2R2\ud835\udc452R2 italic_R, and by r(x):=d(p,x)assign\ud835\udc5f\ud835\udc65\ud835\udc51\ud835\udc5d\ud835\udc65r(x):=d(p,x)italic_r ( italic_x ) := italic_d ( italic_p , italic_x ) the distance function from p\ud835\udc5dpitalic_p. Suppose for some N\u2208(n,\u221e)\ud835\udc41\ud835\udc5bN\\in(n,\\infty)italic_N \u2208 ( italic_n , \u221e ), the mixed weighted Ricci curvature Ric\u2207rNmsuperscriptsuperscriptsubscriptnormal-Ricnormal-\u2207\ud835\udc5f\ud835\udc41\ud835\udc5a{}^{m}\\operatorname{Ric}_{\\nabla r}^{N}start_FLOATSUPERSCRIPT italic_m end_FLOATSUPERSCRIPT roman_Ric start_POSTSUBSCRIPT \u2207 italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT is bounded below by constant \u2212K(2R)\ud835\udc3e2\ud835\udc45-K(2R)- italic_K ( 2 italic_R ) (R>0\ud835\udc450R>0italic_R > 0 and K(2R)\u2a7e0\ud835\udc3e2\ud835\udc450K(2R)\\geqslant 0italic_K ( 2 italic_R ) \u2a7e 0) in Bp(2R)subscript\ud835\udc35\ud835\udc5d2\ud835\udc45B_{p}(2R)italic_B start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( 2 italic_R ). Suppose M\ud835\udc40Mitalic_M has finite upper bounds of misalignment and non-Riemannian tensors by A(2R)\ud835\udc342\ud835\udc45A(2R)italic_A ( 2 italic_R ) and K0subscript\ud835\udc3e0K_{0}italic_K start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT respectively. If u(x,t)\ud835\udc62\ud835\udc65\ud835\udc61u(x,t)italic_u ( italic_x , italic_t ) is a positive solution to equation (1.4) on M\u00d7[0,+\u221e)\ud835\udc400M\\times[0,+\\infty)italic_M \u00d7 [ 0 , + \u221e ) where a\u22600\ud835\udc4e0a\\neq 0italic_a \u2260 0, by setting f(x,t)=logu(x,t)\ud835\udc53\ud835\udc65\ud835\udc61\ud835\udc62\ud835\udc65\ud835\udc61f(x,t)=\\log u(x,t)italic_f ( italic_x , italic_t ) = roman_log italic_u ( italic_x , italic_t ) and \u03b1=max{a/2,\u2212a/4}\ud835\udefc\ud835\udc4e2\ud835\udc4e4\\alpha=\\max\\{a/2,-a/4\\}italic_\u03b1 = roman_max { italic_a / 2 , - italic_a / 4 }, then, for any \u03b2>1\ud835\udefd1\\beta>1italic_\u03b2 > 1, 0<\u03b4<10\ud835\udeff10<\\delta<10 < italic_\u03b4 < 1, x\u2208Bp(R)\ud835\udc65subscript\ud835\udc35\ud835\udc5d\ud835\udc45x\\in B_{p}(R)italic_x \u2208 italic_B start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_R ) and t>0\ud835\udc610t>0italic_t > 0, we have where c1subscript\ud835\udc501c_{1}italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, c2subscript\ud835\udc502c_{2}italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and c3(N,A,K0)subscript\ud835\udc503\ud835\udc41\ud835\udc34subscript\ud835\udc3e0c_{3}(N,A,K_{0})italic_c start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_N , italic_A , italic_K start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) are all positive constants. When dealing with non-compactness, the curvature conditions necessitate the bounds of misalignment and non-Riemannian quantities (further elaborated in Section 2 or [16]). Our conditions are marginally stronger in contrast to those presented in [21]. Nevertheless, our method can be adopted to other Finslerian equations. It is also worth noting that if the manifold degenerates to a Riemannian one, these conditions are just equivalent to the (N\u2212n)\ud835\udc41\ud835\udc5b(N-n)( italic_N - italic_n )-Bakry-Emery Ricci curvature bounded from below. Hence the gradient estimate (1.8) is more general than the one in [9]. Based on the aforementioned estimates, there are several noteworthy results including Harnack\u2019s inequalities and the Liouville-type theorem for a positive smooth solution, on which we will give a detailed elaboration in Section 5. Under the same conditions in Theorem 1.1, we have where for any \u03b2>1\ud835\udefd1\\beta>1italic_\u03b2 > 1, 0<t1<t2<T0subscript\ud835\udc611subscript\ud835\udc612\ud835\udc470<t_{1}<t_{2}<T0 < italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < italic_T and x1subscript\ud835\udc651x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, x2\u2208Msubscript\ud835\udc652\ud835\udc40x_{2}\\in Mitalic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2208 italic_M. Taking a\u21920\u2192\ud835\udc4e0a\\to 0italic_a \u2192 0 and b=0\ud835\udc4f0b=0italic_b = 0 in (1.9), one will find that the Harnack inequality derived from the heat equation in [15] is also a special case of (1.9). Moreover, it can also be extended to non-compact Finsler manifold. Under the assumption in Theorem 1.2, we have where for any 0<\u03b4<10\ud835\udeff10<\\delta<10 < italic_\u03b4 < 1, \u03b2>1\ud835\udefd1\\beta>1italic_\u03b2 > 1, 0<t1<t2<T0subscript\ud835\udc611subscript\ud835\udc612\ud835\udc470<t_{1}<t_{2}<T0 < italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < italic_T and x1subscript\ud835\udc651x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, x2\u2208Bp(R)subscript\ud835\udc652subscript\ud835\udc35\ud835\udc5d\ud835\udc45x_{2}\\in B_{p}(R)italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2208 italic_B start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_R ) . In particular, assume that the solution u\ud835\udc62uitalic_u is independent of t\ud835\udc61titalic_t, that is, u\ud835\udc62uitalic_u solves the elliptic equation where a\u22600\ud835\udc4e0a\\neq 0italic_a \u2260 0, We then have the following Liouville-type theorem. Let (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ) be a forward complete n\ud835\udc5bnitalic_n-dimensional Finsler metric measure space without boundary, with misalignment and non-Riemannian tensors bounded by A\ud835\udc34Aitalic_A and K0subscript\ud835\udc3e0K_{0}italic_K start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, respectively. Let u\ud835\udc62uitalic_u be a positive solution to (1.11) on M\ud835\udc40Mitalic_M and f=logu\ud835\udc53\ud835\udc62f=\\log uitalic_f = roman_log italic_u. Suppose mixed weighted Ricci curvature RicNm\u2a7e\u2212Ksuperscriptsuperscriptnormal-Ric\ud835\udc41\ud835\udc5a\ud835\udc3e{}^{m}\\operatorname{Ric}^{N}\\geqslant-Kstart_FLOATSUPERSCRIPT italic_m end_FLOATSUPERSCRIPT roman_Ric start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2a7e - italic_K for some N\ud835\udc41Nitalic_N and K\ud835\udc3eKitalic_K satisfying where \u03b1=max{a/2,\u2212a/4}\ud835\udefc\ud835\udc4e2\ud835\udc4e4\\alpha=\\max\\{a/2,-a/4\\}italic_\u03b1 = roman_max { italic_a / 2 , - italic_a / 4 }. If there exists an x0\u2208Msubscript\ud835\udc650\ud835\udc40x_{0}\\in Mitalic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2208 italic_M such that or we then have f(x)\u22610\ud835\udc53\ud835\udc650f(x)\\equiv 0italic_f ( italic_x ) \u2261 0 on M\ud835\udc40Mitalic_M. When b=0\ud835\udc4f0b=0italic_b = 0, K=0\ud835\udc3e0K=0italic_K = 0 and M\ud835\udc40Mitalic_M is complete non-compact Riemannian manifold, Y. Yang [22] has proved that for a>0\ud835\udc4e0a>0italic_a > 0, u(x)\u2a7den/2\ud835\udc62\ud835\udc65superscript\ud835\udc52\ud835\udc5b2u(x)\\leqslant e^{n/2}italic_u ( italic_x ) \u2a7d italic_e start_POSTSUPERSCRIPT italic_n / 2 end_POSTSUPERSCRIPT, and for a<0,w(x)\u2a7ee\u2212n/4formulae-sequence\ud835\udc4e0\ud835\udc64\ud835\udc65superscript\ud835\udc52\ud835\udc5b4a<0,w(x)\\geqslant e^{-n/4}italic_a < 0 , italic_w ( italic_x ) \u2a7e italic_e start_POSTSUPERSCRIPT - italic_n / 4 end_POSTSUPERSCRIPT, indicating that u(x)\ud835\udc62\ud835\udc65u(x)italic_u ( italic_x ) has a gap when a<0\ud835\udc4e0a<0italic_a < 0. Then X. Cao, et al. [2] further improved the gap to u(x)\u2a7ee\u2212n/16\ud835\udc62\ud835\udc65superscript\ud835\udc52\ud835\udc5b16u(x)\\geqslant e^{-n/16}italic_u ( italic_x ) \u2a7e italic_e start_POSTSUPERSCRIPT - italic_n / 16 end_POSTSUPERSCRIPT when M\ud835\udc40Mitalic_M is non-compact and u(x)\u2a7e1\ud835\udc62\ud835\udc651u(x)\\geqslant 1italic_u ( italic_x ) \u2a7e 1 when M\ud835\udc40Mitalic_M is closed. By Theorem 1.5 above, we supplement that when b>n\u03b1/2\ud835\udc4f\ud835\udc5b\ud835\udefc2b>n\\alpha/2italic_b > italic_n italic_\u03b1 / 2, u(x)\ud835\udc62\ud835\udc65u(x)italic_u ( italic_x ) also has a gap u(x)\u2a7e1\ud835\udc62\ud835\udc651u(x)\\geqslant 1italic_u ( italic_x ) \u2a7e 1. This manuscript is structured as follows. In Section 2, we offer a brief interpretation of the notions in Finsler geometry. Furthermore, we introduce the Finslerian log-energy functional, the Finslerian log-Sobolev constant and related variational equation. Then in Section 3, we demonstrate global gradient estimates of positive solutions with the CD(\u2212K,N)\ud835\udc36\ud835\udc37\ud835\udc3e\ud835\udc41CD(-K,N)italic_C italic_D ( - italic_K , italic_N ) condition. In Section 4, we show local gradient estimates with the mixed weighted Ricci curvature condition. Finally, Section 5 is devoted to presenting some applications such as Harnack inequalities and a Liouville-type theorem. In this section, we briefly review some definitions and theorems in Finsler geometry (see [1] and [17]). A Finsler manifold is a pair (M,F)\ud835\udc40\ud835\udc39(M,F)( italic_M , italic_F ), where M\ud835\udc40Mitalic_M is an n\ud835\udc5bnitalic_n-dimensional connected smooth manifold and F:TM\u2192[0,\u221e):\ud835\udc39\u2192\ud835\udc47\ud835\udc400F:TM\\rightarrow[0,\\infty)italic_F : italic_T italic_M \u2192 [ 0 , \u221e ), called Finsler structure, is a nonnegative function satisfying F\u2208C\u221e(TM\\\ud835\udfce)\ud835\udc39superscript\ud835\udc36\\\ud835\udc47\ud835\udc400F\\in{C}^{\\infty}(TM\\backslash\\mathbf{0})italic_F \u2208 italic_C start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT ( italic_T italic_M \\ bold_0 ), where \ud835\udfce0\\mathbf{0}bold_0 is the zero section; F(x,cv)=cF(x,v)\ud835\udc39\ud835\udc65\ud835\udc50\ud835\udc63\ud835\udc50\ud835\udc39\ud835\udc65\ud835\udc63F(x,cv)=cF(x,v)italic_F ( italic_x , italic_c italic_v ) = italic_c italic_F ( italic_x , italic_v ) for all (x,v)\u2208TM\ud835\udc65\ud835\udc63\ud835\udc47\ud835\udc40(x,v)\\in TM( italic_x , italic_v ) \u2208 italic_T italic_M and c\u2a7e0\ud835\udc500c\\geqslant 0italic_c \u2a7e 0 The n\u00d7n\ud835\udc5b\ud835\udc5bn\\times nitalic_n \u00d7 italic_n matrix is positive-definite for all (x,v)\u2208TM\\\ud835\udfce\ud835\udc65\ud835\udc63\\\ud835\udc47\ud835\udc400(x,v)\\in TM\\backslash\\mathbf{0}( italic_x , italic_v ) \u2208 italic_T italic_M \\ bold_0. In term of Finsler structure F\ud835\udc39Fitalic_F, any vector field V\u2208TM\\\ud835\udfce\ud835\udc49\\\ud835\udc47\ud835\udc400V\\in TM\\backslash\\mathbf{0}italic_V \u2208 italic_T italic_M \\ bold_0 induces a Riemannian structure gVsubscript\ud835\udc54\ud835\udc49g_{V}italic_g start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT by and a norm FV(\u22c5)subscript\ud835\udc39\ud835\udc49\u22c5F_{V}(\\cdot)italic_F start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ( \u22c5 ) by In particular, the norm of reference vector is FV(V)=F(V)subscript\ud835\udc39\ud835\udc49\ud835\udc49\ud835\udc39\ud835\udc49F_{V}(V)=F(V)italic_F start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ( italic_V ) = italic_F ( italic_V ). There is an almost g\ud835\udc54gitalic_g-compatible and torsion-free connection DVsuperscript\ud835\udc37\ud835\udc49D^{V}italic_D start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT called Chern connection on the pull-back tangent bundle \u03c0*TMsuperscript\ud835\udf0b\ud835\udc47\ud835\udc40\\pi^{*}TMitalic_\u03c0 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT italic_T italic_M. Namely, for any X\ud835\udc4bXitalic_X, Y\ud835\udc4cYitalic_Y, Z\u2208TM\ud835\udc4d\ud835\udc47\ud835\udc40Z\\in TMitalic_Z \u2208 italic_T italic_M, where the Cartan tensor CVsubscript\ud835\udc36\ud835\udc49C_{V}italic_C start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT is one of non-Riemannian tensors given by Also letting \u0393jki(V)subscriptsuperscript\u0393\ud835\udc56\ud835\udc57\ud835\udc58\ud835\udc49\\Gamma^{i}_{jk}(V)roman_\u0393 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j italic_k end_POSTSUBSCRIPT ( italic_V ) denote the coefficients of Chern connection, one defines the spray coefficients as and the spray where \u03b4\u03b4xi=\u2202\u2202xi\u2212Nij\u2202\u2202vj\ud835\udeff\ud835\udeffsuperscript\ud835\udc65\ud835\udc56superscript\ud835\udc65\ud835\udc56subscriptsuperscript\ud835\udc41\ud835\udc57\ud835\udc56superscript\ud835\udc63\ud835\udc57\\frac{\\delta}{\\delta x^{i}}=\\frac{\\partial}{\\partial x^{i}}-N^{j}_{i}\\frac{% \\partial}{\\partial v^{j}}divide start_ARG italic_\u03b4 end_ARG start_ARG italic_\u03b4 italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_ARG = divide start_ARG \u2202 end_ARG start_ARG \u2202 italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_ARG - italic_N start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT divide start_ARG \u2202 end_ARG start_ARG \u2202 italic_v start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_ARG and Nij=\u2202Gj\u2202visubscriptsuperscript\ud835\udc41\ud835\udc57\ud835\udc56superscript\ud835\udc3a\ud835\udc57superscript\ud835\udc63\ud835\udc56N^{j}_{i}=\\frac{\\partial G^{j}}{\\partial v^{i}}italic_N start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG \u2202 italic_G start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_ARG start_ARG \u2202 italic_v start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_ARG. Customarily, we denote the horizontal and vertical Chern derivatives by \u201c||||\u201d and \u201c; \u201d, respectively. For example, for some tensor T=Tidxi\ud835\udc47subscript\ud835\udc47\ud835\udc56\ud835\udc51superscript\ud835\udc65\ud835\udc56T=T_{i}dx^{i}italic_T = italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_d italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT on the pull-back bundle, The spray coefficients could induce the geodesic, a curve \u03b3\ud835\udefe\\gammaitalic_\u03b3 on M\ud835\udc40Mitalic_M satisfying A forward geodesic ball centered at p\ud835\udc5dpitalic_p with radius R\ud835\udc45Ritalic_R can be represented by Here the forward distance from p\ud835\udc5dpitalic_p to q\ud835\udc5eqitalic_q is given by where the infimum is taken over all the C1superscript\ud835\udc361C^{1}italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT curves \u03b3\ud835\udefe\\gammaitalic_\u03b3 : [0,1]\u2192M\u219201\ud835\udc40[0,1]\\to M[ 0 , 1 ] \u2192 italic_M such that \u03b3(0)=p\ud835\udefe0\ud835\udc5d\\gamma(0)=pitalic_\u03b3 ( 0 ) = italic_p and \u03b3(1)=q\ud835\udefe1\ud835\udc5e\\gamma(1)=qitalic_\u03b3 ( 1 ) = italic_q. Different from Riemannian manifold, the distance function is not necessarily symmetric between p\ud835\udc5dpitalic_p and q\ud835\udc5eqitalic_q. Adopting the exponential map, a Finsler manifold (M,F)\ud835\udc40\ud835\udc39(M,F)( italic_M , italic_F ) is said to be forward complete if the exponential map is defined over TM\ud835\udc47\ud835\udc40TMitalic_T italic_M. Thus, any two points in a forward complete manifold M\ud835\udc40Mitalic_M can be connected by a minimal geodesic. Additionally, the forward closed ball Bp+(R)\u00af\u00afsubscriptsuperscript\ud835\udc35\ud835\udc5d\ud835\udc45\\overline{B^{+}_{p}(R)}over\u00af start_ARG italic_B start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_R ) end_ARG is compact. The curvature form \u03a9\u03a9\\Omegaroman_\u03a9 induced by Chern connection can be divided into Chern Riemannian curvature R\ud835\udc45Ritalic_R and Chern non-Riemannian connection P\ud835\udc43Pitalic_P as The flag curvature with pole v\ud835\udc63vitalic_v is locally expressed by for any two linearly independent vectors u,v\u2208TxM\\\ud835\udfce\ud835\udc62\ud835\udc63\\subscript\ud835\udc47\ud835\udc65\ud835\udc400u,v\\in T_{x}M\\backslash\\mathbf{0}italic_u , italic_v \u2208 italic_T start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_M \\ bold_0, which span a tangent plane \u03a0=span{u,v}\u03a0span\ud835\udc62\ud835\udc63\\Pi=\\operatorname{span}\\{u,v\\}roman_\u03a0 = roman_span { italic_u , italic_v }. Then the Finslerian Ricci curvature is defined by where {e1,\u2026en\u22121,v/F(v)}subscript\ud835\udc521\u2026subscript\ud835\udc52\ud835\udc5b1\ud835\udc63\ud835\udc39\ud835\udc63\\{e_{1},...e_{n-1},v/F(v)\\}{ italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 italic_e start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , italic_v / italic_F ( italic_v ) } is an orthonormal basis of TxMsubscript\ud835\udc47\ud835\udc65\ud835\udc40T_{x}Mitalic_T start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_M with respect to gvsubscript\ud835\udc54\ud835\udc63g_{v}italic_g start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. The T-curvature is another non-Riemannian quantity, given by where v\u2208TxM,V\ud835\udc63subscript\ud835\udc47\ud835\udc65\ud835\udc40\ud835\udc49v\\in T_{x}M,Vitalic_v \u2208 italic_T start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_M , italic_V is a vector field with V(x)=v\ud835\udc49\ud835\udc65\ud835\udc63V(x)=vitalic_V ( italic_x ) = italic_v, and D^^\ud835\udc37\\hat{D}over^ start_ARG italic_D end_ARG denotes the Levi-Civita connection of the induced Riemannian metric g^=gY^\ud835\udc54subscript\ud835\udc54\ud835\udc4c\\hat{g}=g_{Y}over^ start_ARG italic_g end_ARG = italic_g start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT. The T-curvature vanishes if and only if the Chern non-Riemannian curvature P\ud835\udc43Pitalic_P vanishes. To find the trace of T-curvature, let {ei}subscript\ud835\udc52\ud835\udc56\\left\\{e_{i}\\right\\}{ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } be an orthonormal basis with respect to metric gVsubscript\ud835\udc54\ud835\udc49g_{V}italic_g start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT at point x\ud835\udc65xitalic_x, where V\ud835\udc49Vitalic_V is a fixed reference vector field. Moreover, Let {Ei}subscript\ud835\udc38\ud835\udc56\\left\\{E_{i}\\right\\}{ italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } be the local vector fields obtained by moving {ei}subscript\ud835\udc52\ud835\udc56\\left\\{e_{i}\\right\\}{ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } in parallel in a neighborhood of x\ud835\udc65xitalic_x on M\ud835\udc40Mitalic_M. define the tensor U\ud835\udc48Uitalic_U as for any local vector field W\ud835\udc4aWitalic_W, with We introduce an important constant on a Finsler manifold defined in [16], called the misalignment. (misalignment) For a Finsler manifold (M,F)\ud835\udc40\ud835\udc39(M,F)( italic_M , italic_F ), the misalignment of a Finsler metric at x\u2208M\ud835\udc65\ud835\udc40x\\in Mitalic_x \u2208 italic_M is defined by and the global misalignment of the Finsler metric by In particular, a Finsler manifold (M,F)\ud835\udc40\ud835\udc39(M,F)( italic_M , italic_F ) is Riemannian if and only if \u03b1(x)\u22611\ud835\udefc\ud835\udc651\\alpha(x)\\equiv 1italic_\u03b1 ( italic_x ) \u2261 1. More characterizations of the misalignment can be found in [16]. The Legendre transform \u2112*:T*M\u2192TM:superscript\u2112\u2192superscript\ud835\udc47\ud835\udc40\ud835\udc47\ud835\udc40\\mathcal{L}^{*}:T^{*}M\\rightarrow TMcaligraphic_L start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT : italic_T start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT italic_M \u2192 italic_T italic_M maps \u03b1\u2208Tx*M\ud835\udefcsuperscriptsubscript\ud835\udc47\ud835\udc65\ud835\udc40\\alpha\\in T_{x}^{*}Mitalic_\u03b1 \u2208 italic_T start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT italic_M to the unique element v\u2208TxM\ud835\udc63subscript\ud835\udc47\ud835\udc65\ud835\udc40v\\in T_{x}Mitalic_v \u2208 italic_T start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_M such that \u03b1(v)=F*(\u03b1)2\ud835\udefc\ud835\udc63superscript\ud835\udc39superscript\ud835\udefc2\\alpha(v)=F^{*}(\\alpha)^{2}italic_\u03b1 ( italic_v ) = italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_\u03b1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT where F*(\u03b1):=F(v)assignsuperscript\ud835\udc39\ud835\udefc\ud835\udc39\ud835\udc63F^{*}(\\alpha):=F(v)italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_\u03b1 ) := italic_F ( italic_v ) is the dual norm of F\ud835\udc39Fitalic_F. For a differentiable function f:M\u2192\u211d:\ud835\udc53\u2192\ud835\udc40\u211df:M\\rightarrow\\mathbb{R}italic_f : italic_M \u2192 blackboard_R, the gradient vector of u\ud835\udc62uitalic_u at x\ud835\udc65xitalic_x is defined by \u2207u(x):=\u2112*(Du(x))\u2208TxMassign\u2207\ud835\udc62\ud835\udc65superscript\u2112\ud835\udc37\ud835\udc62\ud835\udc65subscript\ud835\udc47\ud835\udc65\ud835\udc40\\nabla u(x):=\\mathcal{L}^{*}(Du(x))\\in T_{x}M\u2207 italic_u ( italic_x ) := caligraphic_L start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_D italic_u ( italic_x ) ) \u2208 italic_T start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_M. If Du(x)=0\ud835\udc37\ud835\udc62\ud835\udc650Du(x)=0italic_D italic_u ( italic_x ) = 0, then clearly \u2207u(x)=0\u2207\ud835\udc62\ud835\udc650\\nabla u(x)=0\u2207 italic_u ( italic_x ) = 0. If Du(x)\u22600\ud835\udc37\ud835\udc62\ud835\udc650Du(x)\\neq 0italic_D italic_u ( italic_x ) \u2260 0, and we can write in coordinates On Mu:={x\u2208M:Du(x)\u2260\ud835\udfce}assignsubscript\ud835\udc40\ud835\udc62conditional-set\ud835\udc65\ud835\udc40\ud835\udc37\ud835\udc62\ud835\udc650M_{u}:=\\{x\\in M:Du(x)\\neq\\mathbf{0}\\}italic_M start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT := { italic_x \u2208 italic_M : italic_D italic_u ( italic_x ) \u2260 bold_0 }, we also define the Hessian of u\ud835\udc62uitalic_u by The symmetricity of Hessian tensor can be shown in [15]. A Finsler metric measure space (M,F,d\u03bc)\ud835\udc40\ud835\udc39\ud835\udc51\ud835\udf07(M,F,d\\mu)( italic_M , italic_F , italic_d italic_\u03bc ) is a Finsler manifold equipped with an given measure \u03bc\ud835\udf07\\muitalic_\u03bc. In local coordinates {xi}i=1nsuperscriptsubscriptsuperscript\ud835\udc65\ud835\udc56\ud835\udc561\ud835\udc5b\\left\\{x^{i}\\right\\}_{i=1}^{n}{ italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, the volume form can be expressed as d\u03bc=\u03c3(x)dx1\u2227\u22ef\u2227dxn\ud835\udc51\ud835\udf07\ud835\udf0e\ud835\udc65\ud835\udc51superscript\ud835\udc651\u22ef\ud835\udc51superscript\ud835\udc65\ud835\udc5bd\\mu=\\sigma(x)dx^{1}\\wedge\\cdots\\wedge dx^{n}italic_d italic_\u03bc = italic_\u03c3 ( italic_x ) italic_d italic_x start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT \u2227 \u22ef \u2227 italic_d italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. For any y\u2208TxM\\{0}\ud835\udc66\\subscript\ud835\udc47\ud835\udc65\ud835\udc400y\\in T_{x}M\\backslash\\{0\\}italic_y \u2208 italic_T start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_M \\ { 0 }, define the distortion by For any point x\u2208M\ud835\udc65\ud835\udc40x\\in Mitalic_x \u2208 italic_M, let \u03b3=\u03b3(t)\ud835\udefe\ud835\udefe\ud835\udc61\\gamma=\\gamma(t)italic_\u03b3 = italic_\u03b3 ( italic_t ) be a forward geodesic from x\ud835\udc65xitalic_x with the initial tangent vector \u03b3\u02d9(0)=v\u02d9\ud835\udefe0\ud835\udc63\\dot{\\gamma}(0)=vover\u02d9 start_ARG italic_\u03b3 end_ARG ( 0 ) = italic_v. The S\ud835\udc46Sitalic_S-curvature of (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ) is Modeling the definition of T-curvature, Shen [16] defined another non-Riemannian curvature for vector fields V\ud835\udc49Vitalic_V, W\ud835\udc4aWitalic_W on M\ud835\udc40Mitalic_M. Intuitively, S\ud835\udc46Sitalic_S-curvature is the changing of distortion along the geodesic in direction v\ud835\udc63vitalic_v and \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T is the difference of \u2207\u03c4\u2207\ud835\udf0f\\nabla\\tau\u2207 italic_\u03c4 on the tangent sphere. In local coordinates {xi}superscript\ud835\udc65\ud835\udc56\\{x^{i}\\}{ italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT }, let d\u03bc=e\u03a6dx1\u2026dxn\ud835\udc51\ud835\udf07superscript\ud835\udc52\u03a6\ud835\udc51superscript\ud835\udc651\u2026\ud835\udc51superscript\ud835\udc65\ud835\udc5bd\\mu=e^{\\Phi}dx^{1}...dx^{n}italic_d italic_\u03bc = italic_e start_POSTSUPERSCRIPT roman_\u03a6 end_POSTSUPERSCRIPT italic_d italic_x start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT \u2026 italic_d italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT express the volume form, then the divergence of a smooth vector field V\ud835\udc49Vitalic_V can be written as The Finslerian Laplacian of a function f\ud835\udc53fitalic_f on M\ud835\udc40Mitalic_M could now be given by On Mf:={x\u2208M:\u2207f\u2260\ud835\udfce}assignsubscript\ud835\udc40\ud835\udc53conditional-set\ud835\udc65\ud835\udc40\u2207\ud835\udc530M_{f}:=\\{x\\in M:\\nabla f\\neq\\mathbf{0}\\}italic_M start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT := { italic_x \u2208 italic_M : \u2207 italic_f \u2260 bold_0 }, one notices that \u0394d\u03bc\u2207ff=\u0394d\u03bcfsuperscriptsubscript\u0394\ud835\udc51\ud835\udf07\u2207\ud835\udc53\ud835\udc53subscript\u0394\ud835\udc51\ud835\udf07\ud835\udc53\\Delta_{d\\mu}^{\\nabla f}f=\\Delta_{d\\mu}froman_\u0394 start_POSTSUBSCRIPT italic_d italic_\u03bc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2207 italic_f end_POSTSUPERSCRIPT italic_f = roman_\u0394 start_POSTSUBSCRIPT italic_d italic_\u03bc end_POSTSUBSCRIPT italic_f where \u0394d\u03bc\u2207fsuperscriptsubscript\u0394\ud835\udc51\ud835\udf07\u2207\ud835\udc53\\Delta_{d\\mu}^{\\nabla f}roman_\u0394 start_POSTSUBSCRIPT italic_d italic_\u03bc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2207 italic_f end_POSTSUPERSCRIPT is the weighted Laplacian defined by \u0394d\u03bc\u2207ff:=divd\u03bc(\u2207\u2207ff)assignsuperscriptsubscript\u0394\ud835\udc51\ud835\udf07\u2207\ud835\udc53\ud835\udc53subscriptdiv\ud835\udc51\ud835\udf07superscript\u2207\u2207\ud835\udc53\ud835\udc53\\Delta_{d\\mu}^{\\nabla f}f:=\\operatorname{div}_{d\\mu}\\left(\\nabla^{\\nabla f}f\\right)roman_\u0394 start_POSTSUBSCRIPT italic_d italic_\u03bc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2207 italic_f end_POSTSUPERSCRIPT italic_f := roman_div start_POSTSUBSCRIPT italic_d italic_\u03bc end_POSTSUBSCRIPT ( \u2207 start_POSTSUPERSCRIPT \u2207 italic_f end_POSTSUPERSCRIPT italic_f ) where for a non-vanishing vector field V\ud835\udc49Vitalic_V. In particular, \u2207\u2207ff=\u2207fsuperscript\u2207\u2207\ud835\udc53\ud835\udc53\u2207\ud835\udc53\\nabla^{\\nabla f}f=\\nabla f\u2207 start_POSTSUPERSCRIPT \u2207 italic_f end_POSTSUPERSCRIPT italic_f = \u2207 italic_f also holds on Mfsubscript\ud835\udc40\ud835\udc53M_{f}italic_M start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. So it is valid to use \u2207=\u2207\u2207f\u2207superscript\u2207\u2207\ud835\udc53\\nabla=\\nabla^{\\nabla f}\u2207 = \u2207 start_POSTSUPERSCRIPT \u2207 italic_f end_POSTSUPERSCRIPT and \u0394=\u0394\u2207f=\u0394d\u03bc\u2207f\u0394superscript\u0394\u2207\ud835\udc53subscriptsuperscript\u0394\u2207\ud835\udc53\ud835\udc51\ud835\udf07\\Delta=\\Delta^{\\nabla f}=\\Delta^{\\nabla f}_{d\\mu}roman_\u0394 = roman_\u0394 start_POSTSUPERSCRIPT \u2207 italic_f end_POSTSUPERSCRIPT = roman_\u0394 start_POSTSUPERSCRIPT \u2207 italic_f end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d italic_\u03bc end_POSTSUBSCRIPT for short, when f\ud835\udc53fitalic_f is exactly the function that \u2207\u2207\\nabla\u2207 or \u0394\u0394\\Deltaroman_\u0394 act on. One also observes for later frequent use that, given functions f\ud835\udc53fitalic_f, f1subscript\ud835\udc531f_{1}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and f2subscript\ud835\udc532f_{2}italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT on Mfsubscript\ud835\udc40\ud835\udc53M_{f}italic_M start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, Let H1(M):=W1,2(M)assignsuperscript\ud835\udc3b1\ud835\udc40superscript\ud835\udc4a12\ud835\udc40H^{1}(M):=W^{1,2}(M)italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M ) := italic_W start_POSTSUPERSCRIPT 1 , 2 end_POSTSUPERSCRIPT ( italic_M ) be the Sobolev space under norm where F\u2190(x,v):=F(x,\u2212v)assign\u2190\ud835\udc39\ud835\udc65\ud835\udc63\ud835\udc39\ud835\udc65\ud835\udc63\\overleftarrow{F}(x,v):=F(x,-v)over\u2190 start_ARG italic_F end_ARG ( italic_x , italic_v ) := italic_F ( italic_x , - italic_v ) is the reverse Finsler metric and \u2207\u2190\u2190\u2207\\overleftarrow{\\nabla}over\u2190 start_ARG \u2207 end_ARG is the gradient with respect to the reverse metric F\u2190\u2190\ud835\udc39\\overleftarrow{F}over\u2190 start_ARG italic_F end_ARG. Then H1(M)superscript\ud835\udc3b1\ud835\udc40H^{1}(M)italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M ) is in fact a Banach space with respect to \u2225\u22c5\u2225H1(M)\\|\\cdot\\|_{H^{1}(M)}\u2225 \u22c5 \u2225 start_POSTSUBSCRIPT italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M ) end_POSTSUBSCRIPT, and H01(M)superscriptsubscript\ud835\udc3b01\ud835\udc40H_{0}^{1}(M)italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M ) is also the closure of C0\u221e(M)subscriptsuperscript\ud835\udc360\ud835\udc40C^{\\infty}_{0}(M)italic_C start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_M ) under the norm defined in (2.25). Due to the lack of regularity, Finslerian Laplacian is usually viewed in distributional sense (or weak sense) that for any nonnegative function \u03c6\u2208H01(M)\u2229L\u221e(M)\ud835\udf11superscriptsubscript\ud835\udc3b01\ud835\udc40superscript\ud835\udc3f\ud835\udc40\\varphi\\in H_{0}^{1}(M)\\cap L^{\\infty}(M)italic_\u03c6 \u2208 italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M ) \u2229 italic_L start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT ( italic_M ). Employing S\ud835\udc46Sitalic_S-curvature, we present the definition of the weighted Ricci curvature introduced by S. Ohta [15] as follows. ([15]) Given a unit vector v\u2208TxM\ud835\udc63subscript\ud835\udc47\ud835\udc65\ud835\udc40v\\in T_{x}Mitalic_v \u2208 italic_T start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_M and an integer N\u2208[n,+\u221e]\ud835\udc41\ud835\udc5bN\\in[n,+\\infty]italic_N \u2208 [ italic_n , + \u221e ], the weighted Ricci curvature is defined by Ricn(x,v):={Ric(x,v)+S\u02d9(x,v) if S(x,v)=0;\u2212\u221e if S(x,v)\u22600.assignsuperscriptRic\ud835\udc5b\ud835\udc65\ud835\udc63casesRic\ud835\udc65\ud835\udc63\u02d9\ud835\udc46\ud835\udc65\ud835\udc63 if \ud835\udc46\ud835\udc65\ud835\udc630 if \ud835\udc46\ud835\udc65\ud835\udc630\\operatorname{Ric}^{n}(x,v):=\\begin{cases}\\operatorname{Ric}(x,v)+\\dot{S}(x,v)% &\\text{ if }{S}(x,v)=0;\\\\ -\\infty&\\text{ if }{S}(x,v)\\neq 0.\\end{cases}roman_Ric start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_x , italic_v ) := { start_ROW start_CELL roman_Ric ( italic_x , italic_v ) + over\u02d9 start_ARG italic_S end_ARG ( italic_x , italic_v ) end_CELL start_CELL if italic_S ( italic_x , italic_v ) = 0 ; end_CELL end_ROW start_ROW start_CELL - \u221e end_CELL start_CELL if italic_S ( italic_x , italic_v ) \u2260 0 . end_CELL end_ROW RicN(x,v):=Ric(x,v)+S\u02d9(x,v)\u2212S2(x,v)N\u2212nassignsuperscriptRic\ud835\udc41\ud835\udc65\ud835\udc63Ric\ud835\udc65\ud835\udc63\u02d9\ud835\udc46\ud835\udc65\ud835\udc63superscript\ud835\udc462\ud835\udc65\ud835\udc63\ud835\udc41\ud835\udc5b\\operatorname{Ric}^{N}(x,v):=\\operatorname{Ric}(x,v)+\\dot{S}(x,v)-\\frac{S^{2}(% x,v)}{N-n}roman_Ric start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ( italic_x , italic_v ) := roman_Ric ( italic_x , italic_v ) + over\u02d9 start_ARG italic_S end_ARG ( italic_x , italic_v ) - divide start_ARG italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_x , italic_v ) end_ARG start_ARG italic_N - italic_n end_ARG when n<N<\u221e\ud835\udc5b\ud835\udc41n<N<\\inftyitalic_n < italic_N < \u221e. Ric\u221e(x,v):=Ric(x,v)+S\u02d9(x,v)assignsuperscriptRic\ud835\udc65\ud835\udc63Ric\ud835\udc65\ud835\udc63\u02d9\ud835\udc46\ud835\udc65\ud835\udc63\\operatorname{Ric}^{\\infty}(x,v):=\\operatorname{Ric}(x,v)+\\dot{S}(x,v)roman_Ric start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT ( italic_x , italic_v ) := roman_Ric ( italic_x , italic_v ) + over\u02d9 start_ARG italic_S end_ARG ( italic_x , italic_v ). Here S\u02d9(x,v)\u02d9\ud835\udc46\ud835\udc65\ud835\udc63\\dot{S}(x,v)over\u02d9 start_ARG italic_S end_ARG ( italic_x , italic_v ) is the derivative along the geodesic from x\ud835\udc65xitalic_x in the direction of v\ud835\udc63vitalic_v. B. Wu [20] defined the weighted flag curvature when k\u2260n\ud835\udc58\ud835\udc5bk\\neq nitalic_k \u2260 italic_n and B. Shen [16] completed this concept for any k\ud835\udc58kitalic_k. ([20][16]) Given two linearly independent vectors v\ud835\udc63vitalic_v, w\u2208TxM\ud835\udc64subscript\ud835\udc47\ud835\udc65\ud835\udc40w\\in T_{x}Mitalic_w \u2208 italic_T start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_M and an integer k\u2208[n,+\u221e]\ud835\udc58\ud835\udc5bk\\in[n,+\\infty]italic_k \u2208 [ italic_n , + \u221e ], the weighted Ricci curvature is defined by Kn(v,w):={K(v,w)+S\u02d9(x,v)(n\u22121)F2(v) if S(x,v)=0;\u2212\u221e if S(x,v)\u22600.assignsuperscriptK\ud835\udc5b\ud835\udc63\ud835\udc64casesK\ud835\udc63\ud835\udc64\u02d9\ud835\udc46\ud835\udc65\ud835\udc63\ud835\udc5b1superscript\ud835\udc392\ud835\udc63 if \ud835\udc46\ud835\udc65\ud835\udc630 if \ud835\udc46\ud835\udc65\ud835\udc630\\operatorname{K}^{n}(v,w):=\\begin{cases}\\operatorname{K}(v,w)+\\frac{\\dot{S}(x,% v)}{(n-1)F^{2}(v)}&\\text{ if }{S}(x,v)=0;\\\\ -\\infty&\\text{ if }{S}(x,v)\\neq 0.\\end{cases}roman_K start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_v , italic_w ) := { start_ROW start_CELL roman_K ( italic_v , italic_w ) + divide start_ARG over\u02d9 start_ARG italic_S end_ARG ( italic_x , italic_v ) end_ARG start_ARG ( italic_n - 1 ) italic_F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_v ) end_ARG end_CELL start_CELL if italic_S ( italic_x , italic_v ) = 0 ; end_CELL end_ROW start_ROW start_CELL - \u221e end_CELL start_CELL if italic_S ( italic_x , italic_v ) \u2260 0 . end_CELL end_ROW Kk(v,w):=K(v,w)+S\u02d9(x,v)(n\u22121)F2(v)\u2212S2(x,v)(n\u22121)(k\u2212n)F2(v)assignsuperscriptK\ud835\udc58\ud835\udc63\ud835\udc64K\ud835\udc63\ud835\udc64\u02d9\ud835\udc46\ud835\udc65\ud835\udc63\ud835\udc5b1superscript\ud835\udc392\ud835\udc63superscript\ud835\udc462\ud835\udc65\ud835\udc63\ud835\udc5b1\ud835\udc58\ud835\udc5bsuperscript\ud835\udc392\ud835\udc63\\operatorname{K}^{k}(v,w):=\\operatorname{K}(v,w)+\\frac{\\dot{S}(x,v)}{(n-1)F^{2% }(v)}-\\frac{S^{2}(x,v)}{(n-1)(k-n)F^{2}(v)}roman_K start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_v , italic_w ) := roman_K ( italic_v , italic_w ) + divide start_ARG over\u02d9 start_ARG italic_S end_ARG ( italic_x , italic_v ) end_ARG start_ARG ( italic_n - 1 ) italic_F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_v ) end_ARG - divide start_ARG italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_x , italic_v ) end_ARG start_ARG ( italic_n - 1 ) ( italic_k - italic_n ) italic_F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_v ) end_ARG when n<k<\u221e\ud835\udc5b\ud835\udc58n<k<\\inftyitalic_n < italic_k < \u221e. K\u221e(x,v):=K(v,w)+S\u02d9(x,v)(n\u22121)F2(v)assignsuperscriptK\ud835\udc65\ud835\udc63K\ud835\udc63\ud835\udc64\u02d9\ud835\udc46\ud835\udc65\ud835\udc63\ud835\udc5b1superscript\ud835\udc392\ud835\udc63\\operatorname{K}^{\\infty}(x,v):=\\operatorname{K}(v,w)+\\frac{\\dot{S}(x,v)}{(n-1% )F^{2}(v)}roman_K start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT ( italic_x , italic_v ) := roman_K ( italic_v , italic_w ) + divide start_ARG over\u02d9 start_ARG italic_S end_ARG ( italic_x , italic_v ) end_ARG start_ARG ( italic_n - 1 ) italic_F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_v ) end_ARG. Moreover, B. Shen [16] have also defined the mixed weighted Ricci curvature. ([16]) Given two linearly independent vectors v\ud835\udc63vitalic_v, w\u2208TxM\ud835\udc64subscript\ud835\udc47\ud835\udc65\ud835\udc40w\\in T_{x}Mitalic_w \u2208 italic_T start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_M and an integer k\u2208[n,+\u221e]\ud835\udc58\ud835\udc5bk\\in[n,+\\infty]italic_k \u2208 [ italic_n , + \u221e ], the mixed weighted Ricci curvature is defined by Ricwnm(x,v):={trwRv(v)+S\u02d9(x,v) if S(x,v)=0;\u2212\u221e if S(x,v)\u22600.assignsuperscriptsubscriptsuperscriptRic\ud835\udc5b\ud835\udc64\ud835\udc5a\ud835\udc65\ud835\udc63casessubscripttr\ud835\udc64subscript\ud835\udc45\ud835\udc63\ud835\udc63\u02d9\ud835\udc46\ud835\udc65\ud835\udc63 if \ud835\udc46\ud835\udc65\ud835\udc630 if \ud835\udc46\ud835\udc65\ud835\udc630{}^{m}\\operatorname{Ric}^{n}_{w}(x,v):=\\begin{cases}\\operatorname{tr}_{w}R_{v}% (v)+\\dot{S}(x,v)&\\text{ if }{S}(x,v)=0;\\\\ -\\infty&\\text{ if }{S}(x,v)\\neq 0.\\end{cases}start_FLOATSUPERSCRIPT italic_m end_FLOATSUPERSCRIPT roman_Ric start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ( italic_x , italic_v ) := { start_ROW start_CELL roman_tr start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_v ) + over\u02d9 start_ARG italic_S end_ARG ( italic_x , italic_v ) end_CELL start_CELL if italic_S ( italic_x , italic_v ) = 0 ; end_CELL end_ROW start_ROW start_CELL - \u221e end_CELL start_CELL if italic_S ( italic_x , italic_v ) \u2260 0 . end_CELL end_ROW Ricwkm(x,v):=trwRv(v)+S\u02d9(x,v)\u2212S2(x,v)k\u2212nassignsuperscriptsubscriptsuperscriptRic\ud835\udc58\ud835\udc64\ud835\udc5a\ud835\udc65\ud835\udc63subscripttr\ud835\udc64subscript\ud835\udc45\ud835\udc63\ud835\udc63\u02d9\ud835\udc46\ud835\udc65\ud835\udc63superscript\ud835\udc462\ud835\udc65\ud835\udc63\ud835\udc58\ud835\udc5b{}^{m}\\operatorname{Ric}^{k}_{w}(x,v):=\\operatorname{tr}_{w}R_{v}(v)+\\dot{S}(x% ,v)-\\frac{S^{2}(x,v)}{k-n}start_FLOATSUPERSCRIPT italic_m end_FLOATSUPERSCRIPT roman_Ric start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ( italic_x , italic_v ) := roman_tr start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_v ) + over\u02d9 start_ARG italic_S end_ARG ( italic_x , italic_v ) - divide start_ARG italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_x , italic_v ) end_ARG start_ARG italic_k - italic_n end_ARG when n<N<\u221e\ud835\udc5b\ud835\udc41n<N<\\inftyitalic_n < italic_N < \u221e. Ricw\u221em(x,v):=trwRv(v)+S\u02d9(x,v)assignsuperscriptsubscriptsuperscriptRic\ud835\udc64\ud835\udc5a\ud835\udc65\ud835\udc63subscripttr\ud835\udc64subscript\ud835\udc45\ud835\udc63\ud835\udc63\u02d9\ud835\udc46\ud835\udc65\ud835\udc63{}^{m}\\operatorname{Ric}^{\\infty}_{w}(x,v):=\\operatorname{tr}_{w}R_{v}(v)+\\dot% {S}(x,v)start_FLOATSUPERSCRIPT italic_m end_FLOATSUPERSCRIPT roman_Ric start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ( italic_x , italic_v ) := roman_tr start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_v ) + over\u02d9 start_ARG italic_S end_ARG ( italic_x , italic_v ). where trwRv(v):=gij(w)gv(Rv(ei,v),ej)assignsubscripttr\ud835\udc64subscript\ud835\udc45\ud835\udc63\ud835\udc63superscript\ud835\udc54\ud835\udc56\ud835\udc57\ud835\udc64subscript\ud835\udc54\ud835\udc63subscript\ud835\udc45\ud835\udc63subscript\ud835\udc52\ud835\udc56\ud835\udc63subscript\ud835\udc52\ud835\udc57\\operatorname{tr}_{w}R_{v}(v):=g^{ij}(w)g_{v}(R_{v}(e_{i},v),e_{j})roman_tr start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_v ) := italic_g start_POSTSUPERSCRIPT italic_i italic_j end_POSTSUPERSCRIPT ( italic_w ) italic_g start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_R start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v ) , italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) is the trace of flag curvature with respect to gwsubscript\ud835\udc54\ud835\udc64g_{w}italic_g start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT. Note that the weighted Ricci curvature is a special case of the mixed weighted Ricci curvature since Ricvkm(x,v)=Rick(x,v)superscriptsubscriptsuperscriptRic\ud835\udc58\ud835\udc63\ud835\udc5a\ud835\udc65\ud835\udc63superscriptRic\ud835\udc58\ud835\udc65\ud835\udc63{}^{m}\\operatorname{Ric}^{k}_{v}(x,v)=\\operatorname{Ric}^{k}(x,v)start_FLOATSUPERSCRIPT italic_m end_FLOATSUPERSCRIPT roman_Ric start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_x , italic_v ) = roman_Ric start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_x , italic_v ). Next we introduce two important tools: Let (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ) be an n\ud835\udc5bnitalic_n-dimensional Finsler metric measure space. Given u\u2208Hloc2(M)\u2229C1(M)\ud835\udc62superscriptsubscript\ud835\udc3bnormal-loc2\ud835\udc40superscript\ud835\udc361\ud835\udc40u\\in H_{\\mathrm{loc}}^{2}(M)\\cap{C}^{1}(M)italic_u \u2208 italic_H start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_M ) \u2229 italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M ) with \u0394u\u2208Hloc1(M)normal-\u0394\ud835\udc62superscriptsubscript\ud835\udc3bnormal-loc1\ud835\udc40\\Delta u\\in H_{\\mathrm{loc}}^{1}(M)roman_\u0394 italic_u \u2208 italic_H start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M ), for N\u2208[n,+\u221e]\ud835\udc41\ud835\udc5bN\\in[n,+\\infty]italic_N \u2208 [ italic_n , + \u221e ], we have in sense of distribution, that is, for all nonnegative functions \u03c6\u2208H01(M)\u2229L\u221e(M)\ud835\udf11superscriptsubscript\ud835\udc3b01\ud835\udc40superscript\ud835\udc3f\ud835\udc40\\varphi\\in H_{0}^{1}(M)\\cap L^{\\infty}(M)italic_\u03c6 \u2208 italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M ) \u2229 italic_L start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT ( italic_M ). Let (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ) be a forward complete n\ud835\udc5bnitalic_n-dimensional Finsler metric measure space. Denote the forward distance function by r\ud835\udc5fritalic_r and by V\ud835\udc49Vitalic_V a fixed vector field on M\ud835\udc40Mitalic_M. Suppose the misalignment of M\ud835\udc40Mitalic_M is finite with upper bound A\ud835\udc34Aitalic_A, and the mixed weighted Ricci curvature Ric\u2207rNm(V)superscriptsubscriptsuperscriptnormal-Ric\ud835\udc41normal-\u2207\ud835\udc5f\ud835\udc5a\ud835\udc49{}^{m}\\operatorname{Ric}^{N}_{\\nabla r}(V)start_FLOATSUPERSCRIPT italic_m end_FLOATSUPERSCRIPT roman_Ric start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT start_POSTSUBSCRIPT \u2207 italic_r end_POSTSUBSCRIPT ( italic_V ) of M\ud835\udc40Mitalic_M is bounded from below by \u2212K\ud835\udc3e-K- italic_K with K>0\ud835\udc3e0K>0italic_K > 0, for some N>n\ud835\udc41\ud835\udc5bN>nitalic_N > italic_n. Suppose the non-Riemannian curvatures U\ud835\udc48Uitalic_U, \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T (defined in (2.16) and (2.20) respectively) and divC(V):=Ck|iij(V)Vk\u03b4\u03b4xjassignnormal-div\ud835\udc36\ud835\udc49subscriptsuperscript\ud835\udc36\ud835\udc56\ud835\udc57conditional\ud835\udc58\ud835\udc56\ud835\udc49superscript\ud835\udc49\ud835\udc58\ud835\udeff\ud835\udeffsuperscript\ud835\udc65\ud835\udc57\\operatorname{div}C(V):=C^{ij}_{~{}~{}k|i}(V)V^{k}\\frac{\\delta}{\\delta x^{j}}roman_div italic_C ( italic_V ) := italic_C start_POSTSUPERSCRIPT italic_i italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k | italic_i end_POSTSUBSCRIPT ( italic_V ) italic_V start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT divide start_ARG italic_\u03b4 end_ARG start_ARG italic_\u03b4 italic_x start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_ARG satisfy the norm bounds by Then, by setting l=K/C(N,A)\ud835\udc59\ud835\udc3e\ud835\udc36\ud835\udc41\ud835\udc34l=K/C(N,A)italic_l = italic_K / italic_C ( italic_N , italic_A ) with C(N,A)=\ud835\udc36\ud835\udc41\ud835\udc34absentC(N,A)=italic_C ( italic_N , italic_A ) = N+(A\u22121)n\u2212A\ud835\udc41\ud835\udc341\ud835\udc5b\ud835\udc34N+(A-1)n-Aitalic_N + ( italic_A - 1 ) italic_n - italic_A, wherever r\ud835\udc5fritalic_r is C2superscript\ud835\udc362C^{2}italic_C start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, the nonlinear Laplacian of r\ud835\udc5fritalic_r with reference vector V\ud835\udc49Vitalic_V satisfies Let (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ) be a Finsler measure space. For any function f\u2208H1(M)\ud835\udc53superscript\ud835\udc3b1\ud835\udc40f\\in H^{1}(M)italic_f \u2208 italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M ) with \u222bMf2\ud835\udc51\u03bc=vold\u03bc(M)subscript\ud835\udc40superscript\ud835\udc532differential-d\ud835\udf07subscriptvol\ud835\udc51\ud835\udf07\ud835\udc40\\int_{M}f^{2}d\\mu=\\operatorname{vol}_{d\\mu}(M)\u222b start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT italic_f start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d italic_\u03bc = roman_vol start_POSTSUBSCRIPT italic_d italic_\u03bc end_POSTSUBSCRIPT ( italic_M ), define the Finslerian log-energy functional by and Finslerian log-Sobolev constant CFLSsubscript\ud835\udc36\ud835\udc39\ud835\udc3f\ud835\udc46C_{FLS}italic_C start_POSTSUBSCRIPT italic_F italic_L italic_S end_POSTSUBSCRIPT is the infimum of E\ud835\udc38Eitalic_E, we then have the following theorem: If u\ud835\udc62uitalic_u achieves log-Sobolev constant CFLSsubscript\ud835\udc36\ud835\udc39\ud835\udc3f\ud835\udc46C_{FLS}italic_C start_POSTSUBSCRIPT italic_F italic_L italic_S end_POSTSUBSCRIPT in the constraint of \u222bMu2\ud835\udc51\u03bc=vold\u03bc(M)subscript\ud835\udc40superscript\ud835\udc622differential-d\ud835\udf07subscriptnormal-vol\ud835\udc51\ud835\udf07\ud835\udc40\\int_{M}u^{2}d\\mu=\\operatorname{vol}_{d\\mu}(M)\u222b start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT italic_u start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d italic_\u03bc = roman_vol start_POSTSUBSCRIPT italic_d italic_\u03bc end_POSTSUBSCRIPT ( italic_M ) then the Euler-Lagrange equation is given by Choose any local function v\ud835\udc63vitalic_v with compact support M\ud835\udc40Mitalic_M and t\u2208\u211d\ud835\udc61\u211dt\\in\\mathbb{R}italic_t \u2208 blackboard_R. By a direction variational calculation, we have Since the definition of CFLSsubscript\ud835\udc36\ud835\udc39\ud835\udc3f\ud835\udc46C_{FLS}italic_C start_POSTSUBSCRIPT italic_F italic_L italic_S end_POSTSUBSCRIPT, it follows that The conclusion holds for the arbitrariness of v\ud835\udc63vitalic_v. \u220e Analog to [14], one can define the global and local solutions to equation (1.4): For T>0\ud835\udc470T>0italic_T > 0, we say that a function u\ud835\udc62uitalic_u on M\u00d7[0,T]\ud835\udc400\ud835\udc47M\\times[0,T]italic_M \u00d7 [ 0 , italic_T ] is a global solution to the equation (1.4) if u\u2208L2([0,T],H01(M))\u2229H1([0,T],H\u22121(M))\ud835\udc62superscript\ud835\udc3f20\ud835\udc47superscriptsubscript\ud835\udc3b01\ud835\udc40superscript\ud835\udc3b10\ud835\udc47superscript\ud835\udc3b1\ud835\udc40u\\in L^{2}\\left([0,T],H_{0}^{1}(M)\\right)\\cap H^{1}\\left([0,T],H^{-1}(M)\\right)italic_u \u2208 italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( [ 0 , italic_T ] , italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M ) ) \u2229 italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( [ 0 , italic_T ] , italic_H start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_M ) ) and holds for all t\u2208[0,T]\ud835\udc610\ud835\udc47t\\in[0,T]italic_t \u2208 [ 0 , italic_T ] and \u03c6\u2208C0\u221e(M)\ud835\udf11superscriptsubscript\ud835\udc360\ud835\udc40\\varphi\\in C_{0}^{\\infty}(M)italic_\u03c6 \u2208 italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT ( italic_M ) (or \u03c6\u2208H01(M)\ud835\udf11superscriptsubscript\ud835\udc3b01\ud835\udc40\\varphi\\in H_{0}^{1}(M)italic_\u03c6 \u2208 italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M )). Given an open interval I\u2282\u211d\ud835\udc3c\u211dI\\subset\\mathbb{R}italic_I \u2282 blackboard_R and an open set \u03a9\u2282M\u03a9\ud835\udc40\\Omega\\subset Mroman_\u03a9 \u2282 italic_M, we say that a function u\ud835\udc62uitalic_u is a local solution to the equation (1.4) on \u03a9\u00d7I\u03a9\ud835\udc3c\\Omega\\times Iroman_\u03a9 \u00d7 italic_I if u\u2208Lloc2(\u03a9),F(\u2207u)\u2208Lloc2(\u03a9)formulae-sequence\ud835\udc62superscriptsubscript\ud835\udc3floc2\u03a9\ud835\udc39\u2207\ud835\udc62superscriptsubscript\ud835\udc3floc2\u03a9u\\in L_{\\mathrm{loc}}^{2}(\\Omega),F(\\nabla u)\\in L_{\\mathrm{loc}}^{2}(\\Omega)italic_u \u2208 italic_L start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_\u03a9 ) , italic_F ( \u2207 italic_u ) \u2208 italic_L start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_\u03a9 ) and holds for all \u03c6\u2208C0\u221e(\u03a9\u00d7I)\ud835\udf11superscriptsubscript\ud835\udc360\u03a9\ud835\udc3c\\varphi\\in C_{0}^{\\infty}(\\Omega\\times I)italic_\u03c6 \u2208 italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT ( roman_\u03a9 \u00d7 italic_I )(or \u03c6\u2208H01(\u03a9\u00d7I)\ud835\udf11superscriptsubscript\ud835\udc3b01\u03a9\ud835\udc3c\\varphi\\in H_{0}^{1}(\\Omega\\times I)italic_\u03c6 \u2208 italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( roman_\u03a9 \u00d7 italic_I )). The elliptic and parabolic regularities in [14] and [16] guarantee that the global solution u(x,t)\ud835\udc62\ud835\udc65\ud835\udc61u(x,t)italic_u ( italic_x , italic_t ) of (1.4) enjoys the Hloc2subscriptsuperscript\ud835\udc3b2locH^{2}_{\\text{loc}}italic_H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT loc end_POSTSUBSCRIPT-regularity in x\ud835\udc65xitalic_x and C1,\u03b2superscript\ud835\udc361\ud835\udefdC^{1,\\beta}italic_C start_POSTSUPERSCRIPT 1 , italic_\u03b2 end_POSTSUPERSCRIPT-regularity (0<\u03b2<10\ud835\udefd10<\\beta<10 < italic_\u03b2 < 1) in term of both t\ud835\udc61titalic_t and x\ud835\udc65xitalic_x, meanwhile \u2202tu\u2208Hloc1(M)\u2229C(M)subscript\ud835\udc61\ud835\udc62subscriptsuperscript\ud835\udc3b1loc\ud835\udc40\ud835\udc36\ud835\udc40\\partial_{t}u\\in H^{1}_{\\text{loc}}(M)\\cap C(M)\u2202 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_u \u2208 italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT loc end_POSTSUBSCRIPT ( italic_M ) \u2229 italic_C ( italic_M ). Moreover, for the local solution u(x,t)\ud835\udc62\ud835\udc65\ud835\udc61u(x,t)italic_u ( italic_x , italic_t ) on \u03a9\u00d7I\u03a9\ud835\udc3c\\Omega\\times Iroman_\u03a9 \u00d7 italic_I then u\u2208H2(\u03a9)\u2229C1,\u03b2(\u03a9\u00d7I)\ud835\udc62superscript\ud835\udc3b2\u03a9superscript\ud835\udc361\ud835\udefd\u03a9\ud835\udc3cu\\in H^{2}(\\Omega)\\cap C^{1,\\beta}(\\Omega\\times I)italic_u \u2208 italic_H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_\u03a9 ) \u2229 italic_C start_POSTSUPERSCRIPT 1 , italic_\u03b2 end_POSTSUPERSCRIPT ( roman_\u03a9 \u00d7 italic_I ), \u0394u\u2208H1(M)\u2229C(\u03a9)\u0394\ud835\udc62superscript\ud835\udc3b1\ud835\udc40\ud835\udc36\u03a9\\Delta u\\in H^{1}(M)\\cap C(\\Omega)roman_\u0394 italic_u \u2208 italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M ) \u2229 italic_C ( roman_\u03a9 ) and \u2202tu\u2208Hloc1(\u03a9)\u2229C(\u03a9)subscript\ud835\udc61\ud835\udc62subscriptsuperscript\ud835\udc3b1loc\u03a9\ud835\udc36\u03a9\\partial_{t}u\\in H^{1}_{\\text{loc}}(\\Omega)\\cap C(\\Omega)\u2202 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_u \u2208 italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT loc end_POSTSUBSCRIPT ( roman_\u03a9 ) \u2229 italic_C ( roman_\u03a9 ). In this section we will always assume that (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ) is a compact n\ud835\udc5bnitalic_n-dimensional Finsler metric measure space, whose weighted Ricci curvature RicNsuperscriptRic\ud835\udc41\\operatorname{Ric}^{N}roman_Ric start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT has lower bound of \u2212K\ud835\udc3e-K- italic_K (K\u2a7e0\ud835\udc3e0K\\geqslant 0italic_K \u2a7e 0) for some N\u2208(n,\u221e)\ud835\udc41\ud835\udc5bN\\in(n,\\infty)italic_N \u2208 ( italic_n , \u221e ). Let u\ud835\udc62uitalic_u be a positive solution to (1.4) on M\u00d7[0,+\u221e)\ud835\udc400M\\times[0,+\\infty)italic_M \u00d7 [ 0 , + \u221e ). For convenience, if one replaces u\ud835\udc62uitalic_u by e\u2212b/ausuperscript\ud835\udc52\ud835\udc4f\ud835\udc4e\ud835\udc62e^{-b/a}uitalic_e start_POSTSUPERSCRIPT - italic_b / italic_a end_POSTSUPERSCRIPT italic_u in (1.4), it suffices to consider the following equation: i.e. for any test function \u03c6\u2208H01(M\u00d7[0,T])\ud835\udf11superscriptsubscript\ud835\udc3b01\ud835\udc400\ud835\udc47\\varphi\\in H_{0}^{1}(M\\times[0,T])italic_\u03c6 \u2208 italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_M \u00d7 [ 0 , italic_T ] ) Before we begin the proof of Theorem 1.1, we shall introduce some lemmas: For any fixed T>0\ud835\udc470T>0italic_T > 0, Let u\ud835\udc62uitalic_u be a positive solution to (3.1) on M\u00d7[0,T]\ud835\udc400\ud835\udc47M\\times[0,T]italic_M \u00d7 [ 0 , italic_T ] and f:=loguassign\ud835\udc53\ud835\udc62f:=\\log uitalic_f := roman_log italic_u, then in the distributional sense that Since \u2207u=u\u2207f\u2207\ud835\udc62\ud835\udc62\u2207\ud835\udc53\\nabla u=u\\nabla f\u2207 italic_u = italic_u \u2207 italic_f and u>0\ud835\udc620u>0italic_u > 0 we have g\u2207u=g\u2207fsubscript\ud835\udc54\u2207\ud835\udc62subscript\ud835\udc54\u2207\ud835\udc53g_{\\nabla u}=g_{\\nabla f}italic_g start_POSTSUBSCRIPT \u2207 italic_u end_POSTSUBSCRIPT = italic_g start_POSTSUBSCRIPT \u2207 italic_f end_POSTSUBSCRIPT and hence \u0394\u2207uf=\u0394\u2207ff=\u0394fsuperscript\u0394\u2207\ud835\udc62\ud835\udc53superscript\u0394\u2207\ud835\udc53\ud835\udc53\u0394\ud835\udc53\\Delta^{\\nabla u}f=\\Delta^{\\nabla f}f=\\Delta froman_\u0394 start_POSTSUPERSCRIPT \u2207 italic_u end_POSTSUPERSCRIPT italic_f = roman_\u0394 start_POSTSUPERSCRIPT \u2207 italic_f end_POSTSUPERSCRIPT italic_f = roman_\u0394 italic_f on Mu=Mfsubscript\ud835\udc40\ud835\udc62subscript\ud835\udc40\ud835\udc53M_{u}=M_{f}italic_M start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_M start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. Then in weak sense on Musubscript\ud835\udc40\ud835\udc62M_{u}italic_M start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT. Hence, Since \u0394u=0\u0394\ud835\udc620\\Delta u=0roman_\u0394 italic_u = 0 a.e. on M\\Mu\\\ud835\udc40subscript\ud835\udc40\ud835\udc62M\\backslash M_{u}italic_M \\ italic_M start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, it completes the lemma. \u220e Let (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ) be a Finsler metric measure space with weighted Ricci curvature RicN\u2a7e\u2212Ksuperscriptnormal-Ric\ud835\udc41\ud835\udc3e\\operatorname{Ric}^{N}\\geqslant-Kroman_Ric start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2a7e - italic_K, where the constant K\u2a7e0\ud835\udc3e0K\\geqslant 0italic_K \u2a7e 0. Let f\ud835\udc53fitalic_f satisfy (3.4) and ft=\u2202tfsubscript\ud835\udc53\ud835\udc61subscript\ud835\udc61\ud835\udc53f_{t}=\\partial_{t}fitalic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = \u2202 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_f. Then for any constant \u03b2>1\ud835\udefd1\\beta>1italic_\u03b2 > 1, satisties that for any positive test function \u03c6\u2208C02([0,T])\u2229C02(M)\ud835\udf11superscriptsubscript\ud835\udc36020\ud835\udc47subscriptsuperscript\ud835\udc3620\ud835\udc40\\varphi\\in C_{0}^{2}([0,T])\\cap C^{2}_{0}(M)italic_\u03c6 \u2208 italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( [ 0 , italic_T ] ) \u2229 italic_C start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_M ), where By the same method in Lemma 3.1, it is valid to give the proof in smooth case on Musubscript\ud835\udc40\ud835\udc62M_{u}italic_M start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, instead of the weak sense. From (3.3) and the definition of H\ud835\udc3bHitalic_H, one obtains that Note that [\u0394\u2207u,\u2202t]u=0superscript\u0394\u2207\ud835\udc62subscript\ud835\udc61\ud835\udc620[\\Delta^{\\nabla u},\\partial_{t}]u=0[ roman_\u0394 start_POSTSUPERSCRIPT \u2207 italic_u end_POSTSUPERSCRIPT , \u2202 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] italic_u = 0 holds pointwise on Musubscript\ud835\udc40\ud835\udc62M_{u}italic_M start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT (see [21]), and derivate (3.3) and (3.5) with respect to t\ud835\udc61titalic_t. It then follows that and Now in order to estimate \u0394\u2207uHsuperscript\u0394\u2207\ud835\udc62\ud835\udc3b\\Delta^{\\nabla u}Hroman_\u0394 start_POSTSUPERSCRIPT \u2207 italic_u end_POSTSUPERSCRIPT italic_H (actually in weak sense) , we need to utilize the Bochner\u2013Weitzenb\u00f6ck formula in Theorem 2.1. Using the assumption that RicN(\u2207f)\u2a7e\u2212KF2(\u2207f)superscriptRic\ud835\udc41\u2207\ud835\udc53\ud835\udc3esuperscript\ud835\udc392\u2207\ud835\udc53\\operatorname{Ric}^{N}(\\nabla f)\\geqslant-KF^{2}(\\nabla f)roman_Ric start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ( \u2207 italic_f ) \u2a7e - italic_K italic_F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( \u2207 italic_f ) and combining (3.9) and (3.10), we have Combining (3.11) and (3.13) it implies that \u220e Then we start to prove Theorem 1.1. Let (z,s)\ud835\udc67\ud835\udc60(z,s)( italic_z , italic_s ) be the maximal point of H\ud835\udc3bHitalic_H on M\u00d7[0,T]\ud835\udc400\ud835\udc47M\\times[0,T]italic_M \u00d7 [ 0 , italic_T ] and \u039b:=H(z,s)assign\u039b\ud835\udc3b\ud835\udc67\ud835\udc60\\Lambda:=H(z,s)roman_\u039b := italic_H ( italic_z , italic_s ) be the maximum of H\ud835\udc3bHitalic_H. Without loss of generality, we can assume \u039b>0\u039b0\\Lambda>0roman_\u039b > 0 and s>0\ud835\udc600s>0italic_s > 0, otherwise, the assertion of the theorem obviously holds. Now we shall follow the proof in [15] by S. Ohta to claim that \ud835\udca5(z,s)\u2a7d0\ud835\udca5\ud835\udc67\ud835\udc600\\mathcal{J}(z,s)\\leqslant 0caligraphic_J ( italic_z , italic_s ) \u2a7d 0. By the assumption of contrary, \ud835\udca5(z,s)>0\ud835\udca5\ud835\udc67\ud835\udc600\\mathcal{J}(z,s)>0caligraphic_J ( italic_z , italic_s ) > 0, it follows that \ud835\udca5>0\ud835\udca50\\mathcal{J}>0caligraphic_J > 0 on a neighborhood of (z,s)\ud835\udc67\ud835\udc60(z,s)( italic_z , italic_s ), say, a small parabolic cylinder Bz(\u03b4)\u00d7[s\u2212\u03b4,s]subscript\ud835\udc35\ud835\udc67\ud835\udeff\ud835\udc60\ud835\udeff\ud835\udc60B_{z}(\\delta)\\times[s-\\delta,s]italic_B start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT ( italic_\u03b4 ) \u00d7 [ italic_s - italic_\u03b4 , italic_s ], where Bz(\u03b4)subscript\ud835\udc35\ud835\udc67\ud835\udeffB_{z}(\\delta)italic_B start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT ( italic_\u03b4 ) is the forward geodesic ball with radius \u03b4\ud835\udeff\\deltaitalic_\u03b4. Thus, H\ud835\udc3bHitalic_H must be a strict subsolution to the linear parabolic operator This implies that H(z,s)\ud835\udc3b\ud835\udc67\ud835\udc60H(z,s)italic_H ( italic_z , italic_s ) would be strictly smaller than the supremum of H\ud835\udc3bHitalic_H on the boundary of the parabolic cylinder Bz(\u03b4)\u00d7[s\u2212\u03b4,s]subscript\ud835\udc35\ud835\udc67\ud835\udeff\ud835\udc60\ud835\udeff\ud835\udc60B_{z}(\\delta)\\times[s-\\delta,s]italic_B start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT ( italic_\u03b4 ) \u00d7 [ italic_s - italic_\u03b4 , italic_s ], which yields contradiction to the maximality of H(z,s)\ud835\udc3b\ud835\udc67\ud835\udc60H(z,s)italic_H ( italic_z , italic_s ). Therefore, at point (z,s)\ud835\udc67\ud835\udc60(z,s)( italic_z , italic_s ), from (3.7) and (3.8), we obtain that Denoting v:=F2(\u2207f)/Hassign\ud835\udc63superscript\ud835\udc392\u2207\ud835\udc53\ud835\udc3bv:=F^{2}(\\nabla f)/{H}italic_v := italic_F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( \u2207 italic_f ) / italic_H and multiplying both sides of (3.15) by s\u039b\ud835\udc60\u039bs\\Lambdaitalic_s roman_\u039b, we have Case 1: Firstly we consider the case of a<0\ud835\udc4e0a<0italic_a < 0 which means as+1\u2a7d1\ud835\udc4e\ud835\udc6011as+1\\leqslant 1italic_a italic_s + 1 \u2a7d 1, then (3.16) becomes Notice that, for the constants r,p\u2a7e0\ud835\udc5f\ud835\udc5d0r,p\\geqslant 0italic_r , italic_p \u2a7e 0 and q>0\ud835\udc5e0q>0italic_q > 0, the inequality holds for any x\u2a7e0\ud835\udc650x\\geqslant 0italic_x \u2a7e 0. Hence, Case 2: On the other hand, when a>0\ud835\udc4e0a>0italic_a > 0, then (\u03b2\u22121)avs2\u2a7e0\ud835\udefd1\ud835\udc4e\ud835\udc63superscript\ud835\udc6020(\\beta-1)avs^{2}\\geqslant 0( italic_\u03b2 - 1 ) italic_a italic_v italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT \u2a7e 0, and similarly (3.16) yields Finally, set \u03b1:=max{a/2,\u2212a/4}assign\ud835\udefc\ud835\udc4e2\ud835\udc4e4\\alpha:=\\max\\{a/2,-a/4\\}italic_\u03b1 := roman_max { italic_a / 2 , - italic_a / 4 } and combine (3.17) with (3.18). Since This completes the proof for any T>0\ud835\udc470T>0italic_T > 0. \u220e In this section, We will continue to discuss the case of complete but non-compact Finsler manifolds. Let us review the conditions. In forward complete n\ud835\udc5bnitalic_n-dimensional Finsler metric measure space (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ), let Bp(2R)subscript\ud835\udc35\ud835\udc5d2\ud835\udc45B_{p}(2R)italic_B start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( 2 italic_R ) be the forward geodesic ball centering p\u2208M\ud835\udc5d\ud835\udc40p\\in Mitalic_p \u2208 italic_M with radius 2R2\ud835\udc452R2 italic_R, and r(x):=d(p,x)assign\ud835\udc5f\ud835\udc65\ud835\udc51\ud835\udc5d\ud835\udc65r(x):=d(p,x)italic_r ( italic_x ) := italic_d ( italic_p , italic_x ) be the distance function from p\ud835\udc5dpitalic_p. Suppose for some N\u2208(n,\u221e)\ud835\udc41\ud835\udc5bN\\in(n,\\infty)italic_N \u2208 ( italic_n , \u221e ), the mixed weighted Ricci curvature Ric\u2207rNm\u2a7e\u2212K(2R)superscriptsuperscriptsubscriptRic\u2207\ud835\udc5f\ud835\udc41\ud835\udc5a\ud835\udc3e2\ud835\udc45{}^{m}\\operatorname{Ric}_{\\nabla r}^{N}\\geqslant-K(2R)start_FLOATSUPERSCRIPT italic_m end_FLOATSUPERSCRIPT roman_Ric start_POSTSUBSCRIPT \u2207 italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2a7e - italic_K ( 2 italic_R ), where K(2R)\u2a7e0\ud835\udc3e2\ud835\udc450K(2R)\\geqslant 0italic_K ( 2 italic_R ) \u2a7e 0. Additionally, we need to suppose that M\ud835\udc40Mitalic_M has finite upper bounded misalignment and non-Riemannian tensors by A(2R)\ud835\udc342\ud835\udc45A(2R)italic_A ( 2 italic_R ) and K0subscript\ud835\udc3e0K_{0}italic_K start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT respectively. It is sufficient to consider u(x,t)\ud835\udc62\ud835\udc65\ud835\udc61u(x,t)italic_u ( italic_x , italic_t ) to be a positive solution to the equation (3.1) (i.e. b=0\ud835\udc4f0b=0italic_b = 0) on Bp(2R)\u00d7[0,T]subscript\ud835\udc35\ud835\udc5d2\ud835\udc450\ud835\udc47B_{p}(2R)\\times[0,T]italic_B start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( 2 italic_R ) \u00d7 [ 0 , italic_T ] where a\u22600\ud835\udc4e0a\\neq 0italic_a \u2260 0. Same as the method in section 3, we set f(x,t):=logu(x,t)assign\ud835\udc53\ud835\udc65\ud835\udc61\ud835\udc62\ud835\udc65\ud835\udc61f(x,t):=\\log u(x,t)italic_f ( italic_x , italic_t ) := roman_log italic_u ( italic_x , italic_t ) and H:= t{F2(\u2207f)+\u03b2(af\u2212ft)}\ud835\udc61superscript\ud835\udc392\u2207\ud835\udc53\ud835\udefd\ud835\udc4e\ud835\udc53subscript\ud835\udc53\ud835\udc61t\\left\\{F^{2}(\\nabla f)+\\beta(af-f_{t})\\right\\}italic_t { italic_F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( \u2207 italic_f ) + italic_\u03b2 ( italic_a italic_f - italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) } for any \u03b2>1\ud835\udefd1\\beta>1italic_\u03b2 > 1. The finite bounded misalignment also infers the weighted Ricci curvature RicN\u2a7e\u2212AKsuperscriptRic\ud835\udc41\ud835\udc34\ud835\udc3e\\operatorname{Ric}^{N}\\geqslant-AKroman_Ric start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2a7e - italic_A italic_K. Then Lemma 3.1 and Lemma 3.2 still hold on Bp(2R)\u00d7[0,T]subscript\ud835\udc35\ud835\udc5d2\ud835\udc450\ud835\udc47B_{p}(2R)\\times[0,T]italic_B start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( 2 italic_R ) \u00d7 [ 0 , italic_T ]. Now we start the proof with choosing a cut-off function. We define a smooth function \u03d5italic-\u03d5\\phiitalic_\u03d5 on [0,\u221e)0[0,\\infty)[ 0 , \u221e ) such that \u03d5(d)\u22611italic-\u03d5\ud835\udc511\\phi(d)\\equiv 1italic_\u03d5 ( italic_d ) \u2261 1 for d\u2a7d1\ud835\udc511d\\leqslant 1italic_d \u2a7d 1, \u03d5(d)\u22610italic-\u03d5\ud835\udc510\\phi(d)\\equiv 0italic_\u03d5 ( italic_d ) \u2261 0 for d\u2a7e2\ud835\udc512d\\geqslant 2italic_d \u2a7e 2 and 0\u2a7d\u03d5\u2a7d10italic-\u03d510\\leqslant\\phi\\leqslant 10 \u2a7d italic_\u03d5 \u2a7d 1 otherwise. Assume that for two positive constants c1subscript\ud835\udc501c_{1}italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and c2subscript\ud835\udc502c_{2}italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Then we define the cut-off function on M\ud835\udc40Mitalic_M by Clearly we notice that \u03a8(x)\u03a8\ud835\udc65\\Psi(x)roman_\u03a8 ( italic_x ) is smooth function with support in Bp(2R)subscript\ud835\udc35\ud835\udc5d2\ud835\udc45B_{p}(2R)italic_B start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( 2 italic_R ) and for any reference vector field V\ud835\udc49Vitalic_V, where the inequality is from (4.1) and the bound of misalignment: Applying the Laplacian comparison theorem in Theorem 2.2 and taking V=\u2207u\ud835\udc49\u2207\ud835\udc62V=\\nabla uitalic_V = \u2207 italic_u on Musubscript\ud835\udc40\ud835\udc62M_{u}italic_M start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, we have The last inequality comes from the inequality coth(x)\u2a7d1+1xhyperbolic-cotangent\ud835\udc6511\ud835\udc65\\coth(x)\\leqslant 1+\\frac{1}{x}roman_coth ( italic_x ) \u2a7d 1 + divide start_ARG 1 end_ARG start_ARG italic_x end_ARG. For simplicity, we denote which infers that Since Cauchy inequality and (4.3), it follows that Now, we compute Using Lemma 3.2, (4.7) and (4.8) in (4.9), a straightforward calculation yields that in sense of distribution on M\u00d7(0,T)\ud835\udc400\ud835\udc47M\\times(0,T)italic_M \u00d7 ( 0 , italic_T ), where Let (z,s)\ud835\udc67\ud835\udc60(z,s)( italic_z , italic_s ) be the maximal point of \u03a8H\u03a8\ud835\udc3b\\Psi Hroman_\u03a8 italic_H on Bp(2R)\u00d7[0,T]subscript\ud835\udc35\ud835\udc5d2\ud835\udc450\ud835\udc47B_{p}(2R)\\times[0,T]italic_B start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( 2 italic_R ) \u00d7 [ 0 , italic_T ] and \u039b=\u03a8(z)H(z,s)\u039b\u03a8\ud835\udc67\ud835\udc3b\ud835\udc67\ud835\udc60\\Lambda=\\Psi(z)H(z,s)roman_\u039b = roman_\u03a8 ( italic_z ) italic_H ( italic_z , italic_s ) be the maximum of \u03a8H\u03a8\ud835\udc3b\\Psi Hroman_\u03a8 italic_H. Following the same argument in section 3, we deduce that \ud835\udca6(z,s)\u2a7d0\ud835\udca6\ud835\udc67\ud835\udc600\\mathcal{K}(z,s)\\leqslant 0caligraphic_K ( italic_z , italic_s ) \u2a7d 0. Here we also set v:=F2(\u2207f)/Hassign\ud835\udc63superscript\ud835\udc392\u2207\ud835\udc53\ud835\udc3bv:=F^{2}(\\nabla f)/Hitalic_v := italic_F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( \u2207 italic_f ) / italic_H, and it implies that at (z,s)\ud835\udc67\ud835\udc60(z,s)( italic_z , italic_s ) Note that for any fixed 0<\u03b4<10\ud835\udeff10<\\delta<10 < italic_\u03b4 < 1, and Therefore we have Then by simply modifying how we deal with different cases a>0\ud835\udc4e0a>0italic_a > 0 and a<0\ud835\udc4e0a<0italic_a < 0 in Section 3, one obtains that when a<0\ud835\udc4e0a<0italic_a < 0, And when a>0\ud835\udc4e0a>0italic_a > 0, Finally, which concludes the proof. \u220e As an application of gradient estimates, We now give Harnack inequalities about the solutions to the equation (1.4). Note that by taking a\u21920\u2192\ud835\udc4e0a\\to 0italic_a \u2192 0 and b=0\ud835\udc4f0b=0italic_b = 0 the equation (1.4) becomes the Finslerian heat equation and the corresponding results have already been given in [15] and [21]. Under the same conditions in Theorem 1.1, we have where for any \u03b2>1\ud835\udefd1\\beta>1italic_\u03b2 > 1, 0<t1<t2<T0subscript\ud835\udc611subscript\ud835\udc612\ud835\udc470<t_{1}<t_{2}<T0 < italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < italic_T and x1subscript\ud835\udc651x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, x2\u2208Msubscript\ud835\udc652\ud835\udc40x_{2}\\in Mitalic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2208 italic_M. For the parameter s\u2208[t1,t2]\ud835\udc60subscript\ud835\udc611subscript\ud835\udc612s\\in[t_{1},t_{2}]italic_s \u2208 [ italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] and some tangent vector v\u2208Tx2M\ud835\udc63subscript\ud835\udc47subscript\ud835\udc652\ud835\udc40v\\in T_{x_{2}}Mitalic_v \u2208 italic_T start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_M, define \u03b3(s):=exp(x2,(t2\u2212s)v)assign\ud835\udefe\ud835\udc60subscript\ud835\udc652subscript\ud835\udc612\ud835\udc60\ud835\udc63\\gamma(s):=\\exp(x_{2},(t_{2}-s)v)italic_\u03b3 ( italic_s ) := roman_exp ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ( italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_s ) italic_v ) as a reverse curve of the minimal geodesic from x2subscript\ud835\udc652x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT to x1subscript\ud835\udc651x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. And that implies \u03b3(t1)=x1\ud835\udefesubscript\ud835\udc611subscript\ud835\udc651\\gamma(t_{1})=x_{1}italic_\u03b3 ( italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, \u03b3(t2)=x2\ud835\udefesubscript\ud835\udc612subscript\ud835\udc652\\gamma(t_{2})=x_{2}italic_\u03b3 ( italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and Putting by Theorem 1.1, Cauchy inequality and (5.2) we see that Therefore, by the monotonicity of \u03c3\ud835\udf0e\\sigmaitalic_\u03c3, which demonstrates the assertion. \u220e Under the assumption in Theorem 1.2, then we have where for any 0<\u03b4<10\ud835\udeff10<\\delta<10 < italic_\u03b4 < 1, \u03b2>1\ud835\udefd1\\beta>1italic_\u03b2 > 1, 0<t1<t2<T0subscript\ud835\udc611subscript\ud835\udc612\ud835\udc470<t_{1}<t_{2}<T0 < italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < italic_T and x1subscript\ud835\udc651x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, x2\u2208Bp(R)subscript\ud835\udc652subscript\ud835\udc35\ud835\udc5d\ud835\udc45x_{2}\\in B_{p}(R)italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2208 italic_B start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_R ). The proof is similar with Corollary 5.1. The only different part is to replace the function \u0398\u0398\\Thetaroman_\u0398 related to gradient estimates so we omit the proof here. In this subsection, we turn to the elliptic equation (1.11). Applying the aforementioned gradient estimate yields the following Liouville-type theorem: Let (M,F,\u03bc)\ud835\udc40\ud835\udc39\ud835\udf07(M,F,\\mu)( italic_M , italic_F , italic_\u03bc ) be a forward complete n\ud835\udc5bnitalic_n-dimensional Finsler metric measure space without boundary, with misalignment and non-Riemannian tensors bounded by A\ud835\udc34Aitalic_A and K0subscript\ud835\udc3e0K_{0}italic_K start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, respectively. Let u\ud835\udc62uitalic_u be a positive solution to (1.11) on M\ud835\udc40Mitalic_M and f=logu\ud835\udc53\ud835\udc62f=\\log uitalic_f = roman_log italic_u. Suppose mixed weighted Ricci curvature RicNm\u2a7e\u2212Ksuperscriptsuperscriptnormal-Ric\ud835\udc41\ud835\udc5a\ud835\udc3e{}^{m}\\operatorname{Ric}^{N}\\geqslant-Kstart_FLOATSUPERSCRIPT italic_m end_FLOATSUPERSCRIPT roman_Ric start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2a7e - italic_K for some N\ud835\udc41Nitalic_N and K\ud835\udc3eKitalic_K satisfying where \u03b1=max{a/2,\u2212a/4}\ud835\udefc\ud835\udc4e2\ud835\udc4e4\\alpha=\\max\\{a/2,-a/4\\}italic_\u03b1 = roman_max { italic_a / 2 , - italic_a / 4 }. If there exists an x0\u2208Msubscript\ud835\udc650\ud835\udc40x_{0}\\in Mitalic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2208 italic_M such that or we then have f(x)\u22610\ud835\udc53\ud835\udc650f(x)\\equiv 0italic_f ( italic_x ) \u2261 0 on M\ud835\udc40Mitalic_M. Since u(x,t)\u2261u(x)\ud835\udc62\ud835\udc65\ud835\udc61\ud835\udc62\ud835\udc65u(x,t)\\equiv u(x)italic_u ( italic_x , italic_t ) \u2261 italic_u ( italic_x ), let R\u2192+\u221e\u2192\ud835\udc45R\\to+\\inftyitalic_R \u2192 + \u221e, \u03b4\u21921\u2192\ud835\udeff1\\delta\\to 1italic_\u03b4 \u2192 1 and t\u2192\u221e\u2192\ud835\udc61t\\to\\inftyitalic_t \u2192 \u221e in (1.8), then we have The conditions in (5.7) imply that there exist Substituting either of \u03b20subscript\ud835\udefd0\\beta_{0}italic_\u03b2 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to (5.7), we have When a<0\ud835\udc4e0a<0italic_a < 0 and f(x0)\u2a7d0\ud835\udc53subscript\ud835\udc6500f(x_{0})\\leqslant 0italic_f ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) \u2a7d 0, there exists a neighborhood U(x0)\ud835\udc48subscript\ud835\udc650U(x_{0})italic_U ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) on which f(x)\u2a7d0\ud835\udc53\ud835\udc650f(x)\\leqslant 0italic_f ( italic_x ) \u2a7d 0 (resp. f(x)\u2a7e0\ud835\udc53\ud835\udc650f(x)\\geqslant 0italic_f ( italic_x ) \u2a7e 0 when a>0\ud835\udc4e0a>0italic_a > 0). Then \u2207f(x)\u22610\u2207\ud835\udc53\ud835\udc650\\nabla f(x)\\equiv 0\u2207 italic_f ( italic_x ) \u2261 0 and f(x)\u22610\ud835\udc53\ud835\udc650f(x)\\equiv 0italic_f ( italic_x ) \u2261 0 for any x\u2208U(x0)\ud835\udc65\ud835\udc48subscript\ud835\udc650x\\in U(x_{0})italic_x \u2208 italic_U ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ). \u220e It should be noted that the conditions in (5.7) are sharp. Failure to fulfill these conditions would result in absence of such a feasible \u03b20>1subscript\ud835\udefd01\\beta_{0}>1italic_\u03b2 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT > 1. Moreover, nonnegative mixed weighted Ricci curvature (RicNm\u2a7e0superscriptsuperscriptRic\ud835\udc41\ud835\udc5a0{}^{m}\\operatorname{Ric}^{N}\\geqslant 0start_FLOATSUPERSCRIPT italic_m end_FLOATSUPERSCRIPT roman_Ric start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2a7e 0) or weighted Ricci curvature (RicN\u2a7e0superscriptRic\ud835\udc410\\operatorname{Ric}^{N}\\geqslant 0roman_Ric start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2a7e 0) inherently meet the second condition in (5.7). Since f=logu\ud835\udc53\ud835\udc62f=\\log uitalic_f = roman_log italic_u, one can derive directly from Theorem 5.3 that u(x)\u2a7e1\ud835\udc62\ud835\udc651u(x)\\geqslant 1italic_u ( italic_x ) \u2a7e 1 on M\ud835\udc40Mitalic_M when a<0\ud835\udc4e0a<0italic_a < 0. Namely, there is a gap for u(x)\ud835\udc62\ud835\udc65u(x)italic_u ( italic_x ) when b>\u03b1N/2=\u2212aN/8\ud835\udc4f\ud835\udefc\ud835\udc412\ud835\udc4e\ud835\udc418b>\\alpha N/2=-aN/8italic_b > italic_\u03b1 italic_N / 2 = - italic_a italic_N / 8. Bin Shen School of Mathematics, Southeast University, Nanjing 211189, P. R. China E-mail: shenbin@seu.edu.cn Yuhan Zhu School of Mathematics, Southeast University, Nanjing 211189, P. R. China E-mail: yuhanzhu@seu.edu.cn"
    },
    {
        "id": 12,
        "title": "Exact minimax entropy models of large\u2013scale neuronal activity",
        "corpus": "In the brain, fine\u2013scale correlations combine to produce macroscopic patterns of activity. However, as experiments record from larger and larger populations, we approach a fundamental bottleneck: the number of correlations one would like to include in a model grows larger than the available data. In this undersampled regime, one must focus on a sparse subset of correlations; the optimal choice contains the maximum information about patterns of activity or, equivalently, minimizes the entropy of the inferred maximum entropy model. Applying this \u201cminimax entropy\u201d principle is generally intractable, but here we present an exact and scalable solution for pairwise correlations that combine to form a tree (a network without loops). Applying our method to over one thousand neurons in the mouse hippocampus, we find that the optimal tree of correlations reduces our uncertainty about the population activity by 14% (over 50 times more than a random tree). Despite containing only 0.1% of all pairwise correlations, this minimax entropy model accurately predicts the observed large\u2013scale synchrony in neural activity and becomes even more accurate as the population grows. The inferred Ising model is almost entirely ferromagnetic (with positive interactions) and exhibits signatures of thermodynamic criticality. These results suggest that a sparse backbone of excitatory interactions may play an important role in driving collective neuronal activity. Understanding how collective behaviors emerge from webs of fine\u2013scale interactions is a central goal in statistical mechanics approaches to networks of neurons Wiener (1958); Cooper (1973); Little (1974); Hopfield (1982); Amit (1989); Hertz et al. (1991). At the same time, exploration of the brain has been revolutionized by experimental methods that monitor, simultaneously, the electrical activity of hundreds or even thousands of neurons Segev et al. (2004); Litke et al. (2004); Chung et al. (2019); Dombeck et al. (2010); Tian et al. (2012); Demas et al. (2021); Steinmetz et al. (2021). One approach to connecting these new data with statistical physics models is maximum entropy, in which we construct the maximally disordered model that is consistent with measured expectation values Jaynes (1957). In particular, is seems natural to build models that match the mean activity of individual neurons and the correlations between pairs of neurons. These pairwise maximum entropy models have been strikingly successful in describing collective behavior not only in networks of real neurons, but also in the evolution of protein families, the dynamics of genetic networks, flocks of birds, and social networks Schneidman et al. (2006); Nguyen et al. (2017); Meshulam et al. (2017, 2021); Tka\u010dik et al. (2015); Lezon et al. (2006); Weigt et al. (2009); Marks et al. (2011); Lapedes et al. (2012); Bialek et al. (2012); Russ et al. (2020); Lynn et al. (2019). But as experiments progress to record from larger and larger numbers of neurons, we face a combinatorial explosion. Even if we focus on pairwise correlations, the number of correlations approaches the number of independent samples in modern experiments Segev et al. (2004); Litke et al. (2004); Chung et al. (2019); Dombeck et al. (2010); Tian et al. (2012); Demas et al. (2021); Steinmetz et al. (2021). In this undersampled regime, one is forced to select only a sparse subset of the correlations to include in any model. While constructing an accurate model with only a small number of correlations may seem hopeless, one can draw inspiration from statistical physics, where effective descriptions of macroscopic phenomena can often ignore many of the microscopic details. Here, given restrictions on the number and structure of correlations we can include in a model, we seek to identify the ones that contain the maximum information about system activity. We demonstrate that the optimal correlations are those that induce the maximum entropy model with minimum entropy Zhu et al. (1997). Solving this minimax entropy problem is generally infeasible. But for pairwise correlations that form a tree (a network without loops), the entropy reduction decomposes into a sum over connected pairs; the advantages of tree structure in models of neural activity have been appreciated in other contexts Prentice et al. (2016). This decomposition reduces the minimax entropy problem to a minimum spanning tree problem, which can be solved exactly and efficiently Chow and Liu (1968); Nguyen et al. (2017). The result is a framework for uncovering the maximally informative tree of correlations in very large systems Lynn et al. (2023). We apply our method to investigate the collective activity of N\u223c1500similar-to\ud835\udc411500N\\sim 1500italic_N \u223c 1500 neurons in the mouse hippocampus Gauthier and Tank (2018). While most pairs of neurons are only weakly correlated, some rare pairs have mutual information orders of magnitude larger than average. By focusing on these exceptionally strong correlations, our minimax entropy model captures 50 times more information than a random tree and, despite containing only 0.1% of all pairwise correlations, produces realistic large\u2013scale synchrony in activity. Moreover, the model becomes even more accurate as the population grows, providing hope for statistical physics descriptions of even larger systems. The paper is organized as follows. In \u00a7II, we define the minimax entropy problem and present a solution for trees of pairwise correlations. In \u00a7III, we review a relatively recent experiment on large\u2013scale recordings of neuronal activity in the mouse hippocampus. In \u00a7IV, we demonstrate that the optimal tree of correlations produces realistic patterns of synchronized activity. In \u00a7V, we investigate the structural properties of the optimal tree and the functional properties of the induced Ising model. In \u00a7VI, we show that the minimax entropy model becomes more accurate for larger populations, and then in \u00a7VII we investigate the thermodynamic properties of the minimax entropy model, finding that the real system is poised at a special point in its phase diagram. Finally, in \u00a7VIII we provide conclusions and outlook. Consider a system of N\ud835\udc41Nitalic_N elements i=1,\u2026,N\ud835\udc561\u2026\ud835\udc41i=1,\\ldots,Nitalic_i = 1 , \u2026 , italic_N with states \ud835\udc99={xi}\ud835\udc99subscript\ud835\udc65\ud835\udc56\\bm{x}=\\{x_{i}\\}bold_italic_x = { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, where xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the state of element i\ud835\udc56iitalic_i. From experiments, we have access to M\ud835\udc40Mitalic_M samples of the system activity \ud835\udc99(m)superscript\ud835\udc99\ud835\udc5a\\bm{x}^{(m)}bold_italic_x start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT, where m=1,\u2026,M\ud835\udc5a1\u2026\ud835\udc40m=1,\\ldots,Mitalic_m = 1 , \u2026 , italic_M. Our knowledge about the system is defined by observables, which can be represented as expectation values where f(\ud835\udc99)\ud835\udc53\ud835\udc99f(\\bm{x})italic_f ( bold_italic_x ) is an arbitrary function of the state \ud835\udc99\ud835\udc99\\bm{x}bold_italic_x. For example, one could measure the average states of individual elements \u27e8xi\u27e9expsubscriptdelimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56exp\\left<x_{i}\\right>_{\\text{exp}}\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9 start_POSTSUBSCRIPT exp end_POSTSUBSCRIPT or the correlations among multiple elements \u27e8xixj\u27e9expsubscriptdelimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57exp\\left<x_{i}x_{j}\\right>_{\\text{exp}}\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 start_POSTSUBSCRIPT exp end_POSTSUBSCRIPT, \u27e8xixjxk\u27e9expsubscriptdelimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57subscript\ud835\udc65\ud835\udc58exp\\left<x_{i}x_{j}x_{k}\\right>_{\\text{exp}}\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u27e9 start_POSTSUBSCRIPT exp end_POSTSUBSCRIPT, and so on. Given a set of K\ud835\udc3eKitalic_K observables \ud835\udcaa={f\u03bd(\ud835\udc99)}\ud835\udcaasubscript\ud835\udc53\ud835\udf08\ud835\udc99\\mathcal{O}=\\{f_{\\nu}(\\bm{x})\\}caligraphic_O = { italic_f start_POSTSUBSCRIPT italic_\u03bd end_POSTSUBSCRIPT ( bold_italic_x ) }, where \u03bd=1,\u2026,K\ud835\udf081\u2026\ud835\udc3e\\nu=1,\\ldots,Kitalic_\u03bd = 1 , \u2026 , italic_K, the most unbiased prediction for the distribution over states is the maximum entropy model Shannon (1948); Jaynes (1957) where Z\ud835\udc4dZitalic_Z is the normalizing partition function, and the parameters \u03bb\u03bdsubscript\ud835\udf06\ud835\udf08\\lambda_{\\nu}italic_\u03bb start_POSTSUBSCRIPT italic_\u03bd end_POSTSUBSCRIPT ensure that the model matches the experimental observations, such that To have control over errors in the K\ud835\udc3eKitalic_K expectation values, we must have K\u226aMNmuch-less-than\ud835\udc3e\ud835\udc40\ud835\udc41K\\ll MNitalic_K \u226a italic_M italic_N. But as experiments record from larger systems, one is confronted with an explosion of possible observables. The total number of correlations grows exponentially with N\ud835\udc41Nitalic_N, and even the K\u221dN2proportional-to\ud835\udc3esuperscript\ud835\udc412K\\propto N^{2}italic_K \u221d italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT pairwise correlations violate the good sampling condition as N\ud835\udc41Nitalic_N grows large. Thus, to avoid sampling problems, one must focus on a sparse subset of correlations. Here we arrive at the central question: Among a large set of observables, which should we choose to include in a model? Suppose we want to find the set of observables \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O that yields the most accurate description of the system. We can choose \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O to maximize the log\u2013likelihood of the model P\ud835\udcaasubscript\ud835\udc43\ud835\udcaaP_{\\mathcal{O}}italic_P start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT or, equivalently, minimize the KL divergence with respect to the data DKL(Pexp||P\ud835\udcaa)D_{\\text{KL}}(P_{\\text{exp}}||P_{\\mathcal{O}})italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_P start_POSTSUBSCRIPT exp end_POSTSUBSCRIPT | | italic_P start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT ). Due to the form of P\ud835\udcaasubscript\ud835\udc43\ud835\udcaaP_{\\mathcal{O}}italic_P start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT in Eq. (2), the KL divergence simplifies to a difference in entropies where the final equality follows from Eq. (3), and entropies S\ud835\udc46Sitalic_S are measured in bits. We therefore find that the optimal observables \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O are the ones that minimize the entropy of the maximum entropy model S\ud835\udcaasubscript\ud835\udc46\ud835\udcaaS_{\\mathcal{O}}italic_S start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT. This is the \u201cminimax entropy\u201d principle, which was proposed 25 years ago but has received relatively little attention Zhu et al. (1997). In addition to providing the best description of the data, the optimal observables \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O can also be viewed as containing the maximum information about the system. If we begin by observing each element individually, then we only have access to the marginal distributions Pi(xi)subscript\ud835\udc43\ud835\udc56subscript\ud835\udc65\ud835\udc56P_{i}(x_{i})italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ); in this case, the maximum entropy model is the independent distribution Pind(\ud835\udc99)=\u220fiPi(xi)subscript\ud835\udc43ind\ud835\udc99subscriptproduct\ud835\udc56subscript\ud835\udc43\ud835\udc56subscript\ud835\udc65\ud835\udc56P_{\\text{ind}}(\\bm{x})=\\prod_{i}P_{i}(x_{i})italic_P start_POSTSUBSCRIPT ind end_POSTSUBSCRIPT ( bold_italic_x ) = \u220f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) with entropy Sindsubscript\ud835\udc46indS_{\\text{ind}}italic_S start_POSTSUBSCRIPT ind end_POSTSUBSCRIPT. If, in addition to the marginals, we also observe some of the correlations between elements Schneidman et al. (2003), this knowledge reduces our uncertainty about the system by an amount I\ud835\udcaa=Sind\u2212S\ud835\udcaa\u22650subscript\ud835\udc3c\ud835\udcaasubscript\ud835\udc46indsubscript\ud835\udc46\ud835\udcaa0I_{\\mathcal{O}}=S_{\\text{ind}}-S_{\\mathcal{O}}\\geq 0italic_I start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT = italic_S start_POSTSUBSCRIPT ind end_POSTSUBSCRIPT - italic_S start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT \u2265 0. Thus, by minimizing S\ud835\udcaasubscript\ud835\udc46\ud835\udcaaS_{\\mathcal{O}}italic_S start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT, the optimal observables \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O also maximize the information I\ud835\udcaasubscript\ud835\udc3c\ud835\udcaaI_{\\mathcal{O}}italic_I start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT contained in the observed correlations. In practice, applying the minimax entropy principle poses two distinct challenges. First, for each set of observables \ud835\udcaa={f\u03bd}\ud835\udcaasubscript\ud835\udc53\ud835\udf08\\mathcal{O}=\\{f_{\\nu}\\}caligraphic_O = { italic_f start_POSTSUBSCRIPT italic_\u03bd end_POSTSUBSCRIPT }, one must solve the traditional maximum entropy problem; that is, one must compute the parameters \u03bb\u03bdsubscript\ud835\udf06\ud835\udf08\\lambda_{\\nu}italic_\u03bb start_POSTSUBSCRIPT italic_\u03bd end_POSTSUBSCRIPT such that the model P\ud835\udcaasubscript\ud835\udc43\ud835\udcaaP_{\\mathcal{O}}italic_P start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT matches the expectations \u27e8f\u03bd(\ud835\udc99)\u27e9expsubscriptdelimited-\u27e8\u27e9subscript\ud835\udc53\ud835\udf08\ud835\udc99exp\\left<f_{\\nu}(\\bm{x})\\right>_{\\text{exp}}\u27e8 italic_f start_POSTSUBSCRIPT italic_\u03bd end_POSTSUBSCRIPT ( bold_italic_x ) \u27e9 start_POSTSUBSCRIPT exp end_POSTSUBSCRIPT in the data. Second, one must repeat this calculation for all sets of observables \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O to find the one that minimizes the entropy S\ud835\udcaasubscript\ud835\udc46\ud835\udcaaS_{\\mathcal{O}}italic_S start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT. This search process is generally intractable. In what follows, we study a class of observables that admits an exact and efficient solution, enabling statistical physics models of very large systems. For simplicity, we focus on binary variables xi=0,1subscript\ud835\udc65\ud835\udc5601x_{i}=0,1italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0 , 1, for which the marginals Pi(xi)subscript\ud835\udc43\ud835\udc56subscript\ud835\udc65\ud835\udc56P_{i}(x_{i})italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) are defined by the averages \u27e8xi\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56\\left<x_{i}\\right>\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9. In the search for sources of order in a system, one might begin with the simplest correlations: those between pairs of elements \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\left<x_{i}x_{j}\\right>\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9. In populations of N\u223c100similar-to\ud835\udc41100N\\sim 100italic_N \u223c 100 neurons, one often has sufficient data to fit all the pairwise correlations, which can be very effective in capturing key features of the collective activity Schneidman et al. (2006); Meshulam et al. (2017, 2021). But this corresponds to K\u221dN2proportional-to\ud835\udc3esuperscript\ud835\udc412K\\propto N^{2}italic_K \u221d italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT constraints, and at large N\ud835\udc41Nitalic_N we will violate the good sampling condition K\u226aNMmuch-less-than\ud835\udc3e\ud835\udc41\ud835\udc40K\\ll NMitalic_K \u226a italic_N italic_M. To avoid undersampling, we are forced to select a sparse subset of pairwise correlations, which can be visualized as a network \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G with edges defining the observed correlations between variables. Each network induces a maximum entropy model where the parameters hisubscript\u210e\ud835\udc56h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and Jijsubscript\ud835\udc3d\ud835\udc56\ud835\udc57J_{ij}italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT enforce the constraints on \u27e8xi\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56\\left<x_{i}\\right>\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9 and \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\left<x_{i}x_{j}\\right>\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 in \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G, respectively. The minimax entropy principle tells us that we should find the network \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G (within some allowed set) that produces the maximum entropy model P\ud835\udca2subscript\ud835\udc43\ud835\udca2P_{\\mathcal{G}}italic_P start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT with minimum entropy S\ud835\udca2subscript\ud835\udc46\ud835\udca2S_{\\mathcal{G}}italic_S start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT. In statistical physics, calculations are difficult in part due to feedback loops. By eliminating loops, many statistical physics models become tractable, as in one\u2013dimensional systems or on Bethe lattices Sethna (2021). In the Ising model\u2014which is equivalent to Eq. (5)\u2014if the interactions Jijsubscript\ud835\udc3d\ud835\udc56\ud835\udc57J_{ij}italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT lie on a tree \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T (or a network without loops), then one can efficiently compute the partition function Z\ud835\udc4dZitalic_Z and all statistics of interest (see Appendix A). Inverting this procedure, one can begin with the averages \u27e8xi\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56\\left<x_{i}\\right>\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9 and the correlations \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\left<x_{i}x_{j}\\right>\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 on a tree \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T and analytically derive the maximum entropy parameters Chow and Liu (1968); Nguyen et al. (2017): where \ud835\udca9isubscript\ud835\udca9\ud835\udc56\\mathcal{N}_{i}caligraphic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represents the neighbors of i\ud835\udc56iitalic_i in \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T (see Appendix B). Since each tree contains N\u22121\ud835\udc411N-1italic_N - 1 correlations, the total number of observables is K=2N\u22121\ud835\udc3e2\ud835\udc411K=2N-1italic_K = 2 italic_N - 1, and so we are well sampled if the number of independent samples obeys M\u226b2much-greater-than\ud835\udc402M\\gg 2italic_M \u226b 2. Equations (6) and (7) solve the maximum entropy problem for the distribution P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT, but we still need to search over all of the NN\u22122superscript\ud835\udc41\ud835\udc412N^{N-2}italic_N start_POSTSUPERSCRIPT italic_N - 2 end_POSTSUPERSCRIPT trees to find the one that minimizes the entropy S\ud835\udcafsubscript\ud835\udc46\ud835\udcafS_{\\mathcal{T}}italic_S start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT. This search simplifies significantly by noticing that the information I\ud835\udcafsubscript\ud835\udc3c\ud835\udcafI_{\\mathcal{T}}italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT decomposes into a sum over the connections (ij)\ud835\udc56\ud835\udc57(ij)( italic_i italic_j ) in \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T, where Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT is the mutual information between i\ud835\udc56iitalic_i and j\ud835\udc57jitalic_j (see Appendix C) Chow and Liu (1968); Nguyen et al. (2017). Note that for pairs (ij)\u2208\ud835\udcaf\ud835\udc56\ud835\udc57\ud835\udcaf(ij)\\in\\mathcal{T}( italic_i italic_j ) \u2208 caligraphic_T, the mutual information Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT is the same in the model and the data, so we can compute the entropy S\ud835\udcafsubscript\ud835\udc46\ud835\udcafS_{\\mathcal{T}}italic_S start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT directly from the data without constructing the model itself. Equation (8) tells us that the tree with the minimum entropy S\ud835\udcafsubscript\ud835\udc46\ud835\udcafS_{\\mathcal{T}}italic_S start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT is the one with the largest total mutual information. Identifying this optimal tree is a minimum spanning tree problem Nguyen et al. (2017), which can be solved efficiently using a number of different algorithms Moore and Mertens (2011). To begin, one computes the mutual information Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT between all elements [Fig. 1(a)]. One can then grow the optimal tree by greedily connecting the element i\ud835\udc56iitalic_i in the tree to the new element j\ud835\udc57jitalic_j with the largest mutual information Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT [Fig. 1(b)]; this is Prim\u2019s algorithm, which runs in O(N2)\ud835\udc42superscript\ud835\udc412O(N^{2})italic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) time [Fig. 1(c)]. Thus, by restricting to trees of pairwise correlations, we can solve the minimax entropy problem exactly, even at very large N\ud835\udc41Nitalic_N. We ultimately seek to explain the collective behaviors of very large networks. However, each tree only contains a vanishingly small fraction 2/N2\ud835\udc412/N2 / italic_N of all pairwise correlations; and even if we have access to all of the pairwise statistics, there\u2019s still no guarantee of success. Can such a sparse set of observations capture something important about the system as a whole? To answer this question, we consider patterns of electrical activity in N=1485\ud835\udc411485N=1485italic_N = 1485 neurons in the hippocampus of a mouse, recorded in a recent experiment Gauthier and Tank (2018). Mice are genetically engineered so that their neurons express a protein whose fluorescence is modulated by calcium concentration, which in turn follows the electrical activity of the cells. This fluorescence is recorded using a scanning two\u2013photon microscope as the mouse runs in a virtual environment. The signal from each cell consists of a quiet background punctuated by short bursts of activity Meshulam et al. (2017), providing a natural binarization into active (xi=1subscript\ud835\udc65\ud835\udc561x_{i}=1italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1) or silent (xi=0subscript\ud835\udc65\ud835\udc560x_{i}=0italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0) within each video frame [Fig. 2(a)]. Capturing images at 30Hz30Hz30\\,\\text{Hz}30 Hz for 39min39min39\\,\\text{min}39 min yields M\u223c7\u00d7104similar-to\ud835\udc407superscript104M\\sim 7\\times 10^{4}italic_M \u223c 7 \u00d7 10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT samples of the collective state \ud835\udc99={xi}\ud835\udc99subscript\ud835\udc65\ud835\udc56\\bm{x}=\\{x_{i}\\}bold_italic_x = { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, but these are not all independent. Nonetheless, we can still estimate the mutual information Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT with small errors after correcting for finite data effects (see Appendix D). Among all \u223c106similar-toabsentsuperscript106\\sim 10^{6}\u223c 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT pairs of neurons, only 9%percent99\\%9 % exhibit significant mutual information with values shown in Fig. 2(b). We see that a small number of pairs contain orders of magnitude more information than average (I\u00af=2.9\u00d710\u22124\u00af\ud835\udc3c2.9superscript104\\bar{I}=2.9\\times 10^{-4}over\u00af start_ARG italic_I end_ARG = 2.9 \u00d7 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT bits). This heavy\u2013tailed distribution provides hope for a tree of correlations that contains much more information than typical I\ud835\udcaf\u226b(N\u22121)I\u00afmuch-greater-thansubscript\ud835\udc3c\ud835\udcaf\ud835\udc411\u00af\ud835\udc3cI_{\\mathcal{T}}\\gg(N-1)\\bar{I}italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT \u226b ( italic_N - 1 ) over\u00af start_ARG italic_I end_ARG. Additionally, while most pairs of cells are negatively correlated [Fig. 2(c)], the strongest mutual information corresponds to positive correlations [Fig. 2(d)]. And while most neurons are far from one another [Fig. 2(e)], larger values of Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT are concentrated among pairs of cells that are close to one another, as can be seen by plotting the mean mutual information as a function of distance [Fig. 2(f)]. Together, these observations suggest that a backbone of positively correlated and physically proximate neurons may provide a large amount of information about the collective neural activity. Constructing the minimax entropy tree (Fig. 1), we find that that it captures I\ud835\udcaf=26.2subscript\ud835\udc3c\ud835\udcaf26.2I_{\\mathcal{T}}=26.2italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT = 26.2 bits of information. This reduces our uncertainty about the population activity by I\ud835\udcaf/Sind=14.4%subscript\ud835\udc3c\ud835\udcafsubscript\ud835\udc46indpercent14.4I_{\\mathcal{T}}/S_{\\text{ind}}=14.4\\%italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT / italic_S start_POSTSUBSCRIPT ind end_POSTSUBSCRIPT = 14.4 %, which is equivalent to freezing the states of 214 randomly selected neurons. For comparison, we consider two additional networks: (i) a random tree, which represents a typical collection of correlations, and (ii) the tree of minimum physical distances, which reflects the fact that neighboring neurons are more likely to be strongly correlated [Fig. 2(f)]. The optimal tree captures over twice as much information as the minimum distance tree and over 50 times more than random. While each model P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT is defined to match a sparse subset of the observed correlations \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\left<x_{i}x_{j}\\right>\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 (and thus mutual information Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT) for (ij)\u2208\ud835\udcaf\ud835\udc56\ud835\udc57\ud835\udcaf(ij)\\in\\mathcal{T}( italic_i italic_j ) \u2208 caligraphic_T, we can ask what P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT predicts for all pairs of neurons (see Appendix E). We note that the optimal tree does not simply match the largest N\u22121\ud835\udc411N-1italic_N - 1 values of Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT; in general, these will form loops. Yet we find that the minimax entropy model still predicts the distribution of Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT within experimental error for the top \u223cNsimilar-toabsent\ud835\udc41\\sim N\u223c italic_N values [Fig. 3(a)]. Indeed, we find that the model captures the strong correlations in the population [Fig. 3(b)]; this accuracy decreases significantly for the minimum distance and random trees (see Appendix F). As expected, the optimal tree underpredicts the strengths of weak and negative correlations [Fig. 3(b)]. Although these correlations may seem unimportant individually, we note that they comprise the vast majority of neuron pairs [Fig. 2(c)]. With knowledge of only 2/N\u223c0.1%similar-to2\ud835\udc41percent0.12/N\\sim 0.1\\%2 / italic_N \u223c 0.1 % of the pairwise correlations, can the optimal tree capture collective behavior in the system? In neuronal populations (and other complex systems), one key collective property is synchronized activity Schneidman et al. (2006); Tka\u010dik et al. (2015); Meshulam et al. (2021); Lynn et al. (2019), which is characterized by the probability P(K)\ud835\udc43\ud835\udc3eP(K)italic_P ( italic_K ) that K\ud835\udc3eKitalic_K out of the N\ud835\udc41Nitalic_N neurons are simultaneous active. If the neurons were independent, this distribution would be approximately Gaussian at large N\ud835\udc41Nitalic_N (Fig. 4, dashed). But in real populations, the dependencies among neurons leads to a much broader distribution (Fig. 4, black), with moments of extreme synchrony in both activity (large K\ud835\udc3eKitalic_K) and silence (small K\ud835\udc3eKitalic_K). If one builds a model from pairwise correlations chosen at random, then the distribution P(K)\ud835\udc43\ud835\udc3eP(K)italic_P ( italic_K ) is almost indistinguishable from that of an independent system (see Appendix F). By contrast, the optimal tree captures most of this collective behavior Lynn et al. (2023), correctly predicting \u2273100greater-than-or-equivalent-toabsent100\\gtrsim 100\u2273 100\u2013fold increases in the probabilities that K\u227350greater-than-or-equivalent-to\ud835\udc3e50K\\gtrsim 50italic_K \u2273 50 neurons are active in the same small time bin (Fig. 4, red). Although the detailed patterns of activity in the system are shaped by competing interactions that are missing from our optimal tree, this shows that large\u2013scale synchrony can emerge from a sparse network of the strongest correlations. To understand the nature of the optimal tree, we can study the minimax entropy model P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT itself, which, as discussed above, is equivalent to an Ising model from statistical physics. This mapping is made concrete by considering a system of spins \u03c3i=2xi\u22121\u2208\u00b11subscript\ud835\udf0e\ud835\udc562subscript\ud835\udc65\ud835\udc561plus-or-minus1\\sigma_{i}=2x_{i}-1\\in\\pm 1italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 2 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - 1 \u2208 \u00b1 1 with Ising interactions JijI=Jij/4subscriptsuperscript\ud835\udc3dI\ud835\udc56\ud835\udc57subscript\ud835\udc3d\ud835\udc56\ud835\udc574J^{\\text{I}}_{ij}=J_{ij}/4italic_J start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT / 4 and local fields hiI=hi/2+\u2211jJijIsubscriptsuperscript\u210eI\ud835\udc56subscript\u210e\ud835\udc562subscript\ud835\udc57subscriptsuperscript\ud835\udc3dI\ud835\udc56\ud835\udc57h^{\\text{I}}_{i}=h_{i}/2+\\sum_{j}J^{\\text{I}}_{ij}italic_h start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / 2 + \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_J start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT, where Jijsubscript\ud835\udc3d\ud835\udc56\ud835\udc57J_{ij}italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT and hisubscript\u210e\ud835\udc56h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are defined in Eqs. (6) and (7). If the interaction JijIsubscriptsuperscript\ud835\udc3dI\ud835\udc56\ud835\udc57J^{\\text{I}}_{ij}italic_J start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT is positive (negative), then activity in neuron i\ud835\udc56iitalic_i leads to activity (silence) in neuron j\ud835\udc57jitalic_j, and vice versa. For random trees, the interactions JijIsubscriptsuperscript\ud835\udc3dI\ud835\udc56\ud835\udc57J^{\\text{I}}_{ij}italic_J start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT are nearly evenly split between positive and negative [Fig. 5(a)]; this is consistent with previous investigations of fully\u2013connected models in populations of N\u223c100similar-to\ud835\udc41100N\\sim 100italic_N \u223c 100 neurons Schneidman et al. (2006); Tka\u010dik et al. (2015); Meshulam et al. (2021). Meanwhile, we recall that the largest mutual information in the population belongs to positively correlated neurons [Fig. 2(d)]. Accordingly, the optimal tree has interactions that are almost exclusively positive [Fig. 5(a)]. We have arrived, perhaps surprisingly, at a traditional Ising ferromagnet. While the interactions JijIsubscriptsuperscript\ud835\udc3dI\ud835\udc56\ud835\udc57J^{\\text{I}}_{ij}italic_J start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT define effective influences between neurons, the local fields represent individual biases toward activity (hiI>0subscriptsuperscript\u210eI\ud835\udc560h^{\\text{I}}_{i}>0italic_h start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 0) or silence (hiI<0subscriptsuperscript\u210eI\ud835\udc560h^{\\text{I}}_{i}<0italic_h start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT < 0). For random trees, all of the local fields are negative [Fig. 5(b)], reflecting the fact that neurons are more likely to be silent than active. But in the optimal tree, we see that some neurons are counterintuitively biased toward activity with hiI>0subscriptsuperscript\u210eI\ud835\udc560h^{\\text{I}}_{i}>0italic_h start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 0 [Fig. 5(b)]. These positive biases stand in competition with the positive interactions in the model, which, because neurons favor silence, tend to induce silence in the population. To understand the effects of interactions on individual cells, we note that the average activity of an independent neuron i\ud835\udc56iitalic_i is fully defined by hiIsubscriptsuperscript\u210eI\ud835\udc56h^{\\text{I}}_{i}italic_h start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT through the relation \u27e8\u03c3i\u27e9=tanhhiIdelimited-\u27e8\u27e9subscript\ud835\udf0e\ud835\udc56subscriptsuperscript\u210eI\ud835\udc56\\left<\\sigma_{i}\\right>=\\tanh h^{\\text{I}}_{i}\u27e8 italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9 = roman_tanh italic_h start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Since random trees contain only weak correlations, the neuronal activity closely tracks this independent prediction [Fig. 5(c)]. As interactions become increasingly positive in the minimum distance and optimal trees, the alignment of neighboring neurons produces average activities that are significantly lower than one would expect from local fields alone [Fig. 5(c)]. For each neuron i\ud835\udc56iitalic_i, the competition between internal biases and interactions is made clear by comparing the local field hiIsubscriptsuperscript\u210eI\ud835\udc56h^{\\text{I}}_{i}italic_h start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to the average influence due to interactions hiint=\u2211jJijI\u27e8\u03c3j\u27e9subscriptsuperscript\u210eint\ud835\udc56subscript\ud835\udc57subscriptsuperscript\ud835\udc3dI\ud835\udc56\ud835\udc57delimited-\u27e8\u27e9subscript\ud835\udf0e\ud835\udc57h^{\\text{int}}_{i}=\\sum_{j}J^{\\text{I}}_{ij}\\left<\\sigma_{j}\\right>italic_h start_POSTSUPERSCRIPT int end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_J start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT \u27e8 italic_\u03c3 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9. In random trees, only 1%percent11\\%1 % of neurons are dominated by interactions, such that hiint<hiIsubscriptsuperscript\u210eint\ud835\udc56subscriptsuperscript\u210eI\ud835\udc56h^{\\text{int}}_{i}<h^{\\text{I}}_{i}italic_h start_POSTSUPERSCRIPT int end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT < italic_h start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT [Fig. 5(d)]; this proportion increases to 43%percent4343\\%43 % in the minimum distance tree and 57%percent5757\\%57 % in the optimal tree [Fig. 5(d)]. So despite the fact that the tree structure constrains each cell to only interact with two others in the entire population (on average), most neurons in the optimal tree are driven more strongly by interactions than internal biases. In addition to the functional properties of the model P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT, we can also study the graph structure of the optimal tree \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T. To visualize each tree, we place the cell with the most connections (or largest degree ki=|\ud835\udca9i|subscript\ud835\udc58\ud835\udc56subscript\ud835\udca9\ud835\udc56k_{i}=|\\mathcal{N}_{i}|italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = | caligraphic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT |) at the center and all of the cells with single connections (ki=1subscript\ud835\udc58\ud835\udc561k_{i}=1italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1) around the perimeter [Fig. 6(a)]. For random trees, the distribution of degrees is Poisson [Fig. 6(b)], preventing the emergence of high\u2013degree hub nodes. Degrees are even more sharply peaked in the minimum distance tree, such that we do not observe a single neuron with more than four connections [Fig. 6(b)]. By contrast, the optimal tree has a much broader degree distribution, with a central neuron that connects to 29 other cells in the population [Fig. 6(b)]. Such hub nodes are frequently observed in the brain\u2019s physical connectivity Song et al. (2005); Lynn and Bassett (2019); Lin et al. (2023), and are thought to play an important role in facilitating communication Albert and Barab\u00e1si (2002). By maximizing information about the population, one might hope that the optimal tree captures features of the true interactions between neurons. In the brain, demands on communication are constrained by energetic costs Harris et al. (2012). Networks have evolved to balance efficient communication (minimizing the number of steps between cells in the network, known as topological distance) with energetic efficiency (minimizing the physical lengths of connections) Laughlin and Sejnowski (2003); Lynn and Bassett (2019). These pressures are in direct competition: Networks with physically local connections form lattice\u2013like structures with long topological distances, and networks with short topological distances (known as the small\u2013world property Watts and Strogatz (1998)) require physically long\u2013range connections. Indeed, in the minimum distance tree, which is composed of the physically shortest connections [Fig. 6(c)], communication between two neurons requires \u223c100similar-toabsent100\\sim 100\u223c 100 intermediate cells on average [Fig. 6(d)]; and random trees, which are known to produce short topological distances [Fig. 6(d)], are mostly composed of long\u2013range connections [Fig. 6(c)]. Meanwhile, the minimax entropy model identifies connections that are much shorter than average [Fig. 6(c)] while simultaneously maintaining small\u2013world structure [Fig. 6(d)], just as observed in real neuronal networks Laughlin and Sejnowski (2003). Thus far, we have focused on a single population of N\u223c1500similar-to\ud835\udc411500N\\sim 1500italic_N \u223c 1500 neurons. But as experiments advance to record from even larger populations, how does the minimax entropy model scale with N\ud835\udc41Nitalic_N? To answer this question, in the spirit of Ref. Meshulam et al. (2021) one can imagine growing a contiguous population centered at a single neuron [Fig. 7(a)], and computing the optimal tree for increasing population sizes. Due to the efficiency of our model, we can repeat this process starting from each of the different neurons and average over the results. As the population grows, the independent entropy Sindsubscript\ud835\udc46indS_{\\text{ind}}italic_S start_POSTSUBSCRIPT ind end_POSTSUBSCRIPT must increase extensively (that is, linearly with N\ud835\udc41Nitalic_N) on average [Fig. 7(b)]. Since each tree contains N\u22121\ud835\udc411N-1italic_N - 1 correlations, one might also expect the information I\ud835\udcafsubscript\ud835\udc3c\ud835\udcafI_{\\mathcal{T}}italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT of any tree to scale extensively. However, we find that the scaling of I\ud835\udcafsubscript\ud835\udc3c\ud835\udcafI_{\\mathcal{T}}italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT with population size depends critically on which correlations we use in building the tree [Fig. 7(b)]. If the information I\ud835\udcafsubscript\ud835\udc3c\ud835\udcafI_{\\mathcal{T}}italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT grows extensively, then the model P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT explains a constant proportion of the independent entropy I\ud835\udcaf/Sindsubscript\ud835\udc3c\ud835\udcafsubscript\ud835\udc46indI_{\\mathcal{T}}/S_{\\text{ind}}italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT / italic_S start_POSTSUBSCRIPT ind end_POSTSUBSCRIPT across different population sizes. Indeed, because the properties of the closest neurons do not change as the population grows (on average), the minimum distance tree captures a nearly constant \u223c6%similar-toabsentpercent6\\sim 6\\%\u223c 6 % of the independent entropy [Fig. 7(c)]. By contrast, since the mutual information Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT between neurons tends to decrease with physical distance [Fig. 2(f)], the average mutual information I\u00af\u00af\ud835\udc3c\\bar{I}over\u00af start_ARG italic_I end_ARG in a spatially contiguous population decreases with N\ud835\udc41Nitalic_N. Thus, the typical information in a random tree grows subextensively with the population size [Fig. 7(b)], and the fractional information I\ud835\udcaf/Sindsubscript\ud835\udc3c\ud835\udcafsubscript\ud835\udc46indI_{\\mathcal{T}}/S_{\\text{ind}}italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT / italic_S start_POSTSUBSCRIPT ind end_POSTSUBSCRIPT vanishes [Fig. 7(c)]. But even though the average mutual information I\u00af\u00af\ud835\udc3c\\bar{I}over\u00af start_ARG italic_I end_ARG decreases, as the population grows we uncover more of the exceptionally large mutual information Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT in the tail of the distribution [Fig. 2(b)]. By identifying these highly informative correlations, the optimal tree accumulates a superextensive amount of information I\ud835\udcafsubscript\ud835\udc3c\ud835\udcafI_{\\mathcal{T}}italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT [Fig. 7(b)], thus capturing a greater proportion of the independent entropy as N\ud835\udc41Nitalic_N increases [Fig. 7(c)]. This increased explanatory power is underpinned by stronger interactions and weaker local fields in the Ising network [Fig. 7(d)]. There is no sign that the trend in Fig. 7(c) is saturating at N\u223c103similar-to\ud835\udc41superscript103N\\sim 10^{3}italic_N \u223c 10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT, suggesting that our minimax entropy framework may become even more effective for larger populations. As discussed above, each tree of observed correlations \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T generates a maximum entropy model P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT [Eq. (5)], which in turn is equivalent to a system of Ising spins. This mapping from experimental observations to statistical physics gives us the opportunity to ask whether the model P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT occupies a special place in the space of possible models. In statistical mechanics, equilibrium systems are described by the Boltzmann distribution, where T\ud835\udc47Titalic_T is the temperature of the system and E(\ud835\udc99)\ud835\udc38\ud835\udc99E(\\bm{x})italic_E ( bold_italic_x ) is the Hamiltonian, which defines the energy of state \ud835\udc99\ud835\udc99\\bm{x}bold_italic_x. For a given tree \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T, we notice that P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT defines a Boltzmann distribution with temperature T=1\ud835\udc471T=1italic_T = 1 and energy where Jijsubscript\ud835\udc3d\ud835\udc56\ud835\udc57J_{ij}italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT and hisubscript\u210e\ud835\udc56h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are defined in Eqs. (6) and (7). Note that we do not assume the experimental system itself is in equilibrium; this correspondence is purely mathematical. By perturbing the temperature away from T=1\ud835\udc471T=1italic_T = 1, we can probe at least one slice through the space of possible networks Tka\u010dik et al. (2015). For each value of T\ud835\udc47Titalic_T, we arrive at a hypothetical system P\ud835\udcaf(\ud835\udc99;T)subscript\ud835\udc43\ud835\udcaf\ud835\udc99\ud835\udc47P_{\\mathcal{T}}(\\bm{x};T)italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT ( bold_italic_x ; italic_T ) with average activities \u27e8xi\u27e9Tsubscriptdelimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56\ud835\udc47\\left<x_{i}\\right>_{T}\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9 start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT and correlations \u27e8xixj\u27e9Tsubscriptdelimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\ud835\udc47\\left<x_{i}x_{j}\\right>_{T}\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT that are no longer constrained to match experimental observations. Consider the minimax entropy tree, which (as discussed in \u00a7V) produces a ferromagnetic Ising model with nearly all positive interactions JijIsubscriptsuperscript\ud835\udc3dI\ud835\udc56\ud835\udc57J^{\\text{I}}_{ij}italic_J start_POSTSUPERSCRIPT I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT [Fig. 5(a)]. At high temperatures T\u226b1much-greater-than\ud835\udc471T\\gg 1italic_T \u226b 1, fluctuations destroy the preference for silence over activity, and the system approaches the average activity 1N\u2211i\u27e8xi\u27e9T=0.51\ud835\udc41subscript\ud835\udc56subscriptdelimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56\ud835\udc470.5\\frac{1}{N}\\sum_{i}\\left<x_{i}\\right>_{T}=0.5divide start_ARG 1 end_ARG start_ARG italic_N end_ARG \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9 start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = 0.5 [Fig. 8(a)]. Meanwhile, at low temperatures T\u226a1much-less-than\ud835\udc471T\\ll 1italic_T \u226a 1, activity vanishes as the network freezes into the all\u2013silent ground state \ud835\udc99=0\ud835\udc990\\bm{x}=0bold_italic_x = 0 [Fig. 8(a)]. In both limits, all of the information contained in correlations is lost. As the temperature decreases, most systems experience a gradual transition from disorder to order. But for certain combinations of parameters Jijsubscript\ud835\udc3d\ud835\udc56\ud835\udc57J_{ij}italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT and hisubscript\u210e\ud835\udc56h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, a small change in the temperature T\ud835\udc47Titalic_T can lead to a large change in the behavior of the system, and as N\ud835\udc41Nitalic_N becomes large, this transition becomes sharp Sethna (2021); Tka\u010dik et al. (2015); Schnabel et al. (2011). Such phase transitions mark a critical point in the space of possible systems, with the Ising ferromagnet as the canonical example Peierls (1936). In the optimal tree, as the temperature increases just above T=1\ud835\udc471T=1italic_T = 1, the positive interactions lead to a much steeper increase in activity than an independent system; and this transition grows even sharper for larger populations [Fig. 8(a)]. We emphasize that at any finite N\ud835\udc41Nitalic_N there is no true critical point, but N\u223c1000similar-to\ud835\udc411000N\\sim 1000italic_N \u223c 1000 may be large enough that the idealization N\u2192\u221e\u2192\ud835\udc41N\\rightarrow\\inftyitalic_N \u2192 \u221e is useful. Since we are studying models defined on trees, there are also subtleties about how one would construct the thermodynamic limit, since such a large fraction of sites are on the boundary Baxter (2016). For our purposes, the interesting question is whether real networks of neurons are in any sense at special points in the space of possible networks. One way in which this could happen is if parameters are set so that simple macroscopic quantities have near\u2013extremal values. One example of a macroscopic quantity that provides a global measure of collective behavior is the total susceptibility of the mean activity to changes in the bias fields, where the rewriting in terms of connected correlations can be derived from the Boltzmann distribution [Eq. (9)]. We recall that at conventional critical points we would see a divergence of \u03c7/N\ud835\udf12\ud835\udc41\\chi/Nitalic_\u03c7 / italic_N as N\u2192\u221e\u2192\ud835\udc41N\\rightarrow\\inftyitalic_N \u2192 \u221e. At both high and low temperatures, correlations are destroyed, and the susceptibility vanishes [Fig. 8(b)]. However, at intermediate temperatures, the susceptibility exhibits a peak that becomes sharper as the system grows, even after normalizing by the population size N\ud835\udc41Nitalic_N [Fig. 8(b)]. Moreover, as N\ud835\udc41Nitalic_N increases, the peak temperature decreases toward T=1\ud835\udc471T=1italic_T = 1, corresponding to the true minimax entropy model P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT. By contrast, the minimum distance and random trees undergo smooth transitions from disorder to order [Fig. 8(c)], with the maximum susceptibility and peak temperatures remaining approximately constant across all population sizes N\ud835\udc41Nitalic_N [Fig. 8(d)]. In addition to the susceptibility \u03c7\ud835\udf12\\chiitalic_\u03c7, we also observe a dramatic peak in the specific heat C/N\ud835\udc36\ud835\udc41C/Nitalic_C / italic_N [Fig. 8(e)], where is the heat capacity (see Appendix G). Although there is no meaning to \u201cheat\u201d in this system, because the specific heat is related to the variance in energy, we can think of the peak in C\ud835\udc36Citalic_C as being a peak in the dynamic range of (log) probabilities across the states of the network. These divergences in the susceptibility and heat capacity also align with a sharp peak in the information fraction I\ud835\udcaf/Sindsubscript\ud835\udc3c\ud835\udcafsubscript\ud835\udc46indI_{\\mathcal{T}}/S_{\\text{ind}}italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT / italic_S start_POSTSUBSCRIPT ind end_POSTSUBSCRIPT [Fig. 8(f)], with larger systems becoming even more strongly correlated. Together, these results indicate that the true minimax entropy model P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT is poised near a special point in the space of models P\ud835\udcaf(\ud835\udc99;T)subscript\ud835\udc43\ud835\udcaf\ud835\udc99\ud835\udc47P_{\\mathcal{T}}(\\bm{x};T)italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT ( bold_italic_x ; italic_T ), where small changes in parameter values can produce large changes in the collective behavior of the system. The maximum entropy principle provides the most unbiased mapping from experimental observations to statistical physics models. Over the past two decades, this link has proven useful in understanding the emergence of collective behaviors in populations of neurons and other complex living systems Schneidman et al. (2006); Nguyen et al. (2017); Meshulam et al. (2017, 2021); Tka\u010dik et al. (2015); Lezon et al. (2006); Weigt et al. (2009); Marks et al. (2011); Lapedes et al. (2012); Bialek et al. (2012); Russ et al. (2020); Lynn et al. (2019). Less widely emphasized is the fact that there is not a single maximum entropy model, but rather a landscape of possible models depending on what features of the system we choose to constrain. Quite generally, we should choose the features that are most informative\u2014the ones that minimize the entropy of the maximum entropy model\u2014leading to the minimax entropy principle Zhu et al. (1997). As experiments record from larger and larger populations of neurons Segev et al. (2004); Litke et al. (2004); Chung et al. (2019); Dombeck et al. (2010); Tian et al. (2012); Demas et al. (2021); Steinmetz et al. (2021), we enter an undersampled regime in which selecting a limited number of maximally informative features is not only conceptually appealing, but also practically necessary. While the minimax entropy problem is generally intractable, here we make progress in two steps. First, we build upon previous work by constraining mean activities and pairwise correlations, resulting in models that are equivalent to systems of Ising spins. Second, taking inspiration from the Bethe lattice, we focus only on trees of correlations, or sparse networks without loops. Under these restrictions, we solve the minimax entropy problem exactly, identifying the optimal tree in quadratic time Chow and Liu (1968); Nguyen et al. (2017). The result is a non\u2013trivial family of statistical physics models that can be constructed very efficiently for large neuronal populations. It is far from obvious that these models can capture any of the essential collective behavior in real networks. To answer this question, we study a population of N\u223c1500similar-to\ud835\udc411500N\\sim 1500italic_N \u223c 1500 neurons in the mouse hippocampus Gauthier and Tank (2018), identifying the maximally informative tree of pairwise correlations (Figs. 5 and 6). Despite containing only one correlation per neuron, this minimax entropy model accounts for 14%percent1414\\%14 % of the independent entropy (over 50505050 times more than random trees) and predicts the distribution of large\u2013scale synchrony in activity (Fig. 4). Moreover, the model becomes more effective as the population grows (Fig. 7) and exhibits hints of critical behavior (Fig. 8). The success of such a sparse model hinges on the fact that the distribution of mutual information between neurons is heavy\u2013tailed [Fig. 2(b)], such that a few rare correlations carry much more information than average. In fact, the physical connections between neurons are now understood to be heavy\u2013tailed across a range of animals Lynn et al. (2022), suggesting that our approach my prove effective in other neural systems. While these minimax entropy models cannot capture all of a system\u2019s collective properties, they provide at least a starting point for simplified descriptions of the much larger systems becoming accessible in modern experiments. To establish notation, we begin by reviewing well known ideas about statistical mechanics for models without loops. We then proceed, here and in subsequent Appendices, to technical points needed for the main text. Consider a system of N\ud835\udc41Nitalic_N binary variables xi\u2208{0,1}subscript\ud835\udc65\ud835\udc5601x_{i}\\in\\{0,1\\}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 { 0 , 1 }, i=1,2,\u2026,N\ud835\udc5612\u2026\ud835\udc41i=1,2,\\ldots,Nitalic_i = 1 , 2 , \u2026 , italic_N, defined by fields hisubscript\u210e\ud835\udc56h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and interactions Jijsubscript\ud835\udc3d\ud835\udc56\ud835\udc57J_{ij}italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT that lie on a tree \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T. The Boltzmann distribution [Eq. (5)] takes the form where F=0\ud835\udc390F=0italic_F = 0 is the zero\u2013point energy, which will become useful. To begin, we seek to compute the partition function, To do so, imagine summing over one variable, and finding a new system of N\u22121\ud835\udc411N-1italic_N - 1 variables with the same partition function Z\ud835\udc4dZitalic_Z. If we can repeat this process until no variables remain, then computing Z\ud835\udc4dZitalic_Z will be trivial. We label the nodes i\ud835\udc56iitalic_i based on the order that they are removed, and we let hi(i)subscriptsuperscript\u210e\ud835\udc56\ud835\udc56h^{(i)}_{i}italic_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and F(i)superscript\ud835\udc39\ud835\udc56F^{(i)}italic_F start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT denote the updated parameters at step i\ud835\udc56iitalic_i, while the interactions Jijsubscript\ud835\udc3d\ud835\udc56\ud835\udc57J_{ij}italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT stay fixed. Consider summing over a variable i\ud835\udc56iitalic_i with only one connection in the network, say to variable j\ud835\udc57jitalic_j. We note that such a node is always guaranteed to exist in a tree. To keep the partition function fixed, the new system with i\ud835\udc56iitalic_i removed must satisfy the equations This is a system of two equations (one for each value of xjsubscript\ud835\udc65\ud835\udc57x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT), which we can solve for the new parameters After removing i\ud835\udc56iitalic_i, the new system still forms a tree, so we can repeat the above procedure. When all nodes have been removed, we are left with a single parameter \u2131=F(N+1)\u2131superscript\ud835\udc39\ud835\udc411\\mathcal{F}=F^{(N+1)}caligraphic_F = italic_F start_POSTSUPERSCRIPT ( italic_N + 1 ) end_POSTSUPERSCRIPT, which is the free energy of the system, and the partition function is given by To compute population statistics, one simply needs to take derivatives of the partition function, where ddhi\ud835\udc51\ud835\udc51subscript\u210e\ud835\udc56\\frac{d}{dh_{i}}divide start_ARG italic_d end_ARG start_ARG italic_d italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG and ddJij\ud835\udc51\ud835\udc51subscript\ud835\udc3d\ud835\udc56\ud835\udc57\\frac{d}{dJ_{ij}}divide start_ARG italic_d end_ARG start_ARG italic_d italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG represent total derivatives, which account for indirect dependencies via Eqs. (16) and (17). Since d\u2131dF(i+1)=1\ud835\udc51\u2131\ud835\udc51superscript\ud835\udc39\ud835\udc5611\\frac{d\\mathcal{F}}{dF^{(i+1)}}=1divide start_ARG italic_d caligraphic_F end_ARG start_ARG italic_d italic_F start_POSTSUPERSCRIPT ( italic_i + 1 ) end_POSTSUPERSCRIPT end_ARG = 1 and dhi(i)dhi=1\ud835\udc51superscriptsubscript\u210e\ud835\udc56\ud835\udc56\ud835\udc51subscript\u210e\ud835\udc561\\frac{dh_{i}^{(i)}}{dh_{i}}=1divide start_ARG italic_d italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_ARG start_ARG italic_d italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG = 1, the above procedure yields Noticing that and taking derivatives of Eqs. (16) and (17), we have The correlation follows analogously, Thus, by proceeding in the opposite order from which the nodes were removed, we can compute the average activities \u27e8xi\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56\\langle x_{i}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9 and correlations \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\langle x_{i}x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 for (ij)\u2208\ud835\udcaf\ud835\udc56\ud835\udc57\ud835\udcaf(ij)\\in\\mathcal{T}( italic_i italic_j ) \u2208 caligraphic_T. For the correlations \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\langle x_{i}x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 off the tree (that is, for (ij)\u2209\ud835\udcaf\ud835\udc56\ud835\udc57\ud835\udcaf(ij)\\not\\in\\mathcal{T}( italic_i italic_j ) \u2209 caligraphic_T), see Appendix E. We now solve the inverse problem for the parameters hisubscript\u210e\ud835\udc56h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and Jijsubscript\ud835\udc3d\ud835\udc56\ud835\udc57J_{ij}italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT given the observations \u27e8xi\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56\\langle x_{i}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9 and \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\langle x_{i}x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 on a tree \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T. Inverting Eqs. (23) and (24) yields Combining the above equations, we can solve for the interaction Jijsubscript\ud835\udc3d\ud835\udc56\ud835\udc57J_{ij}italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT in Eq. (6). To compute the local field hisubscript\u210e\ud835\udc56h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we note that we can repeat the procedure in Appendix A ending at any node; this is equivalent to choosing the root of the tree. If we choose i\ud835\udc56iitalic_i to be the final node, then we have Additionally, for each neighbor j\u2208\ud835\udca9i\ud835\udc57subscript\ud835\udca9\ud835\udc56j\\in\\mathcal{N}_{i}italic_j \u2208 caligraphic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, Eq. (16) tells us that we receive a contribution to hi(N)superscriptsubscript\u210e\ud835\udc56\ud835\udc41h_{i}^{(N)}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_N ) end_POSTSUPERSCRIPT of the form Combining these contributions yields We have thus arrived at an analytic solution to the maximum entropy problem on a tree. Our ability to efficiently construct the optimal tree \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T depends critically on the decomposition of the information I\ud835\udcafsubscript\ud835\udc3c\ud835\udcafI_{\\mathcal{T}}italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT into the sum of mutual information Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT over pairs (ij)\u2208\ud835\udcaf\ud835\udc56\ud835\udc57\ud835\udcaf(ij)\\in\\mathcal{T}( italic_i italic_j ) \u2208 caligraphic_T [Eq. (8)]. To derive this result, we note that for each connection (ij)\u2208\ud835\udcaf\ud835\udc56\ud835\udc57\ud835\udcaf(ij)\\in\\mathcal{T}( italic_i italic_j ) \u2208 caligraphic_T, the observables \u27e8xi\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56\\langle x_{i}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9, \u27e8xj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc57\\langle x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9, and \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\langle x_{i}x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 fully define the marginal distribution Pij(xi,xj)subscript\ud835\udc43\ud835\udc56\ud835\udc57subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57P_{ij}(x_{i},x_{j})italic_P start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ). Now consider a new tree \ud835\udcaf\u2032=\ud835\udcaf/(ij)superscript\ud835\udcaf\u2032\ud835\udcaf\ud835\udc56\ud835\udc57\\mathcal{T^{\\prime}}=\\mathcal{T}/(ij)caligraphic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = caligraphic_T / ( italic_i italic_j ) with the connection (ij)\ud835\udc56\ud835\udc57(ij)( italic_i italic_j ) removed, such that we do not observe \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\langle x_{i}x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9. Since \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T has no loops, after removing (ij)\ud835\udc56\ud835\udc57(ij)( italic_i italic_j ) the two elements i\ud835\udc56iitalic_i and j\ud835\udc57jitalic_j become independent. Meanwhile, the dependence of the rest of the system on i\ud835\udc56iitalic_i and j\ud835\udc57jitalic_j remains fixed. Thus, observing the correlation \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\langle x_{i}x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 leads to a drop in entropy where Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT is the observed mutual information between i\ud835\udc56iitalic_i and j\ud835\udc57jitalic_j. Repeating the above argument for every correlation in \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T, we arrive at Eq. (8). In order to estimate the mutual information between neurons Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT, one must correct for finite\u2013data effects Strong et al. (1998). To do so, we subsample the data hierarchically for different data fractions {1,0.9,\u2026,0.2,0.1}10.9\u20260.20.1\\{1,0.9,\\ldots,0.2,0.1\\}{ 1 , 0.9 , \u2026 , 0.2 , 0.1 }, such that each subsample is contained within the larger subsamples. Additionally, to preserve the dependencies between consecutive data points \ud835\udc99(m)superscript\ud835\udc99\ud835\udc5a\\bm{x}^{(m)}bold_italic_x start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT and \ud835\udc99(m+1)superscript\ud835\udc99\ud835\udc5a1\\bm{x}^{(m+1)}bold_italic_x start_POSTSUPERSCRIPT ( italic_m + 1 ) end_POSTSUPERSCRIPT, we sample temporally contiguous fractions of the data. To ensure that each point is sampled with equal probability, we allow subsamples that span the beginning and end of the recording. For each subsample, we estimate the mutual information between neurons i\ud835\udc56iitalic_i and j\ud835\udc57jitalic_j using the equation where The pseudo\u2013counts in Eq. (33) ensure that the mutual information estimates do not diverge. After estimating Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT for each data fraction, following Ref. Strong et al. (1998) we extrapolate to the infinite\u2013data limit using a linear fit with respect to the inverse data fraction. Repeating this process 100 times, we arrive at a distribution of infinite\u2013data estimates for Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT, from which we can compute a mean and standard deviation (Fig. 9). To check the above procedure, we note that shuffling the activity of each neuron in time should destroy the mutual information Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT. Indeed, for time\u2013shuffled data, we do not observe a single significant mutual information in the population. Given a maximum entropy model with parameters hisubscript\u210e\ud835\udc56h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and Jijsubscript\ud835\udc3d\ud835\udc56\ud835\udc57J_{ij}italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT on a tree \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T, in Appendix A we showed how to compute the averages \u27e8xi\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56\\langle x_{i}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9 and correlations \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\langle x_{i}x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 on the tree. Specifically, we computed the partition function Z\ud835\udc4dZitalic_Z by summing over variables xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in the order i=1,2,\u22ef,N\ud835\udc5612\u22ef\ud835\udc41i=1,2,\\cdots,Nitalic_i = 1 , 2 , \u22ef , italic_N, and then computed statistics in the reverse order. Here, we show how to compute the correlations \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\langle x_{i}x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 not on the tree; that is, for (ij)\u2209\ud835\udcaf\ud835\udc56\ud835\udc57\ud835\udcaf(ij)\\not\\in\\mathcal{T}( italic_i italic_j ) \u2209 caligraphic_T. To begin, we assume that we have computed the correlations \u27e8xjxk\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc57subscript\ud835\udc65\ud835\udc58\\langle x_{j}x_{k}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u27e9 for all nodes k>i>j\ud835\udc58\ud835\udc56\ud835\udc57k>i>jitalic_k > italic_i > italic_j. Then, if we compute \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\langle x_{i}x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9, the procedure will follow by induction. From the Boltzmann distribution in Eq. (5), we have We already know how to compute the averages \u27e8xi\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56\\langle x_{i}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u27e9 and \u27e8xj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc57\\langle x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9, so all that remains is to calculate the above derivative. Let p(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i ) denote the parent of i\ud835\udc56iitalic_i (that is, the final neighbor when i\ud835\udc56iitalic_i is removed) and likewise for p(j)\ud835\udc5d\ud835\udc57p(j)italic_p ( italic_j ). Differentiating Eq. (23) with respect to hjsubscript\u210e\ud835\udc57h_{j}italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, we have We note that which we have already computed by assumption, since p(i)>i\ud835\udc5d\ud835\udc56\ud835\udc56p(i)>iitalic_p ( italic_i ) > italic_i. From Eq. (23) we have, and Finally, we note that the dependence of hi(i)superscriptsubscript\u210e\ud835\udc56\ud835\udc56h_{i}^{(i)}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT on hjsubscript\u210e\ud835\udc57h_{j}italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT runs only through hp(j)(j+1)subscriptsuperscript\u210e\ud835\udc571\ud835\udc5d\ud835\udc57h^{(j+1)}_{p(j)}italic_h start_POSTSUPERSCRIPT ( italic_j + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p ( italic_j ) end_POSTSUBSCRIPT, such that Since p(j)>j\ud835\udc5d\ud835\udc57\ud835\udc57p(j)>jitalic_p ( italic_j ) > italic_j, we can assume that we have already computed dhi(i)dhp(j)(j+1)\ud835\udc51superscriptsubscript\u210e\ud835\udc56\ud835\udc56\ud835\udc51subscriptsuperscript\u210e\ud835\udc571\ud835\udc5d\ud835\udc57\\frac{dh_{i}^{(i)}}{dh^{(j+1)}_{p(j)}}divide start_ARG italic_d italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_ARG start_ARG italic_d italic_h start_POSTSUPERSCRIPT ( italic_j + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p ( italic_j ) end_POSTSUBSCRIPT end_ARG. Finally, Eq. (16) yields Plugging everything into Eq. (36), and inducting on i>j\ud835\udc56\ud835\udc57i>jitalic_i > italic_j, one can compute the correlations \u27e8xixj\u27e9delimited-\u27e8\u27e9subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\langle x_{i}x_{j}\\rangle\u27e8 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u27e9 between all variables. In \u00a7IV, we studied the predictions of the minimax entropy model P\ud835\udcafsubscript\ud835\udc43\ud835\udcafP_{\\mathcal{T}}italic_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT corresponding to the optimal tree \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T. For comparison, here we consider the predictions of the minimum distance and random trees. While the optimal tree captures I\ud835\udcaf=26.2bitssubscript\ud835\udc3c\ud835\udcaf26.2bitsI_{\\mathcal{T}}=26.2\\,\\text{bits}italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT = 26.2 bits of information (I\ud835\udcaf/Sind=14.4%subscript\ud835\udc3c\ud835\udcafsubscript\ud835\udc46indpercent14.4I_{\\mathcal{T}}/S_{\\text{ind}}=14.4\\%italic_I start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT / italic_S start_POSTSUBSCRIPT ind end_POSTSUBSCRIPT = 14.4 % of the independent entropy), the minimum distance tree only captures 11.9bits11.9bits11.9\\,\\text{bits}11.9 bits of information (6.5%percent6.56.5\\%6.5 % of the independent entropy), and a typical random tree only captures (N\u22121)I\u00af=0.4bits\ud835\udc411\u00af\ud835\udc3c0.4bits(N-1)\\bar{I}=0.4\\,\\text{bits}( italic_N - 1 ) over\u00af start_ARG italic_I end_ARG = 0.4 bits (0.2%percent0.20.2\\%0.2 % of the independent entropy). For each tree, we can predict the mutual information Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT and correlation coefficients between all pairs of neurons using the procedure in Appendix E. Since the minimum distance tree includes some of the largest mutual information in the population, it is able to match the distribution of Iijsubscript\ud835\udc3c\ud835\udc56\ud835\udc57I_{ij}italic_I start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT (within errors) out to N\u223c100similar-to\ud835\udc41100N\\sim 100italic_N \u223c 100 neurons [Fig. 10(a)]. However, the minimum distance tree fails to predict the observed correlations across most of the dynamic range of the data [Fig. 10(b)]. Meanwhile, random trees typically include only weak mutual information [Fig. 10(a)], such that their predictions are nearly indistinguishable from a population of independent neurons [Figs. 10(b)]. For the minimax entropy model, a backbone of strong positive interactions combine to produce accurate predictions for the distribution P(K)\ud835\udc43\ud835\udc3eP(K)italic_P ( italic_K ) of population\u2013wide synchrony K\ud835\udc3eKitalic_K (Fig. 4). By contrast, random trees predict a Gaussian distribution consistent with independent neurons, and the minimum distance tree only produces a slightly broader distribution (Fig. 11). In both models, large\u2013scale synchrony in activity (K\u227350greater-than-or-equivalent-to\ud835\udc3e50K\\gtrsim 50italic_K \u2273 50) and silence (K\u227210less-than-or-similar-to\ud835\udc3e10K\\lesssim 10italic_K \u2272 10) occurs significantly less frequently than observed the data. Consider a system with fields hisubscript\u210e\ud835\udc56h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, interactions Jijsubscript\ud835\udc3d\ud835\udc56\ud835\udc57J_{ij}italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT that lie on a tree \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T, and temperature T\ud835\udc47Titalic_T. The Boltzmann distribution P(\ud835\udc99)\ud835\udc43\ud835\udc99P(\\bm{x})italic_P ( bold_italic_x ) takes the form in Eq. (9) with energy E(\ud835\udc99)\ud835\udc38\ud835\udc99E(\\bm{x})italic_E ( bold_italic_x ) defined in Eq. (10). Here, we denote averages over P(\ud835\udc99)\ud835\udc43\ud835\udc99P(\\bm{x})italic_P ( bold_italic_x ) by \u27e8\u22c5\u27e9delimited-\u27e8\u27e9\u22c5\\langle\\cdot\\rangle\u27e8 \u22c5 \u27e9, while dropping the subscript T\ud835\udc47Titalic_T. The susceptibility \u03c7\ud835\udf12\\chiitalic_\u03c7 [Eq. (11)], can be computed using the results of Appendix E. To compute the heat capacity C\ud835\udc36Citalic_C [Eq. (12)], we begin with the average energy To calculate the free energy \u2131\u2131\\mathcal{F}caligraphic_F, we proceed as in Appendix A. After including the temperature T\ud835\udc47Titalic_T, Eqs. (16) and (17) take the form Iteratively summing over each variable, we arrive at the free energy \u2131=F(N+1)\u2131superscript\ud835\udc39\ud835\udc411\\mathcal{F}=F^{(N+1)}caligraphic_F = italic_F start_POSTSUPERSCRIPT ( italic_N + 1 ) end_POSTSUPERSCRIPT. To compute d\u2131dT\ud835\udc51\u2131\ud835\udc51\ud835\udc47\\frac{d\\mathcal{F}}{dT}divide start_ARG italic_d caligraphic_F end_ARG start_ARG italic_d italic_T end_ARG, we take derivatives of Eqs. (45) and (46), yielding Iterating the above equations, we arrive at the derivative d\u2131dT=dF(N+1)dT\ud835\udc51\u2131\ud835\udc51\ud835\udc47\ud835\udc51superscript\ud835\udc39\ud835\udc411\ud835\udc51\ud835\udc47\\frac{d\\mathcal{F}}{dT}=\\frac{dF^{(N+1)}}{dT}divide start_ARG italic_d caligraphic_F end_ARG start_ARG italic_d italic_T end_ARG = divide start_ARG italic_d italic_F start_POSTSUPERSCRIPT ( italic_N + 1 ) end_POSTSUPERSCRIPT end_ARG start_ARG italic_d italic_T end_ARG, which completes our calculation of the average energy [Eq. (42)]. The heat capacity is given by Taking derivatives of Eqs. (45) and (46), we have Finally, after computing d2\u2131dT2=d2F(N+1)dT2superscript\ud835\udc512\u2131\ud835\udc51superscript\ud835\udc472superscript\ud835\udc512superscript\ud835\udc39\ud835\udc411\ud835\udc51superscript\ud835\udc472\\frac{d^{2}\\mathcal{F}}{dT^{2}}=\\frac{d^{2}F^{(N+1)}}{dT^{2}}divide start_ARG italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_F end_ARG start_ARG italic_d italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG = divide start_ARG italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_F start_POSTSUPERSCRIPT ( italic_N + 1 ) end_POSTSUPERSCRIPT end_ARG start_ARG italic_d italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG iteratively, we have arrived at the heat capacity C\ud835\udc36Citalic_C."
    },
    {
        "id": 13,
        "title": "Markovian embedding of nonlocal equations using spectral representation",
        "corpus": "Nonlocal evolutionary equations containing memory terms model a variety of non-Markovian processes. We present a Markovian embedding procedure for a class of nonlocal equations by utilising the spectral representation of the nonlinear memory kernel. This allows us to transform the nonlocal system to a local-in-time system in an abstract extended space. We demonstrate our embedding procedure and its efficacy for two different physical models, namely the (i)\ud835\udc56(i)( italic_i ) 1111D walking droplet and the (ii)\ud835\udc56\ud835\udc56(ii)( italic_i italic_i ) 1111D single-phase Stefan problem. [inst1]organization=International Centre for Theoretical Sciences, Tata Institute of Fundamental Research, city=Bengaluru, postcode=560089, country=India \\affiliation[inst2]organization=School of Computer and Mathematical Sciences, University of Adelaide, postcode=5005, country=Australia Several evolutionary processes with memory effects are modelled by nonlocal equations. Examples include particle motion in unsteady hydrodynamic environments (Lovalenti and Brady, 1993; Oza et al., 2013; Peng and Schnitzer, 2023), and boundary evolution in diffusion processes in time-dependent domains (Stefan, 1891; Fokas and Pelloni, 2012) and under nonlinear boundary forcing (Mann and Wolf, 1951; Keller and Olmstead, 1972; Olmstead and Handelsman, 1976). In this manuscript, we are concerned with nonlocal models where the evolution equation for a state variable y(t)\ud835\udc66\ud835\udc61y(t)italic_y ( italic_t ) has the following canonical structure: where the superscript n\ud835\udc5bnitalic_n indicates the order of time derivative. The function L(\u22c5)\ud835\udc3f\u22c5L(\\cdot)italic_L ( \u22c5 ) is a local-in-time term driving the evolution, whereas the function N(\u22c5)\ud835\udc41\u22c5N(\\cdot)italic_N ( \u22c5 ) is a memory kernel of the nonlocal integral, which is also a nonlinear function of y(t)\ud835\udc66\ud835\udc61y(t)italic_y ( italic_t ). Without the nonlocal term, Eq. (1) can be readily transformed into a system of first order ordinary differential equations (ODEs) yielding a Markovian description. However, with the nonlocal term, such a transformation is not trivial. Different Markovian embeddings in an abstract extended space are commonly realised by introducing an auxiliary variable that accounts for the memory (Dafermos, 1970). In this study, we show that a Markovian prescription can be realised for nonlocal equations of the form in Eq. (1) by an embedding procedure that relies on the spectral representation of the nonlinear memory kernel (previously discussed for linear memory kernel in (Jaganathan et al., 2023)). Our approach involves expressing the nonlocal integral term in Eq. (1) as a local-in-time term. We assume that the nonlinear memory kernel has a spectral representation of the following form: where \u0393\u0393\\Gammaroman_\u0393 is a smooth contour in the complex-plane and \u03d5,\u03c8italic-\u03d5\ud835\udf13\\phi,\\psiitalic_\u03d5 , italic_\u03c8 are complex-analytic functions of the variable k\ud835\udc58kitalic_k. The spectral representation allows embedding of Eq. (1) into an extended space. This is done by substituting the spectral representation in the memory term, followed by an interchange of order of integrals, to give: where H(k,t)\ud835\udc3b\ud835\udc58\ud835\udc61H(k,t)italic_H ( italic_k , italic_t ) is the newly-introduced complex-valued auxiliary variable. Since it encapsulates \u201cmemory\u201d, we refer to it as the history function. Owing to the particular spectral form, we can infer that the history function has a Markovian evolution given by an ODE, parameterised by the spectral variable k\ud835\udc58kitalic_k. Therefore, we have the following local-in-time reformulation of Eq. (1) in an infinite-dimensional space: where overdot denotes time derivative. We demonstrate our embedding procedure for two physical models with nonlinear memory effects, namely the one-dimensional walking droplet and the single-phase one-dimensional Stefan problem. Both models illustrate that a Markovian embedding into an infinite-dimensional space can be constructed subject to a natural spectral representation of the nonlinear memory kernel (Eq. (2)). These models differ in the complexity of the auxiliary history variable introduced upon embedding, highlighting the versatility of the approach. A hydrodynamic active system described by non-Markovian dynamics is that of walking (Couder et al., 2005) and superwalking (Valani et al., 2019) droplets. By vertically vibrating an oil bath, a drop of the same oil can be made to bounce and walk on the liquid surface. Each bounce of the droplet locally excites a damped standing wave. The droplet interacts obliquely with these self-excited waves on subsequent bounces to propel itself horizontally, giving rise to a self-propelled, classical wave-particle entity (WPE). At large vibration amplitudes, the droplet-generated waves decay slowly in time. Hence, the motion of the droplet is affected by the history of waves along its trajectory. This gives rise to path memory in the system and makes the dynamics non-Markovian. Oza et al. (Oza et al., 2013) developed a theoretical stroboscopic model to describe the horizontal walking motion of such a WPE. The model averages over the fast vertical periodic bouncing of the droplet and provides a trajectory equation for the slow walking dynamics in the horizontal plane. We consider a reduction of this model to one horizontal dimension, x\u2208\u211d\ud835\udc65\u211dx\\in\\mathbb{R}italic_x \u2208 blackboard_R (see Fig. 1a). Consider a droplet with position and velocity given by (xd(t),x\u02d9d(t))\u2208\u211d2subscript\ud835\udc65\ud835\udc51\ud835\udc61subscript\u02d9\ud835\udc65\ud835\udc51\ud835\udc61superscript\u211d2(x_{d}(t),\\dot{x}_{d}(t))\\in\\mathbb{R}^{2}( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_t ) , over\u02d9 start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_t ) ) \u2208 blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, which continuously generates standing waves with prescribed spatial structure W(x)\ud835\udc4a\ud835\udc65W(x)italic_W ( italic_x ) that decay with time. The dynamics of a 1111D WPE follows the non-dimensional integro-differential equation: where C1subscript\ud835\udc361C_{1}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and C2subscript\ud835\udc362C_{2}italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are non-negative constants representing dimensionless wave-amplitude and inverse memory parameter, respectively111Note that C1subscript\ud835\udc361C_{1}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and C2subscript\ud835\udc362C_{2}italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are related to the dimensionless parameters \u03ba\ud835\udf05\\kappaitalic_\u03ba and \u03b2\ud835\udefd\\betaitalic_\u03b2 in Oza et al. (Oza et al., 2013) by C1=\u03b2\u03ba2subscript\ud835\udc361\ud835\udefdsuperscript\ud835\udf052C_{1}=\\beta\\kappa^{2}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_\u03b2 italic_\u03ba start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and C2=\u03basubscript\ud835\udc362\ud835\udf05C_{2}=\\kappaitalic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_\u03ba.. We refer the reader to Ref. (Oza et al., 2013) for details and explicit expressions for these parameters. Eq. (4) is a horizontal force balance of the WPE with the right-hand side containing an effective drag term proportional to velocity \u2212x\u02d9d(t)subscript\u02d9\ud835\udc65\ud835\udc51\ud835\udc61-\\dot{x}_{d}(t)- over\u02d9 start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_t ) and the nonlocal memory term capturing the cumulative force on the particle from the superposition of the self-generated waves along its path. The memory kernel comprises the functions W\u2032(\u22c5)superscript\ud835\udc4a\u2032\u22c5W^{\\prime}(\\cdot)italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( \u22c5 ) and K(\u22c5)\ud835\udc3e\u22c5K(\\cdot)italic_K ( \u22c5 ); the former represents the wave-gradient where the prime denotes derivative with respect to its argument, and K(\u22c5)\ud835\udc3e\u22c5K(\\cdot)italic_K ( \u22c5 ) imposes the temporal decay. In the stroboscopic model of a walking droplet, \u2212W\u2032(x)=J1(x)superscript\ud835\udc4a\u2032\ud835\udc65subscript\ud835\udc3d1\ud835\udc65-W^{\\prime}(x)=J_{1}(x)- italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_x ) = italic_J start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x ), where J1subscript\ud835\udc3d1J_{1}italic_J start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is the Bessel-J\ud835\udc3dJitalic_J function of order one and K(t)=e\u2212C2t\ud835\udc3e\ud835\udc61superscript\ud835\udc52subscript\ud835\udc362\ud835\udc61K(t)=e^{-C_{2}t}italic_K ( italic_t ) = italic_e start_POSTSUPERSCRIPT - italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_t end_POSTSUPERSCRIPT. In the high-memory regime (C2\u226a1much-less-thansubscript\ud835\udc3621C_{2}\\ll 1italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u226a 1), WPEs exhibit hydrodynamic quantum analogs (Bush and Oza, 2020). However, the regime may become experimentally difficult to access (Bacot et al., 2019) due to the increased susceptibility of the system to the Faraday instability (Faraday, 1831). Numerical simulations provide an alternative with controllability but also entail dealing with the non-Markovian structure of Eq. (4) and time-dependent computational costs therein. We convert Eq.(4) to a Markovian description in the following way. We recall the following integral representation of the Bessel-J1subscript\ud835\udc3d1J_{1}italic_J start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT function for some z\u2208\u211d\ud835\udc67\u211dz\\in\\mathbb{R}italic_z \u2208 blackboard_R: Substituting the above in the memory term of Eq. (4), followed by a switch in the order of integrals, we construct the equivalent local-in-time representation for the memory integral, where the weight function w(k)=1/1\u2212k2\ud835\udc64\ud835\udc5811superscript\ud835\udc582w(k)=1/\\sqrt{1-k^{2}}italic_w ( italic_k ) = 1 / square-root start_ARG 1 - italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG and H(k,t)\ud835\udc3b\ud835\udc58\ud835\udc61H(k,t)italic_H ( italic_k , italic_t ) is a complex-valued function of time t\ud835\udc61titalic_t and a real number k\ud835\udc58kitalic_k with a finite support in [\u22121,1]11[-1,1][ - 1 , 1 ]. The induced definition of H(k,t)\ud835\udc3b\ud835\udc58\ud835\udc61H(k,t)italic_H ( italic_k , italic_t ) is The form of H(\u22c5)\ud835\udc3b\u22c5H(\\cdot)italic_H ( \u22c5 ) in (5) suggests that it has a Markovian evolution according to an ODE parameterised by the spectral variable k\ud835\udc58kitalic_k. Consequently, combined with the definition of the droplet\u2019s velocity x\u02d9d=vdsubscript\u02d9\ud835\udc65\ud835\udc51subscript\ud835\udc63\ud835\udc51\\dot{x}_{d}=v_{d}over\u02d9 start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = italic_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, we derive the following Markovian prescription for the WPE dynamics in the extended state space for t>0\ud835\udc610t>0italic_t > 0: subject to initial conditions (xd0,vd0)subscript\ud835\udc65\ud835\udc510subscript\ud835\udc63\ud835\udc510(x_{d0},v_{d0})( italic_x start_POSTSUBSCRIPT italic_d 0 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_d 0 end_POSTSUBSCRIPT ) and H(k,0)=0\ud835\udc3b\ud835\udc5800H(k,0)=0italic_H ( italic_k , 0 ) = 0. We note in Eq. (5) that H(\u22c5)\ud835\udc3b\u22c5H(\\cdot)italic_H ( \u22c5 ) preserves certain symmetries with respect to the spectral variable k\ud835\udc58kitalic_k at all times: Re(H)Re\ud835\udc3b\\text{Re}(H)Re ( italic_H ) has an even-symmetry whereas Im(H)Im\ud835\udc3b\\text{Im}(H)Im ( italic_H ) is odd-symmetric. Therefore, whereas the real and imaginary parts of the history function drive each other\u2019s dynamics, only the real part contributes to the memory integral in Eq. (6a). The resultant set of local differential equations (2.1.1) can be readily solved using any standard time-integrator; we use the second-order Runge-Kutta scheme. An additional task involves computing the history integral over k\ud835\udc58kitalic_k. The integrand, with its finite support in [\u22121,1]11[-1,1][ - 1 , 1 ] and the form of weight function w\ud835\udc64witalic_w, naturally suggests expansion of H(k,t)\ud835\udc3b\ud835\udc58\ud835\udc61H(k,t)italic_H ( italic_k , italic_t ) in the bases of Chebyshev polynomials of the first kind. Therefore, we use the spectrally-accurate Clenshaw-Curtis quadrature method to approximate the integral: where kn=cos(n\u03c0/M)subscript\ud835\udc58\ud835\udc5b\ud835\udc5b\ud835\udf0b\ud835\udc40k_{n}=\\cos(n\\pi/M)italic_k start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = roman_cos ( italic_n italic_\u03c0 / italic_M ) are the Chebyshev nodes and \u03c9nsubscript\ud835\udf14\ud835\udc5b\\omega_{n}italic_\u03c9 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT are the associated weights. We numerically solve Eq. (2.1.1) for a few representative parameter sets (C1,C2)subscript\ud835\udc361subscript\ud835\udc362(C_{1},C_{2})( italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ). Fig. 1 shows that the embedded system of equations (2.1.1) successfully reproduces the previously known non-walking and walking regimes in the parameter space (Durey et al., 2020; Valani et al., 2021). For a steady walker, an analytical expression for its steady speed (Oza et al., 2013) is : The numerical solution for the steady walker attains the above analytical steady walking speed (dashed line) in Fig. 1c. Additionally, in Fig. 2, we show the evolution of the history function in the k\u2212limit-from\ud835\udc58k-italic_k -domain over time. There have been previous works (Mol\u00e1\u010dek, 2013; Durey et al., 2020; Durey, 2020; Valani, 2022) that rewrite the integro-differential equation for the walker into a system of ODEs. However, these transformations work for only specific choices of the wave form W(x)\ud835\udc4a\ud835\udc65W(x)italic_W ( italic_x ). The Markovian embedding formalism is applicable for a broader class of wave forms that have a suitable spectral representation. This is particularly useful in generalised pilot-wave framework, where new hydrodynamic quantum analogues are being explored by investigating various wave forms (Bush and Oza, 2020). We now consider the class of free boundary problems called the Stefan problem, which primarily describes phase-change processes such as the melting of a solid (Stefan, 1891; Guenther and Lee, 2012). In its simplest non-dimensional formulation, it comprises a one-dimensional domain in \u211d+superscript\u211d\\mathbb{R}^{+}blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT, contiguously supporting a molten phase and a solid phase, separated at their interface, which is free to move as the solid melts (see Fig. 3a). The solid phase is modelled as an infinite heat sink maintained at the melting temperature at all times. Therefore, the simplified problem involves finding the solution pair (\u03b8(x,t),l(t))\ud835\udf03\ud835\udc65\ud835\udc61\ud835\udc59\ud835\udc61(\\theta(x,t),l(t))( italic_\u03b8 ( italic_x , italic_t ) , italic_l ( italic_t ) ), where \u03b8(x,t)\ud835\udf03\ud835\udc65\ud835\udc61\\theta(x,t)italic_\u03b8 ( italic_x , italic_t ) describes the instantaneous temperature distribution in the molten phase and l(t)\ud835\udc59\ud835\udc61l(t)italic_l ( italic_t ) is the location of the melting front. The function \u03b8\ud835\udf03\\thetaitalic_\u03b8 satisfies the diffusion equation \u2202t\u03b8\u2212\u2202x2\u03b8=0subscript\ud835\udc61\ud835\udf03superscriptsubscript\ud835\udc652\ud835\udf030\\partial_{t}\\theta-\\partial_{x}^{2}\\theta=0\u2202 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_\u03b8 - \u2202 start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_\u03b8 = 0 in x\u2208[0,l(t)]\ud835\udc650\ud835\udc59\ud835\udc61x\\in[0,l(t)]italic_x \u2208 [ 0 , italic_l ( italic_t ) ], subject to an arbitrary initial condition \u03b80(x)subscript\ud835\udf030\ud835\udc65\\theta_{0}(x)italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_x ) in the initial domain x\u2208[0,l0]\ud835\udc650subscript\ud835\udc590x\\in[0,l_{0}]italic_x \u2208 [ 0 , italic_l start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] at t=t0\ud835\udc61subscript\ud835\udc610t=t_{0}italic_t = italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and a temperature or heat flux condition at the fixed boundary x=0\ud835\udc650x=0italic_x = 0. The moving front, which is at the melting temperature, is governed by the Stefan condition l\u02d9=\u2212\u2202x\u03b8(l(t),t)\u02d9\ud835\udc59subscript\ud835\udc65\ud835\udf03\ud835\udc59\ud835\udc61\ud835\udc61\\dot{l}=-\\partial_{x}\\theta(l(t),t)over\u02d9 start_ARG italic_l end_ARG = - \u2202 start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_\u03b8 ( italic_l ( italic_t ) , italic_t ). We consider the case where temperature is prescribed at the fixed boundary, \u03b8(0,t)=f(t)\ud835\udf030\ud835\udc61\ud835\udc53\ud835\udc61\\theta(0,t)=f(t)italic_\u03b8 ( 0 , italic_t ) = italic_f ( italic_t ) for exposition. With primary interest in the interface\u2019s location, the bulk heat diffusion process in the molten phase may be effectively \u201cintegrated out\u201d to derive a non-Markovian equation of motion for the moving front. The resulting velocity equation for the moving front, v(t)=l\u02d9(t)\ud835\udc63\ud835\udc61\u02d9\ud835\udc59\ud835\udc61v(t)=\\dot{l}(t)italic_v ( italic_t ) = over\u02d9 start_ARG italic_l end_ARG ( italic_t ), is compactly written in the following nonlinear Volterra integral form for t>t0\ud835\udc61subscript\ud835\udc610t>t_{0}italic_t > italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT (Fokas and Pelloni, 2012; Guenther and Lee, 2012): where \u03b80\u2032,f\u02d9superscriptsubscript\ud835\udf030\u2032\u02d9\ud835\udc53\\theta_{0}^{\\prime},\\dot{f}italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , over\u02d9 start_ARG italic_f end_ARG denote the spatial derivative and temporal derivative of \u03b80,fsubscript\ud835\udf030\ud835\udc53\\theta_{0},fitalic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_f respectively, and the function g\ud835\udc54gitalic_g is: The nonlinear kernel may be decomposed into contributions from forcing at the fixed boundary and the unknown velocity of the solid-liquid interface as follows: with the definitions Note that the function g\ud835\udc54gitalic_g is a local-in-time term. The second term on the right-hand side in Eq. (8) is the memory term, which introduces non-locality and the nonlinear dependence on the moving front l(t)\ud835\udc59\ud835\udc61l(t)italic_l ( italic_t ). As before, we construct an embedding such that the present non-Markovian representation for v(t)\ud835\udc63\ud835\udc61v(t)italic_v ( italic_t ) may be turned Markovian. We claim the following spectral representation of the nonlinear kernel for a real k\ud835\udc58kitalic_k: Substituting the above spectral representations in the memory term, followed by a switch in the order of integrals, we derive the local representation with the introduction of the auxiliary history function H(k,t)\ud835\udc3b\ud835\udc58\ud835\udc61H(k,t)italic_H ( italic_k , italic_t ): The corresponding induced definition of the complex-valued history function is: Differentiating the above with respect to time, one may derive an ODE for the history function and realise the following equivalent Markovian prescription for the moving front for t>t0\ud835\udc61subscript\ud835\udc610t>t_{0}italic_t > italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT: subject to l(t0)=l0,v(t0)=g(0,l0;\u03b80\u2032,l0),H(k,t0)=0formulae-sequence\ud835\udc59subscript\ud835\udc610subscript\ud835\udc590formulae-sequence\ud835\udc63subscript\ud835\udc610\ud835\udc540subscript\ud835\udc590superscriptsubscript\ud835\udf030\u2032subscript\ud835\udc590\ud835\udc3b\ud835\udc58subscript\ud835\udc6100l(t_{0})=l_{0},v(t_{0})=g(0,l_{0};\\theta_{0}^{\\prime},l_{0}),\\;H(k,t_{0})=0italic_l ( italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_l start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_v ( italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_g ( 0 , italic_l start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ; italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_l start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , italic_H ( italic_k , italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = 0. The history function in this case too preserves similar symmetries, suggesting that only its even-symmetric real part contributes to the history integral. We show equivalence of the derived embedded Markovian system to the original non-Markovian system (Eq. (8)) by numerically solving Eq. (2.2.1). We use the second-order Runge-Kutta exponential time-differencing method (Cox and Matthews, 2002) to solve for H(k,t)\ud835\udc3b\ud835\udc58\ud835\udc61H(k,t)italic_H ( italic_k , italic_t ) due to stiffness introduced by the \u2212k2Hsuperscript\ud835\udc582\ud835\udc3b-k^{2}H- italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_H term, and a standard integrator to solve for l(t)\ud835\udc59\ud835\udc61l(t)italic_l ( italic_t ). The latter requires evaluating the history integral whose quadrature approximation, however, demands different treatment from the walker problem on two accounts: H(k,t)\ud835\udc3b\ud835\udc58\ud835\udc61H(k,t)italic_H ( italic_k , italic_t ) has infinite support in the k\u2212limit-from\ud835\udc58k-italic_k -space. While this warrants truncation of the k\u2212limit-from\ud835\udc58k-italic_k -space, its decay behaviour at large k\ud835\udc58kitalic_k constraints the extent of truncation. H(k,t)\ud835\udc3b\ud835\udc58\ud835\udc61H(k,t)italic_H ( italic_k , italic_t ) is highly oscillatory; the frequency of oscillations increases with both k\ud835\udc58kitalic_k and t\ud835\udc61titalic_t, which is ascribed to terms such as eikl(t)superscript\ud835\udc52\ud835\udc56\ud835\udc58\ud835\udc59\ud835\udc61e^{ikl(t)}italic_e start_POSTSUPERSCRIPT italic_i italic_k italic_l ( italic_t ) end_POSTSUPERSCRIPT in Eq. (13b). The dependence on t\ud835\udc61titalic_t through l(t)\ud835\udc59\ud835\udc61l(t)italic_l ( italic_t ) exacerbates the oscillations in domain growth problems such as the one under discussion. Consequently, for accurate quadrature approximation, an increasingly dense set of collocation points in the truncated domain is required. The above points are cautionary observations. While one could potentially address these concerns through computationally efficient methods, such an undertaking exceeds the scope of our present work. Therefore, we adopt a heuristic approach to compute the history integral. This involves truncating the k\u2212limit-from\ud835\udc58k-italic_k -space, mapping it to the interval [\u22121,1]11[-1,1][ - 1 , 1 ], and employing Clenshaw-Curtis quadrature to compute the history integral. We consider the example corresponding to melting due to constant temperature at the fixed end, f(t)=1\ud835\udc53\ud835\udc611f(t)=1italic_f ( italic_t ) = 1, with the following analytical solution pair (Mitchell and Vynnycky, 2009): where the constant \u03b1\ud835\udefc\\alphaitalic_\u03b1 satisfies the transcendental equation: \u03c0\u03b1exp(\u03b12)erf(\u03b1)=1\ud835\udf0b\ud835\udefcsuperscript\ud835\udefc2erf\ud835\udefc1\\sqrt{\\pi}\\alpha\\exp(\\alpha^{2})\\text{erf}(\\alpha)=1square-root start_ARG italic_\u03c0 end_ARG italic_\u03b1 roman_exp ( italic_\u03b1 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) erf ( italic_\u03b1 ) = 1. To avoid the degeneracy at t=0\ud835\udc610t=0italic_t = 0 due to zero-length domain, we let the process evolve for time t0>0subscript\ud835\udc6100t_{0}>0italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT > 0 to a non-zero domain length l0subscript\ud835\udc590l_{0}italic_l start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Prescribing (\u03b8(x,t0),l0)\ud835\udf03\ud835\udc65subscript\ud835\udc610subscript\ud835\udc590(\\theta(x,t_{0}),l_{0})( italic_\u03b8 ( italic_x , italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , italic_l start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) as the initial state, we numerically evolve Eq. (2.2.1) from t0subscript\ud835\udc610t_{0}italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Fig. 3 shows agreement between the numerical and analytical solutions (Eq. (14)) for location of the interface, supplemented with the pointwise error. In Fig. 4, we plot the pertinent history function in the truncated spectral space at different time instances. The highly oscillatory behaviour of H(k,t)\ud835\udc3b\ud835\udc58\ud835\udc61H(k,t)italic_H ( italic_k , italic_t ) in the truncated k\u2212limit-from\ud835\udc58k-italic_k -domain is evident. We have described a Markovian embedding procedure for evolutionary equations with memory effects, which critically relied on the spectral representation of the memory kernel. We have explicitly shown the embedding procedure for two physical models, namely the one-dimensional walking droplet and the single-phase Stefan problem. In both cases, the memory kernel is a nonlinear function of the underlying state variable. Physical processes inherently follow Markovian dynamics when described adequately by all the driving state variables. The non-Markovian description of the evolution of an isolated state variable, such as in Eqs. (4), (8), is often the result of \u201cintegrating out\u201d the effects of \u201cenvironment\u201d comprising other state variables. While identifying these integrated physical variables may not always be feasible, our Markovian embedding procedure provides an alternative mathematical reconstruction of the Markovian dynamics. From a computational standpoint, a Markovian representation ensures that the numerical evolution of the corresponding time-discretized system incurs a time-independent cost. This is in contrast with the standard approaches for memory-dependent systems, where the computational cost grows with time. It is important to recognise that our Markovian embedding procedure comes at the cost of solving an additional local-in-time equation for the history function, which is an infinite-dimensional object. An accurate finite-dimensional approximation of the function depends on the behaviour of H(k,t)\ud835\udc3b\ud835\udc58\ud835\udc61H(k,t)italic_H ( italic_k , italic_t ) in the spectral space. In this regard, the two model problems discussed here demonstrate the extreme scenarios: the Stefan problem required a higher-dimensional approximation of the history function, comprising thousands of spectral variables, while the walker problem allowed a lower-dimensional approximation, with only a few tens of spectral variables for an accurate representation of the history function. DJ acknowledges support of the Department of Atomic Energy, Government of India, under project no. RTI4001. RV was supported by Australian Research Council (ARC) Discovery Project DP200100834 during the course of the work. We thank Vishal Vasan for introducing the Stefan problem to us and for the various discussions pertaining to this work. We also thank Rama Govindarajan for her feedback on the draft."
    },
    {
        "id": 14,
        "title": "On strong p\ud835\udc5dpitalic_p-C\u2062A\u2062P\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups of finite groups and saturated fusion systems\u2020\u2020thanks:  E-mail addresses:\nzsmcau@cau.edu.cn\u2009(S. Zhang); zhencai688@sina.com\u2009(Z. Shen).",
        "corpus": "HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on. Authors: achieve the best HTML results from your LaTeX submissions by following these best practices. A subgroup A\ud835\udc34Aitalic_A of a finite group G\ud835\udc3aGitalic_G is said to be a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, if for any A\u2a7dH\u2a7dG\ud835\udc34\ud835\udc3b\ud835\udc3aA\\leqslant H\\leqslant Gitalic_A \u2a7d italic_H \u2a7d italic_G, A\ud835\udc34Aitalic_A is a p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of H\ud835\udc3bHitalic_H. A subgroup A\ud835\udc34Aitalic_A of a finite group G\ud835\udc3aGitalic_G is called a p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, if A\ud835\udc34Aitalic_A covers or avoids any pd\ud835\udc5d\ud835\udc51pditalic_p italic_d-chief factor of G\ud835\udc3aGitalic_G. Let p\ud835\udc5dpitalic_p be a prime, S\ud835\udc46Sitalic_S be a p\ud835\udc5dpitalic_p-group and \u2131\u2131\\mathcal{F}caligraphic_F be a saturated fusion system over S\ud835\udc46Sitalic_S. Then \u2131\u2131\\mathcal{F}caligraphic_F is said to be supersolvable, if there exists a series of S\ud835\udc46Sitalic_S, namely 1=S0\u2a7dS1\u2a7d\u22ef\u2a7dSn=S1subscript\ud835\udc460subscript\ud835\udc461\u22efsubscript\ud835\udc46\ud835\udc5b\ud835\udc461=S_{0}\\leqslant S_{1}\\leqslant\\cdots\\leqslant S_{n}=S1 = italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2a7d italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2a7d \u22ef \u2a7d italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_S, such that Si+1/Sisubscript\ud835\udc46\ud835\udc561subscript\ud835\udc46\ud835\udc56S_{i+1}/S_{i}italic_S start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT / italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is cyclic, i=0,1,\u22ef,n\u22121\ud835\udc5601\u22ef\ud835\udc5b1i=0,1,\\cdots,n-1italic_i = 0 , 1 , \u22ef , italic_n - 1, Sisubscript\ud835\udc46\ud835\udc56S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is strongly \u2131\u2131\\mathcal{F}caligraphic_F-closed, i=0,1,\u22ef,n\ud835\udc5601\u22ef\ud835\udc5bi=0,1,\\cdots,nitalic_i = 0 , 1 , \u22ef , italic_n. In this paper, we investigate the properties of strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups and give several characterizations for a group G\ud835\udc3aGitalic_G to be p\ud835\udc5dpitalic_p-supersoluble under the hypothesis that some subgroups of G\ud835\udc3aGitalic_G are strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups of G\ud835\udc3aGitalic_G. Also, we investigate the characterizations for supersolvability of \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) under the assumption that some subgroups of G\ud835\udc3aGitalic_G are strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups of G\ud835\udc3aGitalic_G. Keywords: Strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups, Saturated formation, p\ud835\udc5dpitalic_p-supersoluble subgroups, Saturated fusion systems, Supersolvable fusion systems. MSC:20D10,20D15, 20D20 All group considered in this paper will be finite. We say a subgroup A\ud835\udc34Aitalic_A of a group G\ud835\udc3aGitalic_G satisfies the covering and avoidance of G\ud835\udc3aGitalic_G, if for any chief factor H/K\ud835\udc3b\ud835\udc3eH/Kitalic_H / italic_K of G\ud835\udc3aGitalic_G, either HA=KA\ud835\udc3b\ud835\udc34\ud835\udc3e\ud835\udc34HA=KAitalic_H italic_A = italic_K italic_A, or H\u2229A=K\u2229A\ud835\udc3b\ud835\udc34\ud835\udc3e\ud835\udc34H\\cap A=K\\cap Aitalic_H \u2229 italic_A = italic_K \u2229 italic_A. Such group A\ud835\udc34Aitalic_A is called a CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. The properties and characterizations of CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup have been widely researched since it plays an important part in the structural study of soluble or supersoluble groups. In recent years, covering and avoidance property has attracted a lot of authors. Some give the structure of CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups (like [6]), some give the equivalent conditions, while others give some generalizations for the original definition. In [7], partial CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup was introduced as a weaker generalization, by changing any chief factor in to a chief series compared to CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup. In [8], the generalized definition called CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P*-subgroup is restricted into the non-Frattini chief factors, and some equivalent criteria related to Hall groups, Sylow subgroups and maximal subgroups were given. In [9], stronger generalization named strong CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup emerges. A subgroup A\ud835\udc34Aitalic_A of a group G\ud835\udc3aGitalic_G is called a strong CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, if A\ud835\udc34Aitalic_A is a CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup for any H\u2a7eA\ud835\udc3b\ud835\udc34H\\geqslant Aitalic_H \u2a7e italic_A. Let A\ud835\udc34Aitalic_A be a subgroup of G\ud835\udc3aGitalic_G and A\u2a7dH\u2a7dG\ud835\udc34\ud835\udc3b\ud835\udc3aA\\leqslant H\\leqslant Gitalic_A \u2a7d italic_H \u2a7d italic_G. It is clear that strong CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup satisfies the intermidiate property, i.e. A\ud835\udc34Aitalic_A is a strong CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, then A\ud835\udc34Aitalic_A is a strong CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of H\ud835\udc3bHitalic_H. One can find that CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup fails to satisfy this property but partial CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup does. However, four subgroups mentioned above enjoy the same properties about the inheritance of quotient group as mentioned in [7, Lemma 2.6], [8, Lemma 2.3(1)], [9, Lemma 3(2)]. It is easy to find that only partial CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup satisfies both intermidiate property and inheritance property. Our aim is to find a new category of generalization of CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup which still satisfies the two properties. As introduced in [3], we have the following definition: A subgroup A\ud835\udc34Aitalic_A of a group G\ud835\udc3aGitalic_G is called a p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G if A\ud835\udc34Aitalic_A covers or avoids every pd\ud835\udc5d\ud835\udc51pditalic_p italic_d-chief factor of G\ud835\udc3aGitalic_G, where pd\ud835\udc5d\ud835\udc51pditalic_p italic_d-chief factor denotes the chief factor of G\ud835\udc3aGitalic_G with order divided by p\ud835\udc5dpitalic_p. It follows from [3, Lemma 2.1] that p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup satisfies the inheritance of quotient group. However, no evidence shows that p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup satisfies the intermidiate property. In fact, we can generalize the definition by strengthening our assumption into the following one: A subgroup A\ud835\udc34Aitalic_A of a group G\ud835\udc3aGitalic_G is called a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, if A\ud835\udc34Aitalic_A is a p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup for any H\u2a7eA\ud835\udc3b\ud835\udc34H\\geqslant Aitalic_H \u2a7e italic_A. Let G\ud835\udc3aGitalic_G be a finite group. Then for any prime divisor p\ud835\udc5dpitalic_p of G\ud835\udc3aGitalic_G, it is obvious that G\ud835\udc3aGitalic_G and 1111 are two trivial strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. Actually, non-trivial strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups have frequent occurrence in the structure of finite groups. The alternating group on 4 letters A4subscript\ud835\udc344A_{4}italic_A start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT has a maximal subgroup C3subscript\ud835\udc363C_{3}italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT. Then clearly C3subscript\ud835\udc363C_{3}italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT is a 3333-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of C3subscript\ud835\udc363C_{3}italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT itself. On the other hand, there is only one 3d3\ud835\udc513d3 italic_d-chief factor A4/C22subscript\ud835\udc344superscriptsubscript\ud835\udc3622A_{4}/C_{2}^{2}italic_A start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT / italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, and we conclude immediately that A4\u22c5C3=A4=C22\u22c5C3\u22c5subscript\ud835\udc344subscript\ud835\udc363subscript\ud835\udc344\u22c5superscriptsubscript\ud835\udc3622subscript\ud835\udc363A_{4}\\cdot C_{3}=A_{4}=C_{2}^{2}\\cdot C_{3}italic_A start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT \u22c5 italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = italic_A start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT \u22c5 italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT. Hence C3subscript\ud835\udc363C_{3}italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT is a 3333-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of A4subscript\ud835\udc344A_{4}italic_A start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, and so C3subscript\ud835\udc363C_{3}italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of A4subscript\ud835\udc344A_{4}italic_A start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT. Now we would like to give an example to show that p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups are not strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups in general, which differs the concept of strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups from p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups. Consider the group G:=SL2(\ud835\udd3d5)assign\ud835\udc3asubscriptSL2subscript\ud835\udd3d5G:={\\rm SL}_{2}({\\mathbb{F}}_{5})italic_G := roman_SL start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( blackboard_F start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT ). Then there exists a non-normal subgroup SL2(\ud835\udd3d3)subscriptSL2subscript\ud835\udd3d3{\\rm SL}_{2}({\\mathbb{F}}_{3})roman_SL start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( blackboard_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) of SL2(\ud835\udd3d5)subscriptSL2subscript\ud835\udd3d5{\\rm SL}_{2}({\\mathbb{F}}_{5})roman_SL start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( blackboard_F start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT ), and a subgroup C4subscript\ud835\udc364C_{4}italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT of SL2(\ud835\udd3d3)subscriptSL2subscript\ud835\udd3d3{\\rm SL}_{2}({\\mathbb{F}}_{3})roman_SL start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( blackboard_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ). Since there is only one non-trivial normal subgroup C2subscript\ud835\udc362C_{2}italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT of G\ud835\udc3aGitalic_G, and so C2/C1subscript\ud835\udc362subscript\ud835\udc361C_{2}/C_{1}italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT / italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is the only 2d2\ud835\udc512d2 italic_d-chief factor of G\ud835\udc3aGitalic_G. As C4subscript\ud835\udc364C_{4}italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT contains C2subscript\ud835\udc362C_{2}italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, it follows that C4\u22c5C1=C4=C2\u22c5C4\u22c5subscript\ud835\udc364subscript\ud835\udc361subscript\ud835\udc364\u22c5subscript\ud835\udc362subscript\ud835\udc364C_{4}\\cdot C_{1}=C_{4}=C_{2}\\cdot C_{4}italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT \u22c5 italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u22c5 italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, and so C4subscript\ud835\udc364C_{4}italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT covers C2/C1subscript\ud835\udc362subscript\ud835\udc361C_{2}/C_{1}italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT / italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, which implies that C4subscript\ud835\udc364C_{4}italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT is a 2222-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. On the other hand, consider the subgroup C4subscript\ud835\udc364C_{4}italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT again in the group SL2(\ud835\udd3d3)subscriptSL2subscript\ud835\udd3d3{\\rm SL}_{2}({\\mathbb{F}}_{3})roman_SL start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( blackboard_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ). Then one can find that Q8/C2subscript\ud835\udc448subscript\ud835\udc362Q_{8}/C_{2}italic_Q start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT / italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is a 2d2\ud835\udc512d2 italic_d-chief factor of SL2(\ud835\udd3d3)subscriptSL2subscript\ud835\udd3d3{\\rm SL}_{2}({\\mathbb{F}}_{3})roman_SL start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( blackboard_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ). However, since C2\u2a7dC4\u2a7dQ8subscript\ud835\udc362subscript\ud835\udc364subscript\ud835\udc448C_{2}\\leqslant C_{4}\\leqslant Q_{8}italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2a7d italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT \u2a7d italic_Q start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT, it yields that Q8\u22c5C4=Q8\u2260C4=C4\u22c5C2\u22c5subscript\ud835\udc448subscript\ud835\udc364subscript\ud835\udc448subscript\ud835\udc364\u22c5subscript\ud835\udc364subscript\ud835\udc362Q_{8}\\cdot C_{4}=Q_{8}\\neq C_{4}=C_{4}\\cdot C_{2}italic_Q start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT \u22c5 italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT = italic_Q start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT \u2260 italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT \u22c5 italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and C4\u2229Q8=C4\u2260C2=C4\u2229C2subscript\ud835\udc364subscript\ud835\udc448subscript\ud835\udc364subscript\ud835\udc362subscript\ud835\udc364subscript\ud835\udc362C_{4}\\cap Q_{8}=C_{4}\\neq C_{2}=C_{4}\\cap C_{2}italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT \u2229 italic_Q start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT \u2260 italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT \u2229 italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and so C4subscript\ud835\udc364C_{4}italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT is not a 2222-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of SL2(\ud835\udd3d3)subscriptSL2subscript\ud835\udd3d3{\\rm SL}_{2}({\\mathbb{F}}_{3})roman_SL start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( blackboard_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ). Therefore C4subscript\ud835\udc364C_{4}italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT is a 2222-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, but not a strong 2222-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. In the first part of our paper, we will prove later that strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup satisfies both intermidiate property and inheritance property. This spotlight actually gives strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup more characterizations and more criteria. In this paper, we will give two characterizations for a group G\ud835\udc3aGitalic_G to be p\ud835\udc5dpitalic_p-supersoluble or in a saturated formation containing \ud835\udd18\ud835\udd18\\mathfrak{U}fraktur_U under the assumption that some subgroups of G\ud835\udc3aGitalic_G are strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups. Now, we are going to give a brief introduction to the basic theory of fusion systems, and begin to introduce our results on the characterization of the structure of \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) under the assumption that certain subgroups of S\u2208Sylp(G)\ud835\udc46subscriptSyl\ud835\udc5d\ud835\udc3aS\\in{\\rm Syl}_{p}(G)italic_S \u2208 roman_Syl start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) are strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups of G\ud835\udc3aGitalic_G. Let S\ud835\udc46Sitalic_S be a Sylow p\ud835\udc5dpitalic_p-subgroup of G\ud835\udc3aGitalic_G, where p\ud835\udc5dpitalic_p is a prime divisor of |G|\ud835\udc3a|G|| italic_G |. Then the fusion system of G\ud835\udc3aGitalic_G over S\ud835\udc46Sitalic_S, named \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ), is a fusion category over S\ud835\udc46Sitalic_S which is defined as follows: The object of \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) is the set of all subgroups of S\ud835\udc46Sitalic_S. For any P,Q\u2a7dS\ud835\udc43\ud835\udc44\ud835\udc46P,Q\\leqslant Sitalic_P , italic_Q \u2a7d italic_S, Mor\u2131S(G)(P,Q)={\u03d5|\u03d5:P\u2192Q,p\u21a6pg,Pg\u2a7dQ,g\u2208G}subscriptMorsubscript\u2131\ud835\udc46\ud835\udc3a\ud835\udc43\ud835\udc44conditional-setitalic-\u03d5:italic-\u03d5formulae-sequence\u2192\ud835\udc43\ud835\udc44formulae-sequencemaps-to\ud835\udc5dsuperscript\ud835\udc5d\ud835\udc54formulae-sequencesuperscript\ud835\udc43\ud835\udc54\ud835\udc44\ud835\udc54\ud835\udc3a{\\rm Mor}_{\\mathcal{F}_{S}(G)}(P,Q)=\\{\\phi\\,|\\,\\phi:P\\rightarrow Q,\\,p\\mapsto p% ^{g},\\,P^{g}\\leqslant Q,\\,g\\in G\\}roman_Mor start_POSTSUBSCRIPT caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) end_POSTSUBSCRIPT ( italic_P , italic_Q ) = { italic_\u03d5 | italic_\u03d5 : italic_P \u2192 italic_Q , italic_p \u21a6 italic_p start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT , italic_P start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT \u2a7d italic_Q , italic_g \u2208 italic_G }. One can easily find that \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) is exactly a saturated fusion system over S\ud835\udc46Sitalic_S by [1, Theorem 2.3]. As is known to all, the structure of \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) has a strong relationship with the structure of G\ud835\udc3aGitalic_G. Hence some structures of finite groups can be generalized into the fusion system \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ). Recall that G\ud835\udc3aGitalic_G is said to be supersolvable, if there exists a chief series, namely such that Ni+1/Nisubscript\ud835\udc41\ud835\udc561subscript\ud835\udc41\ud835\udc56N_{i+1}/N_{i}italic_N start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT / italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is cyclic, i=0,1,\u22ef,t\u22121\ud835\udc5601\u22ef\ud835\udc611i=0,1,\\cdots,t-1italic_i = 0 , 1 , \u22ef , italic_t - 1. As a natural way of generalization, we want to define a similar structure in \u2131\u2131\\mathcal{F}caligraphic_F, where \u2131\u2131\\mathcal{F}caligraphic_F is a fusion system over a p\ud835\udc5dpitalic_p-group S\ud835\udc46Sitalic_S. Since the objects of \u2131\u2131\\mathcal{F}caligraphic_F are exactly the subgroups of S\ud835\udc46Sitalic_S, we may restrict the chief series of G\ud835\udc3aGitalic_G into a series of S\ud835\udc46Sitalic_S. Note that the normality of subgroups H\ud835\udc3bHitalic_H of S\ud835\udc46Sitalic_S in G\ud835\udc3aGitalic_G represents the invariance of H\ud835\udc3bHitalic_H under the morphisms induced by conjugation of G\ud835\udc3aGitalic_G, we may change the normality of H\ud835\udc3bHitalic_H in G\ud835\udc3aGitalic_G into invariance of H\ud835\udc3bHitalic_H under the morphisms in \u2131\u2131\\mathcal{F}caligraphic_F. Then one can easily find that the invariance of H\ud835\udc3bHitalic_H under the morphisms in \u2131\u2131\\mathcal{F}caligraphic_F exactly suits the concept of weakly \u2131\u2131\\mathcal{F}caligraphic_F-closed property, hence we give the following definition which was introduced by N. Su in [12]. Let \u2131\u2131\\mathcal{F}caligraphic_F be a saturated fusion system over a p\ud835\udc5dpitalic_p-group S\ud835\udc46Sitalic_S. Then \u2131\u2131\\mathcal{F}caligraphic_F is called supersolvable, if there exists a series of subgroups of S\ud835\udc46Sitalic_S, namely: such that Sisubscript\ud835\udc46\ud835\udc56S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is strongly \u2131\u2131\\mathcal{F}caligraphic_F-closed, i=0,1,\u22ef,n\ud835\udc5601\u22ef\ud835\udc5bi=0,1,\\cdots,nitalic_i = 0 , 1 , \u22ef , italic_n, and Si+1/Sisubscript\ud835\udc46\ud835\udc561subscript\ud835\udc46\ud835\udc56S_{i+1}/S_{i}italic_S start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT / italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is cyclic for any i=0,1,\u22ef,n\u22121\ud835\udc5601\u22ef\ud835\udc5b1i=0,1,\\cdots,n-1italic_i = 0 , 1 , \u22ef , italic_n - 1. Now we would like to introduce some concepts which are useful for us to discover the structure of \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ). Let S\ud835\udc46Sitalic_S be a p\ud835\udc5dpitalic_p-group and P\ud835\udc43Pitalic_P be a subgroup of S\ud835\udc46Sitalic_S. Suppose that \u2131\u2131\\mathcal{F}caligraphic_F is a fusion system over S\ud835\udc46Sitalic_S. Then P\ud835\udc43Pitalic_P is called \u2131\u2131\\mathcal{F}caligraphic_F-centric, if CS(Q)=Z(Q)subscript\ud835\udc36\ud835\udc46\ud835\udc44\ud835\udc4d\ud835\udc44C_{S}(Q)=Z(Q)italic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_Q ) = italic_Z ( italic_Q ) for all Q\u2208P\u2131\ud835\udc44superscript\ud835\udc43\u2131Q\\in P^{\\mathcal{F}}italic_Q \u2208 italic_P start_POSTSUPERSCRIPT caligraphic_F end_POSTSUPERSCRIPT, where P\u2131superscript\ud835\udc43\u2131P^{\\mathcal{F}}italic_P start_POSTSUPERSCRIPT caligraphic_F end_POSTSUPERSCRIPT denotes the set of all subgroups of S\ud835\udc46Sitalic_S which are \u2131\u2131\\mathcal{F}caligraphic_F-conjugate to P\ud835\udc43Pitalic_P. P\ud835\udc43Pitalic_P is said to be fully normalized in \u2131\u2131\\mathcal{F}caligraphic_F, if |NS(P)|\u2a7e|NS(Q)|subscript\ud835\udc41\ud835\udc46\ud835\udc43subscript\ud835\udc41\ud835\udc46\ud835\udc44|N_{S}(P)|\\geqslant|N_{S}(Q)|| italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_P ) | \u2a7e | italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_Q ) | for all Q\u2208P\u2131\ud835\udc44superscript\ud835\udc43\u2131Q\\in P^{\\mathcal{F}}italic_Q \u2208 italic_P start_POSTSUPERSCRIPT caligraphic_F end_POSTSUPERSCRIPT. P\ud835\udc43Pitalic_P is said to be \u2131\u2131\\mathcal{F}caligraphic_F-essential, if P\ud835\udc43Pitalic_P is \u2131\u2131\\mathcal{F}caligraphic_F-centric and fully normalized in \u2131\u2131\\mathcal{F}caligraphic_F, and Out\u2131(P)subscriptOut\u2131\ud835\udc43{\\rm Out}_{\\mathcal{F}}(P)roman_Out start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT ( italic_P ) contains a strongly p\ud835\udc5dpitalic_p-embedded subgroup (see [1, Definition A.6]). Now we are ready to introduce the following concept, which was firstly introduced in [2, Notation 2.5] . Let p\ud835\udc5dpitalic_p be a prime, \u2131\u2131\\mathcal{F}caligraphic_F be a saturated fusion system on a finite group S\ud835\udc46Sitalic_S. Let Let G\ud835\udc3aGitalic_G be a finite group and S\ud835\udc46Sitalic_S be a Sylow p\ud835\udc5dpitalic_p-subgroup of G\ud835\udc3aGitalic_G. In many cases, if certain subgroups of S\ud835\udc46Sitalic_S satisfy certain properties, then G\ud835\udc3aGitalic_G is p\ud835\udc5dpitalic_p-nilpotent or p\ud835\udc5dpitalic_p-supersolvable. On the other hand, on the assumption that a smaller collection of subgroups of S\ud835\udc46Sitalic_S satisfy some properties, for example, they are strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups of G\ud835\udc3aGitalic_G, then G\ud835\udc3aGitalic_G is not necessarily a p\ud835\udc5dpitalic_p-supersolvable subgroup. But actually, this does not mean that there is nothing to do with the structure of the finite group G\ud835\udc3aGitalic_G. Notice that the fusion system \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) is just a description of the structure of G\ud835\udc3aGitalic_G, we wonder if we can characterize the structure of \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ). Fortunately, we obtain the following theorem to characterize the structure of \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) under the assumption that certain subgroups of S\ud835\udc46Sitalic_S are strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroups of G\ud835\udc3aGitalic_G, which highlights the paper considerably. Let G\ud835\udc3aGitalic_G be a finite group, p\ud835\udc5dpitalic_p be a prime divisor of |G|\ud835\udc3a|G|| italic_G |, and S\ud835\udc46Sitalic_S be a Sylow p\ud835\udc5dpitalic_p-subgroup of G\ud835\udc3aGitalic_G. Suppose that any cyclic subgroup of S\ud835\udc46Sitalic_S with order p\ud835\udc5dpitalic_p or 4444 (if p\ud835\udc5dpitalic_p=2) is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G , then \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) is supersolvable. In this section, two important lemmas are introduced to support our characterizations. The first lemma is about the intermidiate and inheritance property which have been frequently mentioned above. The second theorem plays an vital role in our characterizations. Let H\ud835\udc3bHitalic_H be a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G and N\\ThisStyle\\stackinsetr.75pt+.15\\LMptt.1\\LMpt \\SavedStyle\u2a7dG\\stackinset\ud835\udc5f.75\ud835\udc5d\ud835\udc61.15\\LMpt\ud835\udc61.1\\LMpt \\SavedStyleabsent\ud835\udc41\\ThisStyle\ud835\udc3aN\\ThisStyle{\\mathrel{\\stackinset{r}{.75pt+.15\\LMpt}{t}{.1\\LMpt}{\\rule{0.3pt}{1% .1pt}}{\\SavedStyle\\leqslant}}}Gitalic_N start_RELOP italic_r .75 italic_p italic_t + .15 italic_t .1 italic_\\SavedStyle \u2a7d end_RELOP italic_G. Then the following statements are true. N\ud835\udc41Nitalic_N is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. H\ud835\udc3bHitalic_H is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup for any K\u2a7eH\ud835\udc3e\ud835\udc3bK\\geqslant Hitalic_K \u2a7e italic_H. If H\u2a7eN\ud835\udc3b\ud835\udc41H\\geqslant Nitalic_H \u2a7e italic_N, then H/N\ud835\udc3b\ud835\udc41H/Nitalic_H / italic_N is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G/N\ud835\udc3a\ud835\udc41G/Nitalic_G / italic_N. If (|H|,|N|)=1\ud835\udc3b\ud835\udc411(|H|,|N|)=1( | italic_H | , | italic_N | ) = 1, then HN/N\ud835\udc3b\ud835\udc41\ud835\udc41HN/Nitalic_H italic_N / italic_N is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G/N\ud835\udc3a\ud835\udc41G/Nitalic_G / italic_N. [Proof](1) It follows from N\\ThisStyle\\stackinsetr.75pt+.15\\LMptt.1\\LMpt \\SavedStyle\u2a7dK\\stackinset\ud835\udc5f.75\ud835\udc5d\ud835\udc61.15\\LMpt\ud835\udc61.1\\LMpt \\SavedStyleabsent\ud835\udc41\\ThisStyle\ud835\udc3eN\\ThisStyle{\\mathrel{\\stackinset{r}{.75pt+.15\\LMpt}{t}{.1\\LMpt}{\\rule{0.3pt}{1% .1pt}}{\\SavedStyle\\leqslant}}}Kitalic_N start_RELOP italic_r .75 italic_p italic_t + .15 italic_t .1 italic_\\SavedStyle \u2a7d end_RELOP italic_K for any N\u2a7dK\u2a7dG\ud835\udc41\ud835\udc3e\ud835\udc3aN\\leqslant K\\leqslant Gitalic_N \u2a7d italic_K \u2a7d italic_G and [3, Lemma 2.1(a)] that N\ud835\udc41Nitalic_N is a p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup for any N\u2a7dK\u2a7dG\ud835\udc41\ud835\udc3e\ud835\udc3aN\\leqslant K\\leqslant Gitalic_N \u2a7d italic_K \u2a7d italic_G. Hence N\ud835\udc41Nitalic_N is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. (2) Since H\ud835\udc3bHitalic_H is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, H\ud835\udc3bHitalic_H is a p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup for any H\u2a7dR\u2a7dG\ud835\udc3b\ud835\udc45\ud835\udc3aH\\leqslant R\\leqslant Gitalic_H \u2a7d italic_R \u2a7d italic_G. Therefore H\ud835\udc3bHitalic_H is a p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup for any H\u2a7dR\u2a7dK\ud835\udc3b\ud835\udc45\ud835\udc3eH\\leqslant R\\leqslant Kitalic_H \u2a7d italic_R \u2a7d italic_K. Hence we conclude that H\ud835\udc3bHitalic_H is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of K\ud835\udc3eKitalic_K. (3) Since H\ud835\udc3bHitalic_H is a p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup for any H\u2a7dK\u2a7dG\ud835\udc3b\ud835\udc3e\ud835\udc3aH\\leqslant K\\leqslant Gitalic_H \u2a7d italic_K \u2a7d italic_G, it follows from [3, Lemma 2.1(b)] and N\\ThisStyle\\stackinsetr.75pt+.15\\LMptt.1\\LMpt \\SavedStyle\u2a7dK\\stackinset\ud835\udc5f.75\ud835\udc5d\ud835\udc61.15\\LMpt\ud835\udc61.1\\LMpt \\SavedStyleabsent\ud835\udc41\\ThisStyle\ud835\udc3eN\\ThisStyle{\\mathrel{\\stackinset{r}{.75pt+.15\\LMpt}{t}{.1\\LMpt}{\\rule{0.3pt}{1% .1pt}}{\\SavedStyle\\leqslant}}}Kitalic_N start_RELOP italic_r .75 italic_p italic_t + .15 italic_t .1 italic_\\SavedStyle \u2a7d end_RELOP italic_K that H/N\ud835\udc3b\ud835\udc41H/Nitalic_H / italic_N is a p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup for any H/N\u2a7dK/N\u2a7dG/N\ud835\udc3b\ud835\udc41\ud835\udc3e\ud835\udc41\ud835\udc3a\ud835\udc41H/N\\leqslant K/N\\leqslant G/Nitalic_H / italic_N \u2a7d italic_K / italic_N \u2a7d italic_G / italic_N. Thus it indicates that H/N\ud835\udc3b\ud835\udc41H/Nitalic_H / italic_N is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G/N\ud835\udc3a\ud835\udc41G/Nitalic_G / italic_N. (4) Since H\ud835\udc3bHitalic_H is a p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup for any HN\u2a7dK\u2a7dG\ud835\udc3b\ud835\udc41\ud835\udc3e\ud835\udc3aHN\\leqslant K\\leqslant Gitalic_H italic_N \u2a7d italic_K \u2a7d italic_G, and (|H|,|N|)=1\ud835\udc3b\ud835\udc411(|H|,|N|)=1( | italic_H | , | italic_N | ) = 1, it follows from [3, Lemma 2.1(c)] and N\\ThisStyle\\stackinsetr.75pt+.15\\LMptt.1\\LMpt \\SavedStyle\u2a7dK\\stackinset\ud835\udc5f.75\ud835\udc5d\ud835\udc61.15\\LMpt\ud835\udc61.1\\LMpt \\SavedStyleabsent\ud835\udc41\\ThisStyle\ud835\udc3eN\\ThisStyle{\\mathrel{\\stackinset{r}{.75pt+.15\\LMpt}{t}{.1\\LMpt}{\\rule{0.3pt}{1% .1pt}}{\\SavedStyle\\leqslant}}}Kitalic_N start_RELOP italic_r .75 italic_p italic_t + .15 italic_t .1 italic_\\SavedStyle \u2a7d end_RELOP italic_K that HN/N\ud835\udc3b\ud835\udc41\ud835\udc41HN/Nitalic_H italic_N / italic_N is a p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup for any HN/N\u2a7dK/N\u2a7dG/N\ud835\udc3b\ud835\udc41\ud835\udc41\ud835\udc3e\ud835\udc41\ud835\udc3a\ud835\udc41HN/N\\leqslant K/N\\leqslant G/Nitalic_H italic_N / italic_N \u2a7d italic_K / italic_N \u2a7d italic_G / italic_N. Hence HN/N\ud835\udc3b\ud835\udc41\ud835\udc41HN/Nitalic_H italic_N / italic_N is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G/N\ud835\udc3a\ud835\udc41G/Nitalic_G / italic_N. According to [10, Section 9.5], a class of finite groups \ud835\udd09\ud835\udd09\\mathfrak{F}fraktur_F is said to be a formation if every image of an \ud835\udd09\ud835\udd09\\mathfrak{F}fraktur_F-group is an \ud835\udd09\ud835\udd09\\mathfrak{F}fraktur_F-group and if G/N1\u2229N2\ud835\udc3asubscript\ud835\udc411subscript\ud835\udc412G/N_{1}\\cap N_{2}italic_G / italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2229 italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT belongs to \ud835\udd09\ud835\udd09\\mathfrak{F}fraktur_F whenever G/N1\ud835\udc3asubscript\ud835\udc411G/N_{1}italic_G / italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and G/N2\ud835\udc3asubscript\ud835\udc412G/N_{2}italic_G / italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT belong to \ud835\udd09\ud835\udd09\\mathfrak{F}fraktur_F. A formation \ud835\udd09\ud835\udd09\\mathfrak{F}fraktur_F is said to be saturated if a finite group G\u2208\ud835\udd09\ud835\udc3a\ud835\udd09G\\in\\mathfrak{F}italic_G \u2208 fraktur_F whenever G/\u03a6(G)\ud835\udc3a\u03a6\ud835\udc3aG/\\Phi(G)italic_G / roman_\u03a6 ( italic_G ). Let \ud835\udd18\ud835\udd18\\mathfrak{U}fraktur_U denotes the class of all supersoluble groups and \ud835\udd18\ud835\udd2dsubscript\ud835\udd18\ud835\udd2d\\mathfrak{U_{p}}fraktur_U start_POSTSUBSCRIPT fraktur_p end_POSTSUBSCRIPT denotes the class of all p\ud835\udc5dpitalic_p-supersoluble groups, where p\ud835\udc5dpitalic_p is a prime. It is well known that both \ud835\udd18\ud835\udd18\\mathfrak{U}fraktur_U and \ud835\udd18\ud835\udd2dsubscript\ud835\udd18\ud835\udd2d\\mathfrak{U_{p}}fraktur_U start_POSTSUBSCRIPT fraktur_p end_POSTSUBSCRIPT are saturated formations, and we have \ud835\udd18\ud835\udd2d\u2287\ud835\udd18\ud835\udd18subscript\ud835\udd18\ud835\udd2d\\mathfrak{U_{p}}\\supseteq\\mathfrak{U}fraktur_U start_POSTSUBSCRIPT fraktur_p end_POSTSUBSCRIPT \u2287 fraktur_U. Let \u210c\u210c\\mathfrak{H}fraktur_H be a non-empty class of groups. According to [11, Definition 1.2.9, Definition 2.3.18], a chief factor H/K\ud835\udc3b\ud835\udc3eH/Kitalic_H / italic_K of a group G\ud835\udc3aGitalic_G is called \u210c\u210c\\mathfrak{H}fraktur_H-central in G\ud835\udc3aGitalic_G if [H/K]\u2217G\u2217delimited-[]\ud835\udc3b\ud835\udc3e\ud835\udc3a[H/K]\\ast G[ italic_H / italic_K ] \u2217 italic_G belongs to \u210c\u210c\\mathfrak{H}fraktur_H, where [H/K]\u2217G\u2217delimited-[]\ud835\udc3b\ud835\udc3e\ud835\udc3a[H/K]\\ast G[ italic_H / italic_K ] \u2217 italic_G is the semi-direct product [H/K](G/CG(H/K))delimited-[]\ud835\udc3b\ud835\udc3e\ud835\udc3asubscript\ud835\udc36\ud835\udc3a\ud835\udc3b\ud835\udc3e[H/K](G/C_{G}(H/K))[ italic_H / italic_K ] ( italic_G / italic_C start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_H / italic_K ) ) if H/K\ud835\udc3b\ud835\udc3eH/Kitalic_H / italic_K is abelian and G/CG(H/K)\ud835\udc3asubscript\ud835\udc36\ud835\udc3a\ud835\udc3b\ud835\udc3eG/C_{G}(H/K)italic_G / italic_C start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_H / italic_K ) if H/K\ud835\udc3b\ud835\udc3eH/Kitalic_H / italic_K is non-abelian. A normal subgroup N\ud835\udc41Nitalic_N of a group G\ud835\udc3aGitalic_G is said to be \u210c\u210c\\mathfrak{H}fraktur_H-hypercentral in G\ud835\udc3aGitalic_G if every chief factor of G\ud835\udc3aGitalic_G below N\ud835\udc41Nitalic_N is \u210c\u210c\\mathfrak{H}fraktur_H-central in G\ud835\udc3aGitalic_G. By generalised Jordan-Holder theorem, we conclude that the product of \u210c\u210c\\mathfrak{H}fraktur_H-hypercentral normal subgroups of a group G\ud835\udc3aGitalic_G is \u210c\u210c\\mathfrak{H}fraktur_H-hypercentral in G\ud835\udc3aGitalic_G as well. Thus every group G\ud835\udc3aGitalic_G possesses a unique maximal normal \u210c\u210c\\mathfrak{H}fraktur_H-hypercentral subgroup namely the \u210c\u210c\\mathfrak{H}fraktur_H-hypercentre of G\ud835\udc3aGitalic_G and denoted by Z\u210c(G)subscript\ud835\udc4d\u210c\ud835\udc3aZ_{\\mathfrak{H}}(G)italic_Z start_POSTSUBSCRIPT fraktur_H end_POSTSUBSCRIPT ( italic_G ). Apply the generalised Jordan-Holder theorem again, it yields that every chief factor of G\ud835\udc3aGitalic_G below Z\u210c(G)subscript\ud835\udc4d\u210c\ud835\udc3aZ_{\\mathfrak{H}}(G)italic_Z start_POSTSUBSCRIPT fraktur_H end_POSTSUBSCRIPT ( italic_G ) is \u210c\u210c\\mathfrak{H}fraktur_H-central in G\ud835\udc3aGitalic_G. Let p\ud835\udc5dpitalic_p be a prime and \u2131\u2131\\mathcal{F}caligraphic_F be a saturated fusion system on a finite p\ud835\udc5dpitalic_p-group S\ud835\udc46Sitalic_S. Assume that the fusion system N\u2131(Q)subscript\ud835\udc41\u2131\ud835\udc44N_{\\mathcal{F}}(Q)italic_N start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT ( italic_Q ) is supersolvable for any Q\u2208\u2130\u2131*\ud835\udc44superscriptsubscript\u2130\u2131Q\\in\\mathcal{E}_{\\mathcal{F}}^{*}italic_Q \u2208 caligraphic_E start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, then \u2131\u2131\\mathcal{F}caligraphic_F is supersolvable. Let G\ud835\udc3aGitalic_G be a finite group, p\u2208\u03c0(G)\ud835\udc5d\ud835\udf0b\ud835\udc3ap\\in\\pi(G)italic_p \u2208 italic_\u03c0 ( italic_G ), and S\ud835\udc46Sitalic_S be a Sylow p\ud835\udc5dpitalic_p-subgroup of G\ud835\udc3aGitalic_G. Suppose that for any proper subgroup H\ud835\udc3bHitalic_H of G\ud835\udc3aGitalic_G with Op(G)<S\u2229Hsubscript\ud835\udc42\ud835\udc5d\ud835\udc3a\ud835\udc46\ud835\udc3bO_{p}(G)<S\\cap Hitalic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) < italic_S \u2229 italic_H and S\u2229H\u2208Sylp(H)\ud835\udc46\ud835\udc3bsubscriptnormal-Syl\ud835\udc5d\ud835\udc3bS\\cap H\\in{\\rm Syl}_{p}(H)italic_S \u2229 italic_H \u2208 roman_Syl start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_H ), the fusion system \u2131S\u2229H(H)subscript\u2131\ud835\udc46\ud835\udc3b\ud835\udc3b\\mathcal{F}_{S\\cap H}(H)caligraphic_F start_POSTSUBSCRIPT italic_S \u2229 italic_H end_POSTSUBSCRIPT ( italic_H ) is supersolvable. Assume additionally that Op(G)\u2a7dZ\ud835\udd18(G)subscript\ud835\udc42\ud835\udc5d\ud835\udc3asubscript\ud835\udc4d\ud835\udd18\ud835\udc3aO_{p}(G)\\leqslant Z_{\\mathfrak{U}}(G)italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) \u2a7d italic_Z start_POSTSUBSCRIPT fraktur_U end_POSTSUBSCRIPT ( italic_G ). Then \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) is supersolvable. Let p\ud835\udc5dpitalic_p be a prime number and P\ud835\udc43Pitalic_P be a normal p\ud835\udc5dpitalic_p-subgroup of G\ud835\udc3aGitalic_G. If every cyclic subgroup of P\ud835\udc43Pitalic_P of prime order or 4444 is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, then P\u2a7dZ\ud835\udd18(G)\ud835\udc43subscript\ud835\udc4d\ud835\udd18\ud835\udc3aP\\leqslant Z_{\\mathfrak{U}}(G)italic_P \u2a7d italic_Z start_POSTSUBSCRIPT fraktur_U end_POSTSUBSCRIPT ( italic_G ), where Z\ud835\udd18(G)subscript\ud835\udc4d\ud835\udd18\ud835\udc3aZ_{\\mathfrak{U}}(G)italic_Z start_POSTSUBSCRIPT fraktur_U end_POSTSUBSCRIPT ( italic_G ) denotes the \ud835\udd18\ud835\udd18\\mathfrak{U}fraktur_U-hypercentre of G\ud835\udc3aGitalic_G. [Proof]Assume that the theorem is not true and choose G\ud835\udc3aGitalic_G and P\ud835\udc43Pitalic_P for which it fails. Then there exists a chief factor of G\ud835\udc3aGitalic_G below P\ud835\udc43Pitalic_P which is not of prime order. Between all of the non-cyclic chief factors of G\ud835\udc3aGitalic_G below P\ud835\udc43Pitalic_P, we choose L/K\ud835\udc3f\ud835\udc3eL/Kitalic_L / italic_K such that |L|\ud835\udc3f|L|| italic_L | is of minimal size. Assume there exists an element x\ud835\udc65xitalic_x of L\ud835\udc3fLitalic_L of order p\ud835\udc5dpitalic_p or 4444 which is not in K\ud835\udc3eKitalic_K. Then \u27e8x\u27e9delimited-\u27e8\u27e9\ud835\udc65\\langle x\\rangle\u27e8 italic_x \u27e9 is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. Consider a chief series of G\ud835\udc3aGitalic_G passing through L\ud835\udc3fLitalic_L, namely \u0393normal-\u0393\\Gammaroman_\u0393: It follows from L/K\ud835\udc3f\ud835\udc3eL/Kitalic_L / italic_K below P\ud835\udc43Pitalic_P that \u27e8x\u27e9delimited-\u27e8\u27e9\ud835\udc65\\langle x\\rangle\u27e8 italic_x \u27e9 covers or avoids every Li+1/Lisubscript\ud835\udc3f\ud835\udc561subscript\ud835\udc3f\ud835\udc56L_{i+1}/L_{i}italic_L start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT / italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, i=0,1,\u22ef,s\u22121\ud835\udc5601normal-\u22ef\ud835\udc601i=0,1,\\cdots,s-1italic_i = 0 , 1 , \u22ef , italic_s - 1. Since Li\u2a7e\u27e8x\u27e9subscript\ud835\udc3f\ud835\udc56delimited-\u27e8\u27e9\ud835\udc65L_{i}\\geqslant\\langle x\\rangleitalic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2a7e \u27e8 italic_x \u27e9, i\u2a7es\ud835\udc56\ud835\udc60i\\geqslant sitalic_i \u2a7e italic_s, we conclude that \u27e8x\u27e9delimited-\u27e8\u27e9\ud835\udc65\\langle x\\rangle\u27e8 italic_x \u27e9 avoids Li+1/Lisubscript\ud835\udc3f\ud835\udc561subscript\ud835\udc3f\ud835\udc56L_{i+1}/L_{i}italic_L start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT / italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, i\u2a7es\ud835\udc56\ud835\udc60i\\geqslant sitalic_i \u2a7e italic_s. Hence \u27e8x\u27e9delimited-\u27e8\u27e9\ud835\udc65\\langle x\\rangle\u27e8 italic_x \u27e9 covers or avoids each chief factor of \u0393normal-\u0393\\Gammaroman_\u0393. By the choice of L/K\ud835\udc3f\ud835\udc3eL/Kitalic_L / italic_K, Li+1/Lisubscript\ud835\udc3f\ud835\udc561subscript\ud835\udc3f\ud835\udc56L_{i+1}/L_{i}italic_L start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT / italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, i=0,1,\u22ef,s\u22121\ud835\udc5601normal-\u22ef\ud835\udc601i=0,1,\\cdots,s-1italic_i = 0 , 1 , \u22ef , italic_s - 1 is of prime order. If \u27e8x\u27e9delimited-\u27e8\u27e9\ud835\udc65\\langle x\\rangle\u27e8 italic_x \u27e9 covers Ls/Ls\u22121subscript\ud835\udc3f\ud835\udc60subscript\ud835\udc3f\ud835\udc601L_{s}/L_{s-1}italic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT / italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT, it follows from Ls=Ls\u22121\u27e8x\u27e9subscript\ud835\udc3f\ud835\udc60subscript\ud835\udc3f\ud835\udc601delimited-\u27e8\u27e9\ud835\udc65L_{s}=L_{s-1}\\langle x\\rangleitalic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT \u27e8 italic_x \u27e9 that Ls/Ls\u22121subscript\ud835\udc3f\ud835\udc60subscript\ud835\udc3f\ud835\udc601L_{s}/L_{s-1}italic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT / italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT is of prime order. Hence we conclude that L\u2a7dZ\ud835\udd18(G)\ud835\udc3fsubscript\ud835\udc4d\ud835\udd18\ud835\udc3aL\\leqslant Z_{\\mathfrak{U}}(G)italic_L \u2a7d italic_Z start_POSTSUBSCRIPT fraktur_U end_POSTSUBSCRIPT ( italic_G ). This implies that L/K\ud835\udc3f\ud835\udc3eL/Kitalic_L / italic_K is of prime order, a contradiction. Hence we have that \u27e8x\u27e9delimited-\u27e8\u27e9\ud835\udc65\\langle x\\rangle\u27e8 italic_x \u27e9 avoids Ls/Ls\u22121subscript\ud835\udc3f\ud835\udc60subscript\ud835\udc3f\ud835\udc601L_{s}/L_{s-1}italic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT / italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT. Then Ls=Ls\u2229\u27e8x\u27e9=Ls\u22121\u2229\u27e8x\u27e9subscript\ud835\udc3f\ud835\udc60subscript\ud835\udc3f\ud835\udc60delimited-\u27e8\u27e9\ud835\udc65subscript\ud835\udc3f\ud835\udc601delimited-\u27e8\u27e9\ud835\udc65L_{s}=L_{s}\\cap\\langle x\\rangle=L_{s-1}\\cap\\langle x\\rangleitalic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = italic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT \u2229 \u27e8 italic_x \u27e9 = italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT \u2229 \u27e8 italic_x \u27e9, i.e. x\u2208Ls\u22121\ud835\udc65subscript\ud835\udc3f\ud835\udc601x\\in L_{s-1}italic_x \u2208 italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT. Thus we have that Ls\u22121K>Ksubscript\ud835\udc3f\ud835\udc601\ud835\udc3e\ud835\udc3eL_{s-1}K>Kitalic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT italic_K > italic_K, which implies that Ls=Ls\u22121Ksubscript\ud835\udc3f\ud835\udc60subscript\ud835\udc3f\ud835\udc601\ud835\udc3eL_{s}=L_{s-1}Kitalic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT italic_K. Since Ls\u22121K/Ls\u22121subscript\ud835\udc3f\ud835\udc601\ud835\udc3esubscript\ud835\udc3f\ud835\udc601L_{s-1}K/L_{s-1}italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT italic_K / italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT is G\ud835\udc3aGitalic_G-isomorphic to Ls\u22121/(K\u2229Ls\u22121)subscript\ud835\udc3f\ud835\udc601\ud835\udc3esubscript\ud835\udc3f\ud835\udc601L_{s-1}/(K\\cap L_{s-1})italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT / ( italic_K \u2229 italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT ), we conclude that Ls\u22121/(K\u2229Ls\u22121)subscript\ud835\udc3f\ud835\udc601\ud835\udc3esubscript\ud835\udc3f\ud835\udc601L_{s-1}/(K\\cap L_{s-1})italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT / ( italic_K \u2229 italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT ) is a chief factor of G\ud835\udc3aGitalic_G as well. By the minimality of |L|\ud835\udc3f|L|| italic_L |, we get that Ls\u22121/(K\u2229Ls\u22121)subscript\ud835\udc3f\ud835\udc601\ud835\udc3esubscript\ud835\udc3f\ud835\udc601L_{s-1}/(K\\cap L_{s-1})italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT / ( italic_K \u2229 italic_L start_POSTSUBSCRIPT italic_s - 1 end_POSTSUBSCRIPT ) is of prime order, i.e. L/K\ud835\udc3f\ud835\udc3eL/Kitalic_L / italic_K is of prime order, a contradiction. Hence it yields that every element of L\ud835\udc3fLitalic_L of order p\ud835\udc5dpitalic_p or 4444 is contained in K\ud835\udc3eKitalic_K. Let X\ud835\udc4bXitalic_X denote the intersection of the centralisers of the chief factors of G\ud835\udc3aGitalic_G below K\ud835\udc3eKitalic_K. Since all chief factor below K\ud835\udc3eKitalic_K is of prime order, it follows that all of them are cyclic. Let denotes part of a chief series of G\ud835\udc3aGitalic_G passing through K\ud835\udc3eKitalic_K. Then [Ki,K]\u2a7dKi\u22121subscript\ud835\udc3e\ud835\udc56\ud835\udc3esubscript\ud835\udc3e\ud835\udc561[K_{i},K]\\leqslant K_{i-1}[ italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_K ] \u2a7d italic_K start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, i=2,3,\u22ef,q\ud835\udc5623normal-\u22ef\ud835\udc5ei=2,3,\\cdots,qitalic_i = 2 , 3 , \u22ef , italic_q. Hence X\ud835\udc4bXitalic_X stabilises a chain of subgroups of K\ud835\udc3eKitalic_K. By applying [4, Chapter A, Corollary 12.4], Op(X)superscript\ud835\udc42\ud835\udc5d\ud835\udc4bO^{p}(X)italic_O start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ( italic_X ) centralises K\ud835\udc3eKitalic_K. In particular, Op(X)superscript\ud835\udc42\ud835\udc5d\ud835\udc4bO^{p}(X)italic_O start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ( italic_X ) centralises every element of prime order or order 4444 of L\ud835\udc3fLitalic_L. By [5, Chapter IV, Satz 5.12], Op(X)superscript\ud835\udc42\ud835\udc5d\ud835\udc4bO^{p}(X)italic_O start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ( italic_X ) centralises L\ud835\udc3fLitalic_L. Thus X/CX(L/K)\ud835\udc4bsubscript\ud835\udc36\ud835\udc4b\ud835\udc3f\ud835\udc3eX/C_{X}(L/K)italic_X / italic_C start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ( italic_L / italic_K ) is a normal p\ud835\udc5dpitalic_p-subgroup of G/CX(L/K)\ud835\udc3asubscript\ud835\udc36\ud835\udc4b\ud835\udc3f\ud835\udc3eG/C_{X}(L/K)italic_G / italic_C start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ( italic_L / italic_K ). By [5, Chapter B, Proposition 3.12], X\ud835\udc4bXitalic_X centralizes L/K\ud835\udc3f\ud835\udc3eL/Kitalic_L / italic_K. This implies that L/K\ud835\udc3f\ud835\udc3eL/Kitalic_L / italic_K can be regarded as an irreducible G/X\ud835\udc3a\ud835\udc4bG/Xitalic_G / italic_X-module over the finite field of p\ud835\udc5dpitalic_p-elements. Note that every chief factor U/V\ud835\udc48\ud835\udc49U/Vitalic_U / italic_V of G\ud835\udc3aGitalic_G below K\ud835\udc3eKitalic_K is of order p\ud835\udc5dpitalic_p, hence we conclude that G/CG(U/V)\ud835\udc3asubscript\ud835\udc36\ud835\udc3a\ud835\udc48\ud835\udc49G/C_{G}(U/V)italic_G / italic_C start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_U / italic_V ) is cyclic of order dividing p\u22121\ud835\udc5d1p-1italic_p - 1. Consequently, G/X\ud835\udc3a\ud835\udc4bG/Xitalic_G / italic_X is abelian of exponent dividing p\u22121\ud835\udc5d1p-1italic_p - 1. By applying [4, Chapter B, Theorem 9.8], L/K\ud835\udc3f\ud835\udc3eL/Kitalic_L / italic_K has order p\ud835\udc5dpitalic_p. This final contradiction shows that no such counterexample G\ud835\udc3aGitalic_G exists. It follows from every cyclic chief factor of order 2222 in a chief series of a group is central that we have the following corollary: Let P\ud835\udc43Pitalic_P be a normal 2222-subgroup of G\ud835\udc3aGitalic_G. If every cyclic subgroup of P\ud835\udc43Pitalic_P of order 2222 or 4444 is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, then P\ud835\udc43Pitalic_P lies in the nilpotent hypercentre of G\ud835\udc3aGitalic_G, namely Z\u221e(G)subscript\ud835\udc4d\ud835\udc3aZ_{\\infty}(G)italic_Z start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT ( italic_G ). Since we have several useful lemmas and theorems above, we are now ready to prove our main results. Let p\ud835\udc5dpitalic_p be a prime and G\ud835\udc3aGitalic_G be a group of finite order. Suppose that every cyclic subgroup of G\ud835\udc3aGitalic_G with order p\ud835\udc5dpitalic_p or 4444 (if p=2\ud835\udc5d2p=2italic_p = 2) is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, then G\ud835\udc3aGitalic_G is p\ud835\udc5dpitalic_p-supersoluble. [Proof]Suppose that the theorem is false and let G\ud835\udc3aGitalic_G be a counterexample of minimal size. For the ease of reading we break the argument into separately stated steps. G\ud835\udc3aGitalic_G is a minimal non-p\ud835\udc5dpitalic_p-supersoluble group. Let M\ud835\udc40Mitalic_M be a maximal subgroup of G\ud835\udc3aGitalic_G, and L\ud835\udc3fLitalic_L be a subgroup of M\ud835\udc40Mitalic_M of order p\ud835\udc5dpitalic_p or 4(p=2)4\ud835\udc5d24\\,(p=2)4 ( italic_p = 2 ). By lemma 2.1(2), we conclude that L\ud835\udc3fLitalic_L is a strong p\ud835\udc5dpitalic_p-subgroup of M\ud835\udc40Mitalic_M, hence M\ud835\udc40Mitalic_M satisfies the assumption of the theorem. By the minimality of G\ud835\udc3aGitalic_G, M\ud835\udc40Mitalic_M is p\ud835\udc5dpitalic_p-supersoluble. It follows from the inheritance of p\ud835\udc5dpitalic_p-supersolvability that every proper subgroup of G\ud835\udc3aGitalic_G is p\ud835\udc5dpitalic_p-supersoluble, i.e. G\ud835\udc3aGitalic_G is a minimal non-p\ud835\udc5dpitalic_p-supersoluble group. Op\u2032(G)=1subscript\ud835\udc42superscript\ud835\udc5d\u2032\ud835\udc3a1O_{p^{{}^{\\prime}}}(G)=1italic_O start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT \u2032 end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_G ) = 1, and F(G)=Op(G)\ud835\udc39\ud835\udc3asubscript\ud835\udc42\ud835\udc5d\ud835\udc3aF(G)=O_{p}(G)italic_F ( italic_G ) = italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ). Set G\u00af=G/Op\u2032(G)normal-\u00af\ud835\udc3a\ud835\udc3asubscript\ud835\udc42superscript\ud835\udc5dnormal-\u2032\ud835\udc3a\\overline{G}=G/O_{p^{\\prime}}(G)over\u00af start_ARG italic_G end_ARG = italic_G / italic_O start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_G ). Suppose that L\u00afnormal-\u00af\ud835\udc3f\\overline{L}over\u00af start_ARG italic_L end_ARG is a cyclic subgroup of G\u00afnormal-\u00af\ud835\udc3a\\overline{G}over\u00af start_ARG italic_G end_ARG of order p\ud835\udc5dpitalic_p or 4(p=2)4\ud835\udc5d24\\,(p=2)4 ( italic_p = 2 ). Then we can write L\u00af=LOp\u2032(G)/Op\u2032(G)normal-\u00af\ud835\udc3f\ud835\udc3fsubscript\ud835\udc42superscript\ud835\udc5dnormal-\u2032\ud835\udc3asubscript\ud835\udc42superscript\ud835\udc5dnormal-\u2032\ud835\udc3a\\overline{L}=LO_{p^{\\prime}}(G)/O_{p^{\\prime}}(G)over\u00af start_ARG italic_L end_ARG = italic_L italic_O start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_G ) / italic_O start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_G ), where L\ud835\udc3fLitalic_L is a cyclic subgroup of G\ud835\udc3aGitalic_G of order p\ud835\udc5dpitalic_p or 4444. It follows from our assumption that L\ud835\udc3fLitalic_L is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. Since (|L|,|Op\u2032(G)|)=1\ud835\udc3fsubscript\ud835\udc42superscript\ud835\udc5dnormal-\u2032\ud835\udc3a1(|L|,|O_{p^{\\prime}}(G)|)=1( | italic_L | , | italic_O start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_G ) | ) = 1, we conclude from lemma 2.1(4) that LOp\u2032(G)/Op\u2032(G)=L\u00af\ud835\udc3fsubscript\ud835\udc42superscript\ud835\udc5dnormal-\u2032\ud835\udc3asubscript\ud835\udc42superscript\ud835\udc5dnormal-\u2032\ud835\udc3anormal-\u00af\ud835\udc3fLO_{p^{\\prime}}(G)/O_{p^{\\prime}}(G)=\\overline{L}italic_L italic_O start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_G ) / italic_O start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_G ) = over\u00af start_ARG italic_L end_ARG is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\u00afnormal-\u00af\ud835\udc3a\\overline{G}over\u00af start_ARG italic_G end_ARG. Therefore G\u00afnormal-\u00af\ud835\udc3a\\overline{G}over\u00af start_ARG italic_G end_ARG satisfies the hypothesis of the theorem. Suppose that Op\u2032(G)>1subscript\ud835\udc42superscript\ud835\udc5dnormal-\u2032\ud835\udc3a1O_{p^{\\prime}}(G)>1italic_O start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_G ) > 1, then G\u00afnormal-\u00af\ud835\udc3a\\overline{G}over\u00af start_ARG italic_G end_ARG is p\ud835\udc5dpitalic_p-supersoluble by our choice of G\ud835\udc3aGitalic_G. Then by inheritance of p\ud835\udc5dpitalic_p-supersolvability we get that G\ud835\udc3aGitalic_G is p\ud835\udc5dpitalic_p-supersoluble, which contradicts the fact that G\ud835\udc3aGitalic_G is not p\ud835\udc5dpitalic_p-supersoluble and step 2 has finished. F(G)\u2a7dZ\ud835\udd18(G)\ud835\udc39\ud835\udc3asubscript\ud835\udc4d\ud835\udd18\ud835\udc3aF(G)\\leqslant Z_{\\mathfrak{U}}(G)italic_F ( italic_G ) \u2a7d italic_Z start_POSTSUBSCRIPT fraktur_U end_POSTSUBSCRIPT ( italic_G ). Since F(G)=Op(G)\ud835\udc39\ud835\udc3asubscript\ud835\udc42\ud835\udc5d\ud835\udc3aF(G)=O_{p}(G)italic_F ( italic_G ) = italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ), a normal p\ud835\udc5dpitalic_p-subgroup of G\ud835\udc3aGitalic_G, by theorem 2.4, it indicates that Op(G)\u2a7dZ\ud835\udd18(G)subscript\ud835\udc42\ud835\udc5d\ud835\udc3asubscript\ud835\udc4d\ud835\udd18\ud835\udc3aO_{p}(G)\\leqslant Z_{\\mathfrak{U}}(G)italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) \u2a7d italic_Z start_POSTSUBSCRIPT fraktur_U end_POSTSUBSCRIPT ( italic_G ). G=F*(G)\ud835\udc3asuperscript\ud835\udc39\ud835\udc3aG=F^{*}(G)italic_G = italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_G ). Suppose that F*(G)<Gsuperscript\ud835\udc39\ud835\udc3a\ud835\udc3aF^{*}(G)<Gitalic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_G ) < italic_G. Then by the p\ud835\udc5dpitalic_p-supersolvability of F*(G)superscript\ud835\udc39\ud835\udc3aF^{*}(G)italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_G ) we get that F*(G)=F(G)=Op(G)superscript\ud835\udc39\ud835\udc3a\ud835\udc39\ud835\udc3asubscript\ud835\udc42\ud835\udc5d\ud835\udc3aF^{*}(G)=F(G)=O_{p}(G)italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_G ) = italic_F ( italic_G ) = italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ). By step 3 we conclude that F(G)\ud835\udc39\ud835\udc3aF(G)italic_F ( italic_G ) is \ud835\udd18\ud835\udd18\\mathfrak{U}fraktur_U-hypercentral in G\ud835\udc3aGitalic_G. It follows from [4, Chapter IV, Theorem 6.10] that [G\ud835\udd18,F(G)]=1superscript\ud835\udc3a\ud835\udd18\ud835\udc39\ud835\udc3a1[G^{\\mathfrak{U}},F(G)]=1[ italic_G start_POSTSUPERSCRIPT fraktur_U end_POSTSUPERSCRIPT , italic_F ( italic_G ) ] = 1, i.e. G/CG(F(G))\u2208\ud835\udd18\ud835\udc3asubscript\ud835\udc36\ud835\udc3a\ud835\udc39\ud835\udc3a\ud835\udd18G/C_{G}(F(G))\\in\\mathfrak{U}italic_G / italic_C start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_F ( italic_G ) ) \u2208 fraktur_U. By CG(F*(G))\u2a7dF*(G)subscript\ud835\udc36\ud835\udc3asuperscript\ud835\udc39\ud835\udc3asuperscript\ud835\udc39\ud835\udc3aC_{G}(F^{*}(G))\\leqslant F^{*}(G)italic_C start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_G ) ) \u2a7d italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_G ) and inheritance of supersolvability, we have that G/F(G)\u2208\ud835\udd18\ud835\udc3a\ud835\udc39\ud835\udc3a\ud835\udd18G/F(G)\\in\\mathfrak{U}italic_G / italic_F ( italic_G ) \u2208 fraktur_U. Since every G\ud835\udc3aGitalic_G-chief factor below F(G)\ud835\udc39\ud835\udc3aF(G)italic_F ( italic_G ) is of order p\ud835\udc5dpitalic_p, it follows that G\ud835\udc3aGitalic_G is p\ud835\udc5dpitalic_p-supersoluble, a contradiction. Hence it yields that G=F*(G)\ud835\udc3asuperscript\ud835\udc39\ud835\udc3aG=F^{*}(G)italic_G = italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_G ). G/Z(G)\ud835\udc3a\ud835\udc4d\ud835\udc3aG/Z(G)italic_G / italic_Z ( italic_G ) is non-abelian simple and G\ud835\udc3aGitalic_G is perfect. By step 4 we conclude that G=F*(G)=F(G)E(G)\ud835\udc3asuperscript\ud835\udc39\ud835\udc3a\ud835\udc39\ud835\udc3a\ud835\udc38\ud835\udc3aG=F^{*}(G)=F(G)E(G)italic_G = italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_G ) = italic_F ( italic_G ) italic_E ( italic_G ). Since G\ud835\udc3aGitalic_G is not nilpotent, it follows that E(G)\ud835\udc38\ud835\udc3aE(G)italic_E ( italic_G ) is not contained in F(G)\ud835\udc39\ud835\udc3aF(G)italic_F ( italic_G ). Hence we can find a component H\ud835\udc3bHitalic_H of G\ud835\udc3aGitalic_G. Then H\ud835\udc3bHitalic_H is normal in F*(G)=Gsuperscript\ud835\udc39\ud835\udc3a\ud835\udc3aF^{*}(G)=Gitalic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_G ) = italic_G, and H/Z(H)\ud835\udc3b\ud835\udc4d\ud835\udc3bH/Z(H)italic_H / italic_Z ( italic_H ) is non-abelian simple. By step 2, it indicates that p||H|p\\,|\\,|H|italic_p | | italic_H |. In particular, H\ud835\udc3bHitalic_H is not p\ud835\udc5dpitalic_p-supersoluble, hence we conclude that H=G\ud835\udc3b\ud835\udc3aH=Gitalic_H = italic_G, i.e. G/Z(G)\ud835\udc3a\ud835\udc4d\ud835\udc3aG/Z(G)italic_G / italic_Z ( italic_G ) is non-abelian simple and G\ud835\udc3aGitalic_G is perfect. Final contradiction. Let M\ud835\udc40Mitalic_M be a maximal subgroup of G\ud835\udc3aGitalic_G which does not contain Z(G)\ud835\udc4d\ud835\udc3aZ(G)italic_Z ( italic_G ). Then MZ(G)=G\ud835\udc40\ud835\udc4d\ud835\udc3a\ud835\udc3aMZ(G)=Gitalic_M italic_Z ( italic_G ) = italic_G and it yields that M\\ThisStyle\\stackinsetr.75pt+.15\\LMptt.1\\LMpt \\SavedStyle\u2a7dG\\stackinset\ud835\udc5f.75\ud835\udc5d\ud835\udc61.15\\LMpt\ud835\udc61.1\\LMpt \\SavedStyleabsent\ud835\udc40\\ThisStyle\ud835\udc3aM\\ThisStyle{\\mathrel{\\stackinset{r}{.75pt+.15\\LMpt}{t}{.1\\LMpt}{\\rule{0.3pt}{1% .1pt}}{\\SavedStyle\\leqslant}}}Gitalic_M start_RELOP italic_r .75 italic_p italic_t + .15 italic_t .1 italic_\\SavedStyle \u2a7d end_RELOP italic_G. Since G/M=Z(G)M/M\u2245Z(G)/M\u2229Z(G)\ud835\udc3a\ud835\udc40\ud835\udc4d\ud835\udc3a\ud835\udc40\ud835\udc40\ud835\udc4d\ud835\udc3a\ud835\udc40\ud835\udc4d\ud835\udc3aG/M=Z(G)M/M\\cong Z(G)/M\\cap Z(G)italic_G / italic_M = italic_Z ( italic_G ) italic_M / italic_M \u2245 italic_Z ( italic_G ) / italic_M \u2229 italic_Z ( italic_G ), which is abelian, it follows that G\u2032\u2a7dMsuperscript\ud835\udc3anormal-\u2032\ud835\udc40G^{{}^{\\prime}}\\leqslant Mitalic_G start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT \u2032 end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT \u2a7d italic_M, contradicting the fact that G\ud835\udc3aGitalic_G is perfect. Thus no such M\ud835\udc40Mitalic_M exists and Z(G)\u2a7d\u03a6(G)\ud835\udc4d\ud835\udc3anormal-\u03a6\ud835\udc3aZ(G)\\leqslant\\Phi(G)italic_Z ( italic_G ) \u2a7d roman_\u03a6 ( italic_G ). Now let G/A\ud835\udc3a\ud835\udc34G/Aitalic_G / italic_A be a chief factor of G\ud835\udc3aGitalic_G. Since A\\ThisStyle\\stackinsetr.75pt+.15\\LMptt.1\\LMpt \\SavedStyle\u2a7dG\\stackinset\ud835\udc5f.75\ud835\udc5d\ud835\udc61.15\\LMpt\ud835\udc61.1\\LMpt \\SavedStyleabsent\ud835\udc34\\ThisStyle\ud835\udc3aA\\ThisStyle{\\mathrel{\\stackinset{r}{.75pt+.15\\LMpt}{t}{.1\\LMpt}{\\rule{0.3pt}{1% .1pt}}{\\SavedStyle\\leqslant}}}Gitalic_A start_RELOP italic_r .75 italic_p italic_t + .15 italic_t .1 italic_\\SavedStyle \u2a7d end_RELOP italic_G, it follows that A\u2a7dZ(G)\ud835\udc34\ud835\udc4d\ud835\udc3aA\\leqslant Z(G)italic_A \u2a7d italic_Z ( italic_G ), therefore we have that A=Z(G)\ud835\udc34\ud835\udc4d\ud835\udc3aA=Z(G)italic_A = italic_Z ( italic_G ). Hence Z(G)\ud835\udc4d\ud835\udc3aZ(G)italic_Z ( italic_G ) belongs to all chief series of G\ud835\udc3aGitalic_G. If p||G/Z(G)|p\\,|\\,|G/Z(G)|italic_p | | italic_G / italic_Z ( italic_G ) |, let x\ud835\udc65xitalic_x be an element of order p\ud835\udc5dpitalic_p or 4(p=2)4\ud835\udc5d24\\,(p=2)4 ( italic_p = 2 ). It follows from our assumption that \u27e8x\u27e9delimited-\u27e8\u27e9\ud835\udc65\\langle x\\rangle\u27e8 italic_x \u27e9 is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. Then \u27e8x\u27e9delimited-\u27e8\u27e9\ud835\udc65\\langle x\\rangle\u27e8 italic_x \u27e9 covers or avoids G/Z(G)\ud835\udc3a\ud835\udc4d\ud835\udc3aG/Z(G)italic_G / italic_Z ( italic_G ). If G=G\u27e8x\u27e9=Z(G)\u27e8x\u27e9\ud835\udc3a\ud835\udc3adelimited-\u27e8\u27e9\ud835\udc65\ud835\udc4d\ud835\udc3adelimited-\u27e8\u27e9\ud835\udc65G=G\\langle x\\rangle=Z(G)\\langle x\\rangleitalic_G = italic_G \u27e8 italic_x \u27e9 = italic_Z ( italic_G ) \u27e8 italic_x \u27e9, it follows that G/Z(G)\ud835\udc3a\ud835\udc4d\ud835\udc3aG/Z(G)italic_G / italic_Z ( italic_G ) is abelian, a contradiction to the fact that G=H\ud835\udc3a\ud835\udc3bG=Hitalic_G = italic_H is semi-simple. Hence we have that G\u2229\u27e8x\u27e9=\u27e8x\u27e9=Z(G)\u2229\u27e8x\u27e9\ud835\udc3adelimited-\u27e8\u27e9\ud835\udc65delimited-\u27e8\u27e9\ud835\udc65\ud835\udc4d\ud835\udc3adelimited-\u27e8\u27e9\ud835\udc65G\\cap\\langle x\\rangle=\\langle x\\rangle=Z(G)\\cap\\langle x\\rangleitalic_G \u2229 \u27e8 italic_x \u27e9 = \u27e8 italic_x \u27e9 = italic_Z ( italic_G ) \u2229 \u27e8 italic_x \u27e9, i.e. x\u2208Z(G)\ud835\udc65\ud835\udc4d\ud835\udc3ax\\in Z(G)italic_x \u2208 italic_Z ( italic_G ). By the randomness of x\ud835\udc65xitalic_x, we conclude from [5, Chapter IV, Satz 5.5] that G\ud835\udc3aGitalic_G is p\ud835\udc5dpitalic_p-nilpotent, contradicting the fact that G\ud835\udc3aGitalic_G is not p\ud835\udc5dpitalic_p-supersoluble. Thus it yields that p\u2224|G/Z(G)|not-divides\ud835\udc5d\ud835\udc3a\ud835\udc4d\ud835\udc3ap\\nmid|G/Z(G)|italic_p \u2224 | italic_G / italic_Z ( italic_G ) |. If Z(G)\ud835\udc4d\ud835\udc3aZ(G)italic_Z ( italic_G ) has non-trivial p\u2032superscript\ud835\udc5dnormal-\u2032p^{\\prime}italic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT-part, it follows that Op\u2032(G)>1subscript\ud835\udc42superscript\ud835\udc5dnormal-\u2032\ud835\udc3a1O_{p^{\\prime}}(G)>1italic_O start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_G ) > 1, a contradiction to step 2. Thus Z(G)\ud835\udc4d\ud835\udc3aZ(G)italic_Z ( italic_G ) is a p\ud835\udc5dpitalic_p-group and then Z(G)\u2a7dOp(G)=F(G)\ud835\udc4d\ud835\udc3asubscript\ud835\udc42\ud835\udc5d\ud835\udc3a\ud835\udc39\ud835\udc3aZ(G)\\leqslant O_{p}(G)=F(G)italic_Z ( italic_G ) \u2a7d italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) = italic_F ( italic_G ). However, since G\ud835\udc3aGitalic_G is semi-simple, it indicates from F(G)\\ThisStyle\\stackinsetr.75pt+.15\\LMptt.1\\LMpt \\SavedStyle\u2a7dG\\stackinset\ud835\udc5f.75\ud835\udc5d\ud835\udc61.15\\LMpt\ud835\udc61.1\\LMpt \\SavedStyleabsent\ud835\udc39\ud835\udc3a\\ThisStyle\ud835\udc3aF(G)\\ThisStyle{\\mathrel{\\stackinset{r}{.75pt+.15\\LMpt}{t}{.1\\LMpt}{\\rule{0.3pt% }{1.1pt}}{\\SavedStyle\\leqslant}}}Gitalic_F ( italic_G ) start_RELOP italic_r .75 italic_p italic_t + .15 italic_t .1 italic_\\SavedStyle \u2a7d end_RELOP italic_G that F(G)\u2a7dZ(G)\ud835\udc39\ud835\udc3a\ud835\udc4d\ud835\udc3aF(G)\\leqslant Z(G)italic_F ( italic_G ) \u2a7d italic_Z ( italic_G ). Thus we get that F(G)=Z(G)\ud835\udc39\ud835\udc3a\ud835\udc4d\ud835\udc3aF(G)=Z(G)italic_F ( italic_G ) = italic_Z ( italic_G ). By step 2, we conclude that every G\ud835\udc3aGitalic_G-chief factor below Z(G)\ud835\udc4d\ud835\udc3aZ(G)italic_Z ( italic_G ) is of order p\ud835\udc5dpitalic_p. It follows that G\ud835\udc3aGitalic_G is p\ud835\udc5dpitalic_p-supersoluble, a contradiction. Finally we have that no such counterexample of G\ud835\udc3aGitalic_G exists and we are done. Let \ud835\udd09\ud835\udd09\\mathfrak{F}fraktur_F be a saturated formation containing \ud835\udd18\ud835\udd18\\mathfrak{U}fraktur_U, and let G\ud835\udc3aGitalic_G be a group with a normal subgroup H\ud835\udc3bHitalic_H such that G/H\u2208\ud835\udd09\ud835\udc3a\ud835\udc3b\ud835\udd09G/H\\in\\mathfrak{F}italic_G / italic_H \u2208 fraktur_F. If for any p\u2208\u03c0(G)\ud835\udc5d\ud835\udf0b\ud835\udc3ap\\in\\pi(G)italic_p \u2208 italic_\u03c0 ( italic_G ), every cyclic subgroup of F*(H)superscript\ud835\udc39\ud835\udc3bF^{*}(H)italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_H ) of order p\ud835\udc5dpitalic_p or 4444 (if p=2\ud835\udc5d2p=2italic_p = 2) is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, then G\u2208\ud835\udd09\ud835\udc3a\ud835\udd09G\\in\\mathfrak{F}italic_G \u2208 fraktur_F. [Proof]We assume that H>1\ud835\udc3b1H>1italic_H > 1, and p\ud835\udc5dpitalic_p be a prime such that p||F*(H)|p\\,|\\,|F^{*}(H)|italic_p | | italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_H ) |. By our assumption, lemma 2.1(2) and theorem 3.1 that F*(H)superscript\ud835\udc39\ud835\udc3bF^{*}(H)italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_H ) is p\ud835\udc5dpitalic_p-supersoluble for all primes p||F*(H)|p\\,|\\,|F^{*}(H)|italic_p | | italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_H ) |. Therefore F*(H)superscript\ud835\udc39\ud835\udc3bF^{*}(H)italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_H ) is supersoluble and so F*(H)=F(H)superscript\ud835\udc39\ud835\udc3b\ud835\udc39\ud835\udc3bF^{*}(H)=F(H)italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_H ) = italic_F ( italic_H ). By definition of F(H)\ud835\udc39\ud835\udc3bF(H)italic_F ( italic_H ) and theorem 2.4, we conclude that F*(H)=F(H)\u2a7dZ\ud835\udd18(G)superscript\ud835\udc39\ud835\udc3b\ud835\udc39\ud835\udc3bsubscript\ud835\udc4d\ud835\udd18\ud835\udc3aF^{*}(H)=F(H)\\leqslant Z_{\\mathfrak{U}}(G)italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_H ) = italic_F ( italic_H ) \u2a7d italic_Z start_POSTSUBSCRIPT fraktur_U end_POSTSUBSCRIPT ( italic_G ). By [4, Chapter IV, Theorem 6.10], it follows that G/CG(F(H))\u2208\ud835\udd18\ud835\udc3asubscript\ud835\udc36\ud835\udc3a\ud835\udc39\ud835\udc3b\ud835\udd18G/C_{G}(F(H))\\in\\mathfrak{U}italic_G / italic_C start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_F ( italic_H ) ) \u2208 fraktur_U. Since G/H\u2208\ud835\udd09\ud835\udc3a\ud835\udc3b\ud835\udd09G/H\\in\\mathfrak{F}italic_G / italic_H \u2208 fraktur_F and \ud835\udd09\u2287\ud835\udd18\ud835\udd18\ud835\udd09\\mathfrak{F}\\supseteq\\mathfrak{U}fraktur_F \u2287 fraktur_U, it yields that G/CH(F(H))\u2208\ud835\udd09\ud835\udc3asubscript\ud835\udc36\ud835\udc3b\ud835\udc39\ud835\udc3b\ud835\udd09G/C_{H}(F(H))\\in\\mathfrak{F}italic_G / italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_F ( italic_H ) ) \u2208 fraktur_F. Since CH(F*(H))\u2a7dF*(H)subscript\ud835\udc36\ud835\udc3bsuperscript\ud835\udc39\ud835\udc3bsuperscript\ud835\udc39\ud835\udc3bC_{H}(F^{*}(H))\\leqslant F^{*}(H)italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_H ) ) \u2a7d italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_H ), it implies from the inheritance of \ud835\udd09\ud835\udd09\\mathfrak{F}fraktur_F that G/F*(H)=G/F(H)\u2208\ud835\udd09\ud835\udc3asuperscript\ud835\udc39\ud835\udc3b\ud835\udc3a\ud835\udc39\ud835\udc3b\ud835\udd09G/F^{*}(H)=G/F(H)\\in\\mathfrak{F}italic_G / italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_H ) = italic_G / italic_F ( italic_H ) \u2208 fraktur_F. Since any G\ud835\udc3aGitalic_G-chief factor below F(H)\u2a7dZ\ud835\udd18(G)\ud835\udc39\ud835\udc3bsubscript\ud835\udc4d\ud835\udd18\ud835\udc3aF(H)\\leqslant Z_{\\mathfrak{U}}(G)italic_F ( italic_H ) \u2a7d italic_Z start_POSTSUBSCRIPT fraktur_U end_POSTSUBSCRIPT ( italic_G ) is of prime order, and \ud835\udd09\ud835\udd09\\mathfrak{F}fraktur_F contains \ud835\udd18\ud835\udd18\\mathfrak{U}fraktur_U, we get that G\u2208\ud835\udd09\ud835\udc3a\ud835\udd09G\\in\\mathfrak{F}italic_G \u2208 fraktur_F and we are done. In this section, we investigate the structure of \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) under the assumption that any cyclic subgroup of S\ud835\udc46Sitalic_S with order p\ud835\udc5dpitalic_p or 4444 (if p\ud835\udc5dpitalic_p=2) is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, and prove the Theorem 1.7. The proof of the theorem strongly relies on the proceeding results we have obtained in Section 3. [Proof of Theorem 1.7] Assume that the theorem is false, and let G\ud835\udc3aGitalic_G be a counterexample of minimal order. Now denote \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) by \u2131\u2131\\mathcal{F}caligraphic_F. Let H\ud835\udc3bHitalic_H be a proper subgroup of G\ud835\udc3aGitalic_G such that S\u2229H\u2208Sylp(H)\ud835\udc46\ud835\udc3bsubscriptSyl\ud835\udc5d\ud835\udc3bS\\cap H\\in{\\rm Syl}_{p}(H)italic_S \u2229 italic_H \u2208 roman_Syl start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_H ) and |S\u2229H|\u2a7ep2\ud835\udc46\ud835\udc3bsuperscript\ud835\udc5d2|S\\cap H|\\geqslant p^{2}| italic_S \u2229 italic_H | \u2a7e italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Then \u2131S\u2229H(H)subscript\u2131\ud835\udc46\ud835\udc3b\ud835\udc3b\\mathcal{F}_{S\\cap H}(H)caligraphic_F start_POSTSUBSCRIPT italic_S \u2229 italic_H end_POSTSUBSCRIPT ( italic_H ) is supersolvable. By our hypothesis, every cyclic subgroup T\ud835\udc47Titalic_T of S\u2229H\ud835\udc46\ud835\udc3bS\\cap Hitalic_S \u2229 italic_H with order p\ud835\udc5dpitalic_p or 4444 (If p=2\ud835\udc5d2p=2italic_p = 2) is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. Then every cyclic subgroup T\ud835\udc47Titalic_T of S\u2229H\ud835\udc46\ud835\udc3bS\\cap Hitalic_S \u2229 italic_H with order p\ud835\udc5dpitalic_p or 4444 (If p=2\ud835\udc5d2p=2italic_p = 2) is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of H\ud835\udc3bHitalic_H by lemma 2.2 (2). Hence H\ud835\udc3bHitalic_H satisfies the hypothesis of the theorem and it follows from the minimal choice of G\ud835\udc3aGitalic_G that \u2131S\u2229H(H)subscript\u2131\ud835\udc46\ud835\udc3b\ud835\udc3b\\mathcal{F}_{S\\cap H}(H)caligraphic_F start_POSTSUBSCRIPT italic_S \u2229 italic_H end_POSTSUBSCRIPT ( italic_H ) is supersolvable. Let Q\u2208\u2130\u2131*\ud835\udc44superscriptsubscript\u2130\u2131Q\\in\\mathcal{E}_{\\mathcal{F}}^{*}italic_Q \u2208 caligraphic_E start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, then |Q|\u2a7ep2\ud835\udc44superscript\ud835\udc5d2|Q|\\geqslant p^{2}| italic_Q | \u2a7e italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. If moreover that Q\\ThisStyle\\stackinsetr.75pt+.15\\LMptt.1\\LMpt \\SavedStyle\u2a7dG\\stackinset\ud835\udc5f.75\ud835\udc5d\ud835\udc61.15\\LMpt\ud835\udc61.1\\LMpt \\SavedStyleabsent\ud835\udc44\\ThisStyle\ud835\udc3aQ\\not\\ThisStyle{\\mathrel{\\stackinset{r}{.75pt+.15\\LMpt}{t}{.1\\LMpt}{\\rule{0.3% pt}{1.1pt}}{\\SavedStyle\\leqslant}}}Gitalic_Q not start_RELOP italic_r .75 italic_p italic_t + .15 italic_t .1 italic_\\SavedStyle \u2a7d end_RELOP italic_G, then N\u2131(Q)subscript\ud835\udc41\u2131\ud835\udc44N_{\\mathcal{F}}(Q)italic_N start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT ( italic_Q ) is supersolvable. Suppose that there exists a subgroup Q\u2208\u2130\u2131*\ud835\udc44superscriptsubscript\u2130\u2131Q\\in\\mathcal{E}_{\\mathcal{F}}^{*}italic_Q \u2208 caligraphic_E start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT such that |Q|<p2\ud835\udc44superscript\ud835\udc5d2|Q|<p^{2}| italic_Q | < italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Then there is a subgroup R\ud835\udc45Ritalic_R of S\ud835\udc46Sitalic_S such that |R|=p\ud835\udc45\ud835\udc5d|R|=p| italic_R | = italic_p, and Q<R\ud835\udc44\ud835\udc45Q<Ritalic_Q < italic_R. It follows directly that R\u2a7dCS(Q)\ud835\udc45subscript\ud835\udc36\ud835\udc46\ud835\udc44R\\leqslant C_{S}(Q)italic_R \u2a7d italic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_Q ). Since Q<R\u2a7dS\ud835\udc44\ud835\udc45\ud835\udc46Q<R\\leqslant Sitalic_Q < italic_R \u2a7d italic_S, we conclude from Q\ud835\udc44Qitalic_Q is a member of \u2130\u2131*superscriptsubscript\u2130\u2131\\mathcal{E}_{\\mathcal{F}}^{*}caligraphic_E start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT that Q\ud835\udc44Qitalic_Q is \u2131\u2131\\mathcal{F}caligraphic_F-essential. By the definition, Q\ud835\udc44Qitalic_Q is \u2131\u2131\\mathcal{F}caligraphic_F-centric. Hence R\u2a7dCS(Q)=Z(Q)\u2a7dQ\ud835\udc45subscript\ud835\udc36\ud835\udc46\ud835\udc44\ud835\udc4d\ud835\udc44\ud835\udc44R\\leqslant C_{S}(Q)=Z(Q)\\leqslant Qitalic_R \u2a7d italic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_Q ) = italic_Z ( italic_Q ) \u2a7d italic_Q, a contradiction. Thus |Q|\u2a7ep2\ud835\udc44superscript\ud835\udc5d2|Q|\\geqslant p^{2}| italic_Q | \u2a7e italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Assume that Q\ud835\udc44Qitalic_Q is not normal in G\ud835\udc3aGitalic_G. Therefore NG(Q)subscript\ud835\udc41\ud835\udc3a\ud835\udc44N_{G}(Q)italic_N start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Q ) is a proper subgroup of G\ud835\udc3aGitalic_G. Since Q\u2208\u2130\u2131*\ud835\udc44superscriptsubscript\u2130\u2131Q\\in\\mathcal{E}_{\\mathcal{F}}^{*}italic_Q \u2208 caligraphic_E start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, Q\ud835\udc44Qitalic_Q is fully \u2131\u2131\\mathcal{F}caligraphic_F-normalized or Q=S\ud835\udc44\ud835\udc46Q=Sitalic_Q = italic_S. Clearly S\ud835\udc46Sitalic_S is fully \u2131\u2131\\mathcal{F}caligraphic_F-normalized, hence Q\ud835\udc44Qitalic_Q is always fully \u2131\u2131\\mathcal{F}caligraphic_F-normalized. By the argument below [1, Definition 2.4], S\u2229NG(Q)=NS(Q)\u2208Sylp(NG(Q))\ud835\udc46subscript\ud835\udc41\ud835\udc3a\ud835\udc44subscript\ud835\udc41\ud835\udc46\ud835\udc44subscriptnormal-Syl\ud835\udc5dsubscript\ud835\udc41\ud835\udc3a\ud835\udc44S\\cap N_{G}(Q)=N_{S}(Q)\\in{\\rm Syl}_{p}(N_{G}(Q))italic_S \u2229 italic_N start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Q ) = italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_Q ) \u2208 roman_Syl start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_N start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Q ) ). Since |NS(Q)|\u2a7e|Q|\u2a7ep2subscript\ud835\udc41\ud835\udc46\ud835\udc44\ud835\udc44superscript\ud835\udc5d2|N_{S}(Q)|\\geqslant|Q|\\geqslant p^{2}| italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_Q ) | \u2a7e | italic_Q | \u2a7e italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, it yields that NG(Q)subscript\ud835\udc41\ud835\udc3a\ud835\udc44N_{G}(Q)italic_N start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Q ) satisfies the hypothesis of Step 1, and so \u2131NS(Q)(NG(Q))=N\u2131(Q)subscript\u2131subscript\ud835\udc41\ud835\udc46\ud835\udc44subscript\ud835\udc41\ud835\udc3a\ud835\udc44subscript\ud835\udc41\u2131\ud835\udc44\\mathcal{F}_{N_{S}(Q)}(N_{G}(Q))=N_{\\mathcal{F}}(Q)caligraphic_F start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_Q ) end_POSTSUBSCRIPT ( italic_N start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Q ) ) = italic_N start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT ( italic_Q ) is supersolvable. |Op(G)|\u2a7ep2subscript\ud835\udc42\ud835\udc5d\ud835\udc3asuperscript\ud835\udc5d2|O_{p}(G)|\\geqslant p^{2}| italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) | \u2a7e italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Assume that there does not exist a subgroup Q\u2208\u2130\u2131*\ud835\udc44superscriptsubscript\u2130\u2131Q\\in\\mathcal{E}_{\\mathcal{F}}^{*}italic_Q \u2208 caligraphic_E start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT such that Q\\ThisStyle\\stackinsetr.75pt+.15\\LMptt.1\\LMpt \\SavedStyle\u2a7dG\\stackinset\ud835\udc5f.75\ud835\udc5d\ud835\udc61.15\\LMpt\ud835\udc61.1\\LMpt \\SavedStyleabsent\ud835\udc44\\ThisStyle\ud835\udc3aQ\\ThisStyle{\\mathrel{\\stackinset{r}{.75pt+.15\\LMpt}{t}{.1\\LMpt}{\\rule{0.3pt}{1% .1pt}}{\\SavedStyle\\leqslant}}}Gitalic_Q start_RELOP italic_r .75 italic_p italic_t + .15 italic_t .1 italic_\\SavedStyle \u2a7d end_RELOP italic_G. Then for each Q\u2208\u2130\u2131*\ud835\udc44superscriptsubscript\u2130\u2131Q\\in\\mathcal{E}_{\\mathcal{F}}^{*}italic_Q \u2208 caligraphic_E start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, the fusion system N\u2131(Q)subscript\ud835\udc41\u2131\ud835\udc44N_{\\mathcal{F}}(Q)italic_N start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT ( italic_Q ) is supersolvable by Step 2. By Lemma 2.2, \u2131\u2131\\mathcal{F}caligraphic_F is supersolvable, a contradiction. Thus there exists a subgroup Q\u2208\u2130\u2131*\ud835\udc44superscriptsubscript\u2130\u2131Q\\in\\mathcal{E}_{\\mathcal{F}}^{*}italic_Q \u2208 caligraphic_E start_POSTSUBSCRIPT caligraphic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT such that Q\\ThisStyle\\stackinsetr.75pt+.15\\LMptt.1\\LMpt \\SavedStyle\u2a7dG\\stackinset\ud835\udc5f.75\ud835\udc5d\ud835\udc61.15\\LMpt\ud835\udc61.1\\LMpt \\SavedStyleabsent\ud835\udc44\\ThisStyle\ud835\udc3aQ\\ThisStyle{\\mathrel{\\stackinset{r}{.75pt+.15\\LMpt}{t}{.1\\LMpt}{\\rule{0.3pt}{1% .1pt}}{\\SavedStyle\\leqslant}}}Gitalic_Q start_RELOP italic_r .75 italic_p italic_t + .15 italic_t .1 italic_\\SavedStyle \u2a7d end_RELOP italic_G. Hence we conclude from Step 2 that |Op(G)|\u2a7e|Q|\u2a7ep2subscript\ud835\udc42\ud835\udc5d\ud835\udc3a\ud835\udc44superscript\ud835\udc5d2|O_{p}(G)|\\geqslant|Q|\\geqslant p^{2}| italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) | \u2a7e | italic_Q | \u2a7e italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Op(G)\u2a7dZ\ud835\udd18(G)subscript\ud835\udc42\ud835\udc5d\ud835\udc3asubscript\ud835\udc4d\ud835\udd18\ud835\udc3aO_{p}(G)\\leqslant Z_{\\mathfrak{U}}(G)italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) \u2a7d italic_Z start_POSTSUBSCRIPT fraktur_U end_POSTSUBSCRIPT ( italic_G ). It follows from |Op(G)|\u2a7ep2subscript\ud835\udc42\ud835\udc5d\ud835\udc3asuperscript\ud835\udc5d2|O_{p}(G)|\\geqslant p^{2}| italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) | \u2a7e italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT that any subgroup T\ud835\udc47Titalic_T of Op(G)subscript\ud835\udc42\ud835\udc5d\ud835\udc3aO_{p}(G)italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) of order p\ud835\udc5dpitalic_p or any cyclic subgroup T\ud835\udc47Titalic_T of Op(G)subscript\ud835\udc42\ud835\udc5d\ud835\udc3aO_{p}(G)italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) of order 4444 (If p=2\ud835\udc5d2p=2italic_p = 2) is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. Then it yields from Theorem 2.3 that Op(G)\u2a7dZ\ud835\udd18(G)subscript\ud835\udc42\ud835\udc5d\ud835\udc3asubscript\ud835\udc4d\ud835\udd18\ud835\udc3aO_{p}(G)\\leqslant Z_{\\mathfrak{U}}(G)italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) \u2a7d italic_Z start_POSTSUBSCRIPT fraktur_U end_POSTSUBSCRIPT ( italic_G ) and this part is complete. Final contradiction. Suppose that H\ud835\udc3bHitalic_H is a proper subgroup of G\ud835\udc3aGitalic_G such that Op(G)<S\u2229Hsubscript\ud835\udc42\ud835\udc5d\ud835\udc3a\ud835\udc46\ud835\udc3bO_{p}(G)<S\\cap Hitalic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) < italic_S \u2229 italic_H and S\u2229H\u2208Sylp(H)\ud835\udc46\ud835\udc3bsubscriptnormal-Syl\ud835\udc5d\ud835\udc3bS\\cap H\\in{\\rm Syl}_{p}(H)italic_S \u2229 italic_H \u2208 roman_Syl start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_H ). By Step 1 and Step 3, |S\u2229H|>|Op(G)|\u2a7ep2\ud835\udc46\ud835\udc3bsubscript\ud835\udc42\ud835\udc5d\ud835\udc3asuperscript\ud835\udc5d2|S\\cap H|>|O_{p}(G)|\\geqslant p^{2}| italic_S \u2229 italic_H | > | italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) | \u2a7e italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and so \u2131S\u2229H(H)subscript\u2131\ud835\udc46\ud835\udc3b\ud835\udc3b\\mathcal{F}_{S\\cap H}(H)caligraphic_F start_POSTSUBSCRIPT italic_S \u2229 italic_H end_POSTSUBSCRIPT ( italic_H ) is supersolvable. Since Op(G)\u2a7dZ\ud835\udd18(G)subscript\ud835\udc42\ud835\udc5d\ud835\udc3asubscript\ud835\udc4d\ud835\udd18\ud835\udc3aO_{p}(G)\\leqslant Z_{\\mathfrak{U}}(G)italic_O start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_G ) \u2a7d italic_Z start_POSTSUBSCRIPT fraktur_U end_POSTSUBSCRIPT ( italic_G ) by Step 4, it follows directly from Lemma 2.3 that \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) is supersolvable, a contradiction. Hence our proof is complete. As a direct application of the theorem above, we obtain the following characterization for the structure of finite groups under the assumption that any cyclic subgroup of S\ud835\udc46Sitalic_S with order p\ud835\udc5dpitalic_p or 4444 (if p\ud835\udc5dpitalic_p=2) is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G. Let G\ud835\udc3aGitalic_G be a finite group and S\ud835\udc46Sitalic_S a Sylow p\ud835\udc5dpitalic_p-subgroup of G\ud835\udc3aGitalic_G, where p\ud835\udc5dpitalic_p is a prime divisor of |G|\ud835\udc3a|G|| italic_G | such that (p\u22121,|G|)=1\ud835\udc5d1\ud835\udc3a1(p-1,|G|)=1( italic_p - 1 , | italic_G | ) = 1. Suppose that any cyclic subgroup of S\ud835\udc46Sitalic_S with order p\ud835\udc5dpitalic_p or 4444 (if p\ud835\udc5dpitalic_p=2) is a strong p\ud835\udc5dpitalic_p-CAP\ud835\udc36\ud835\udc34\ud835\udc43CAPitalic_C italic_A italic_P-subgroup of G\ud835\udc3aGitalic_G, then G\ud835\udc3aGitalic_G is p\ud835\udc5dpitalic_p-nilpotent. [Proof]It follows from Theorem 1.7 that \u2131S(G)subscript\u2131\ud835\udc46\ud835\udc3a\\mathcal{F}_{S}(G)caligraphic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_G ) is supersolvable. Thus we conclude from [12, Theorem 1.9] that G\ud835\udc3aGitalic_G is p\ud835\udc5dpitalic_p-nilpotent, as desired."
    },
    {
        "id": 15,
        "title": "No More Trade-Offs.\nGPT and Fully Informative Privacy Policies",
        "corpus": "The paper reports the results of an experiment aimed at testing to what extent ChatGPT 3.5 and 4 is able to answer questions regarding privacy policies designed in the new format that we propose. In a world of human-only interpreters, there was a trade-off between comprehensiveness and comprehensibility of privacy policies, leading to the actual policies not containing enough information for users to learn anything meaningful. Having shown that GPT performs relatively well with the new format, we provide experimental evidence supporting our policy suggestion, namely that the law should require fully comprehensive privacy policies, even if this means they become less concise. Imagine you just listened to a song or ordered food using an app. Imagine asking your smartphone: \u201cHey Skynet, could you tell me the names of all corporations who will know that I just listened to Italodisco, or ordered tofu, and list all the purposes for which they will use this data?\u201d111These are no (longer) trivial matters; in a ML-powered inference economy [33] such data can be proxy for estimating one\u2019s race [22] or political convictions [12]. As a consumer, you have the right to know this [7, 16, 21]. Such knowledge is a precondition for choosing whether and how to use various services [29]. And yet, in 2023, asking such a question and receiving a straightforward answer is not possible. The reason behind this impossibility is not (only) technological. Due to the spread of large language models (LLMs), machines are nowadays able to read and comprehend natural language at a level that was unthinkable only a few years ago [27]. Therefore, we posit that the reason is chiefly that the information necessary to answer such questions is not publicly available in privacy policies [26]. And that the law is partly to blame for that. The law222In this paper we focus on EU law, i.e. the GDPR, and account for the American law. requires privacy policies to be simultaneously comprehensive, i.e., to contain information about all the data categories processed, purposes of use, and their recipients; and comprehensible, i.e., to be concise, written in simple language, and easy to understand. In the world of human-only interpreters, there is a trade-off between the two: a shorter and simpler policy would be less comprehensive, and vice versa. The comprehensibility requirement has been advocated for in scholarship, calling for ever more simplification [7, 13, 18, 37]. In this paper, we challenge the conventional wisdom and take the opposite approach. We argue that the law should focus on requiring the privacy policies to be fully comprehensive, even if the cost of comprehensiveness is lowering comprehensibility of such documents for individual consumers. The reason is simple: soon anyone could engage in simplification of, or information retrieval from, long privacy policies, as such efforts can be increasingly automated. However, for these summaries or answers to be meaningful, they need to be based on comprehensive information about corporate practices concerning consumers\u2019 personal data. This policy suggestion assumes the capacity of AI to process fully comprehensive privacy policies. Hence, we decided to verify that premise. In this paper, we report some preliminary results of our first experiment, aimed at testing the extent to which tools like ChatGPT are already capable of answering consumer questions based on fully comprehensive privacy policies (and not the existing privacy policies, as tested in [15, 28, 34]). To do so, we have written a mock privacy policy of \u201cOrderoo Inc.,\u201d a company similar to DoorDash, UberEats, etc., in the format we propose as fully comprehensive. The format is presented below in Appendix C, and the full privacy policy, with all the experiment documentation, made available on GitHub.333 https://github.com/ruutaliepina/full-privacy We design our experiment as a proof-of-concept across four different settings, i.e., using short and long prompts, for ChatGPT 3.5 and 4. The results are interesting and promising, as they clearly show how LLMs are able to digest the information contained in long documents, and to answer complex questions that need to connect several paragraphs. Nevertheless, we note how the provided answers sometimes contain mistakes, which poses an additional challenge from the perspective of computer science, to what extent the output of these models can be further controlled and post-processed in order to make reliable information available to the end user. The paper proceeds in five steps. First, we discuss the related work, and second, the legal background. Third, we describe what a fully comprehensive privacy policy could look like. Fourth, we report the results of our preliminary experiments. Finally, we discuss implications for further research in computer science, law, and policy. Empirical scholars have shown that consumers do not read privacy policies [3, 24] and, when they do, they often misunderstand them [29]. One study estimated that reading the privacy policies of all the websites one uses would take hundreds of hours each year [23]. As a solution to this problem, numerous researchers have suggested making privacy policies even shorter or easier to understand, e.g., by using the so-called \u201cprivacy nutrition labels\u201d [7, 13, 18, 37]. However, it has been also shown that even simplified privacy policies have a close-to-zero effect on consumers\u2019 understanding of corporate data practices or their rights [6]. This might suggest that the problem with privacy policies is not (only) their form but also their content. Put simply: privacy policies often do not contain the information necessary for consumers \u2013 even the hypothetical ones with time and expertise to read them \u2013 to learn how exactly their data will be processed [10]. A recent study [26] highlighted how, by using vague terms for data categories (\u201cwe collect data about your use of our service\u201d) or purposes of processing (\u201cwe use your data to improve our service\u201d), many privacy policies leave corporations the freedom to engage in data practices that consumers cannot reasonably expect. The same study demonstrated that many privacy policies do not explain what data will be used for what purpose or shared with what recipient [26]. From a technical perspective, several studies have shown that natural language processing (NLP) tools can be used to summarize privacy policies [40], answer users\u2019 questions [28], or represent the contents of the policies in a user-friendly form [15]. However, the usefulness and the practical value of such automated representation, summarizing, and question-answering will be only as good and specific as the information actually included in the privacy policies. As of today, the value of this information is limited. The recent breakthroughs in LLMs have created new opportunities for analyzing texts through simple prompt-based queries. The LLMs use transformer architecture and have been trained on vast amounts of general data such as webpages, books, articles and cover a wide range of topics [36]. Legal language is highly specialised and normally general language models perform poorly without additional domain specific fine-tuning [8]. While these developments are fairly recent, some studies testing the abilities of LLMs in legal analytics have already emerged [31]. It has been shown that GPT models (3, 3.5, and 4) can answer the US [17] and Japanese [39] bar exam questions on par with the examinees. These tests include various reasoning tasks such as rule application, legal opinion drafting, and multiple choice questions. As for privacy policy analysis, Tang et al. [34] experimented with ChatGPT and GPT4 to improve information extraction from privacy policy relevant to the GDPR \u2013 e.g., \u201chow and why a service provider collects user information,\u201d \u201chow long user information is stored.\u201d The reported results of their PolicyGPT show a significant increase in retrieval metrics as compared to the older models, which used expert annotated datasets. The law assigns the drafters of privacy policies a difficult task. On the one hand, it requires the privacy policies to be comprehensive, i.e., contain all the information relevant to a consumer pondering whether to use a service. On the other, it demands privacy policies be comprehensible, i.e., written in simple language and easy to understand for consumers. In the United States, online consumer privacy is governed by the so-called \u201cnotice and choice\u201d model [30]. Within its logic, companies who wish to collect and use personal data should make privacy policies available to consumers (\u201cnotice\u201d) who, in turn, can decide whether such data practices are acceptable to them (\u201cchoice\u201d). Whether companies actually live up to their promises is policed by both contract law [5] and public enforcement by the Federal Trade Commission [32] and state Attorneys General [9]. The latter can also issue guidelines. For example, in 2014, then-AG of California, Kamala Harris [1], stated that privacy policies \u201cshould provide a comprehensive overview of [\u2026] practices regarding the collection, use, sharing and protection of personally identifiable information\u201d and recommended that drafters of privacy policies \u201cuse plain, straightforward language. Avoid technical or legal jargon. Use short sentences.\u201d444 [1], pp 9-10. In the European Union, the omnibus General Data Protection Regulation555Regulation (EU) 2016/679, OJ L 119, 4.5.2016, p. 1\u201388, hereinafter \u201cthe GDPR.\u201d governs online consumer privacy, among other areas of life [38]. It requires that data controllers provide consumers with detailed information about their data practices,666The GDPR, arts. 13-14. yet do so in \u201cin a concise, transparent, intelligible and easily accessible form, using clear and plain language.\u201d777The GDPR, art. 12.1. The guidelines endorsed by the European Data Protection Board888Art. 29 WP, Guidelines on transparency under Regulation 2016/679, last Revised and Adopted on 11 April 2018, available at: https://edpb.europa.eu/our-work-tools/our-documents/guidelines/transparency. Hereinafter \u201cThe Transparency Guidelines.\u201d \u2013 a body comprised of the Supervisory Authorities from all the EU Member States \u2013 provide some examples of how to square the two: Poor Practice Examples \u201cWe may use your personal data to develop new services\u201d (as it is unclear what the \u201cservices\u201d are or how the data will help develop them); \u201cWe may use your personal data for research purposes\u201d (as it is unclear what kind of \u201c\u2018research\u201d this refers to); and \u201cWe may use your personal data to offer personalised services\u201d (as it is unclear what the \u201cpersonalisation\u201d entails). Good Practice Examples \u201cWe will retain your shopping history and use details of the products you have previously purchased to make suggestions to you for other products which we believe you will also be interested in\u201d (it is clear what types of data will be processed, that the data subject will be subject to targeted advertisements for products, that their data will be used to enable this).999The Transparency Guidelines, p. 9. This looks clear and simple when presented as an isolated example. However, when one considers the sheer volume of data processed by online companies nowadays, the multitude of purposes for which different categories of data are used, or the range of entities with whom data is shared, the comprehensive privacy policy quickly becomes inflated. In the world of human-only readers, there is an inherent trade-off between comprehensiveness and comprehensibility. Admittedly, corporations have business interests not to disclose too much, like trade secrets or good PR. However, by requiring that privacy policies were simultaneously comprehensive and comprehensible, the law has given the corporations an excuse. They can always defend themselves by stating \u201cbut we simply tried to make the privacy policy shorter!\u201d Fortunately, human readers could soon be significantly assisted by AI. Our policy suggestion aims to create an informational environment where AI\u2019s full potential in assisting consumers can be realized. We propose that the law should require corporations to disclose fully comprehensive privacy policies, ideally in a standardized form, and relax the comprehensibility requirements. Such notices can be later processed by other parties to suit particular consumers\u2019 informational needs, potentially via automated means. Under the GDPR\u2019s transparency requirements (which we consider a good start) the unit of information that must be disclosed is an act of processing of personal data. An act of processing means any action that a data controller takes regarding personal data.101010GDPR art. 4.1. Thus, transparency at the stage in which personal data are collected requires the consumers to be informed, among others, about the following: Categories of personal data concerned (e.g., an email address); Purpose of processing for which each category of data are intended (e.g., issuing receipts) Legal basis for each data processing111111Art. 6.1. GDPR lists six possible bases for processing: (a) consent, (b) contractual necessity, (c) legal obligation, (d) subject\u2019s vital interest, (e) public task, or (f) controller\u2019s legitimate interest. In the analyzed context, (a), (b), (c) and (f) are relevant. Whenever a data controller relies on (f), they should specify what legitimate interest they have in mind, to enable the proportionality assessment(Article 6(1)(f)). (e.g., contractual necessity) Storage period or, when it is not possible, the criteria to determine such a period (e.g., till account closure), and with regard to each data and purpose Recipients or at least the categories of recipients ( e.g., a cloud provider), as well as their Role (controller or processor) Purpose of sharing (e.g., data backup) Legal basis of sharing (e.g., legitimate interest) In our empirical work on privacy policies [10, 11, 20, 26] we have not yet encountered a single policy that is compliant with all these requirements. This may be due to different reasons, one of them probably being that such a document would not meet the concise and comprehensibility requirements. Note how a single category of data (e.g., an email address) might be used for several purposes, e.g., sending receipts, marketing information, serve as an identifier, etc. Each of these purposes must be described as a separate unit of information, as they might have different legal bases. Such information can be represented in several different ways. On the one had, there are various machine-readable formats [19, 35] (though scholarly attempts at constructing those seem to have culminated about a decade ago), potentially based on ontologies [14, 25]. On the other hand, there are human-readable formats, i.e., texts or tables. However, the advances in NLP [41] contribute to the blurring of the machine-/human-readable distinction. In this paper, we propose two formats for fully comprehensive privacy policies. First, we suggest creating a spreadsheet with 8 columns: (A) category identifier, (B) data type, (C) source, (D) purpose, (E) purpose explanation, (F) legal basis, (G) legal basis explanation, and (H) storage period. The number of rows should equal the number of purposes of processing for each data type, as these might have different legal bases and be stored for different periods. In addition, a second sheet describes the sharing of data with other parties, in a similar form. We illustrate how this could look like in Appendix B. The value of this approach is in its clarity and logical structure of all the data points without the added noise. Second, exactly the same information can be represented as solid text. Here, each paragraph corresponds to one data type and lists the source and all the purposes with their legal bases. Then, it lists all the recipients of personal data and identifies their role (controller or processor), the purpose of sharing, and its legal basis. We illustrate how this could look like in Appendix C. We conduct the experiment on the solid text version. We run an experiment as a proof-of-concept of our methodology. Our goal is to test to what extent the consumer-facing versions of ChatGPT 3.5 and 4 may correctly answer user questions regarding the contents of fully comprehensive privacy policies. To do so, we created a mock privacy policy of Orderoo Inc., a food delivery company similar to DoorDash and Uber Eats.121212Note that the mock policy is in the fully comprehensive format but could be longer if we accounted for more categories of data. We identified the following six questions to be used for the test: What data does Orderoo process about me? For what purposes does Orderoo use my email address? Who does Orderoo share my geolocation with? What types of data are processed on the basis of consent, and for what purposes? What data does Orderoo share with Facebook? Does Orderoo share my data with insurers? The three initial questions are rather straightforward and aimed to test the model\u2019s ability to retrieve the relevant information and connect different informative elements. The answer to Q1 can be found at the beginning of each paragraph of the policy, while the answers to Q2 and Q3 are contained in a single paragraph. The last three questions are more complicated. The answers to Q4 and Q5 are spread throughout the entire document. Q6 requires a negative answer, i.e., the policy does not mention any data sharing with insurers. The latter is mostly aimed to test potential hallucinations. As noted above, for the purpose of this work, we tested ChatGPT versions 3.5 and 4, on Sep 30-Oct 2, 2023, while located in Bologna, Italy. We defined two prompts, one short and one long, the latter aimed at mitigating hallucinations (i.e., making answers up) [2, 4], unwarranted simplification (i.e., listing only some answers, preceded by phrases like \u201cfor example\u201d or \u201csuch as\u201d), and unwarranted text generation (i.e., adding information that the user did not ask for, e.g., advice that one should carefully read the laws, etc.). As an example consider the following: Short prompt: What data does Orderoo process about me? Long prompt: What data does Orderoo process about me? In answering the question please rely solely on the information included in the text and not your knowledge from other sources; please read the document carefully and mention everything, do not omit any information included in the text; please do not shorten or simplify the answers by inserting elements like \u201cfor example\u201d, \u201cincluding\u201d or \u201csuch as\u201d, please limit your answer strictly to what I am asking about and refrain from giving me advice or informing me about things I have not asked about. Hence, we considered four settings: (1) GPT 3.5, short prompt; (2) GPT 3.5, long prompt; (3) GPT 4 short prompt; (4) GPT 4 long prompt. For each setting, we repeated the test five times with two independent accounts, for a total of ten runs. Each chat began with the same statement, i.e., Hi, I will copy-paste a document here and then ask you some questions about its contents, is that ok?, followed by the entire mock privacy policy of Orderoo Inc. For the purpose of evaluation, we consider the following four cases: (i) correct answer; (ii) hallucination; (iii) false positive (when the answer mentioned information that clearly was taken from the document, though wrong); and (iv) false negative (when the answer failed to mention something). We also decided, whenever the answer was not correct, to prompt GPT again with a generic \u201credo it\u201d question (i.e. Are you sure? Please try again), thus recording whether this led to improved performance or not. All the answers were manually checked, and the whole set of conversations is available at the aforementioned repository. Our initial hypotheses were the following: There will be a problem with hallucination, especially when asking about Facebook (as GPT \u201cknows\u201d a lot about it from other sources) There should be no problem with false positives, and false negatives should occur sporadically, due to unwarranted simplification GPT-4 will do better than GPT-3.5 on all the questions Longer prompts will lead to better answers than shorter ones Table 1 reports the results obtained across the different settings. Notably, in none of the settings did GPT manage to answer all six questions correctly. However, several interesting observations can be made based on these results. Significantly, results were not always consistent across the runs. For Q3 and Q4, in fact, one of the two accounts obtained different answers when questioning GPT-4 both with short and long prompts. Regarding H1, none of the experiments reported hallucination. This suggests that, when asked questions about a document uploaded by the user, the hallucination problem is less pertinent than with open-ended questions that rely on GPT\u2019s \u201cown knowledge.\u201d Hence, H1 has been falsified. Concerning H2 and H3, the only instances of false positives occurred in Q3, with GPT-4 (though only for one account). The system listed one additional entity with whom Orderoo allegedly shares geolocation, namely \u201cCloud711\u201d. This is the name of a company made up by us, mentioned repeatedly in the document, so this was not a hallucination. Moreover, every data type other than geolocalization is shared by Orderoo with Cloud711, according to the policy. Thus, in this case, the system just wrongly interpreted the text. All other runs got this correct. False negatives, instead, consistently appeared in Q3 and Q4 for GPT-3.5 (all 10 runs, both short and long prompts) and also, though less frequently, for GPT-4 (6 and 8 cases for short and long prompts in Q4, respectively; only once in Q3). Therefore, given all this evidence, also H2 and H3 have been partly falsified. Remarkably, all runs correctly answered Q1 and Q2 and Q6 (where a negative answer was correct). This suggests that having even longer privacy policies, where additional sections are organized not only around data categories but also around purposes and legal bases, could help. However, this would mean that the documents will be significantly longer (more processing power needed) and come with the additional need to ensure consistency throughout the document. Surprisingly, the longer prompts had no effect on GPT 3.5. For GPT 4, instead, the results were interesting: the additional instruction led to more false negatives. In this sense, H4 has been falsified. The good performance of GPT-4 when answering questions about the fully comprehensive privacy policy (if we consider the majority of cases, 5 correct answers out of 6 questions) suggests that our policy proposal should be seriously considered by lawmakers or even privacy-sensitive corporations acting on their own motion. This proposal challenges the conventional wisdom that privacy policies should be made shorter rather than longer and simpler rather than more complex [7, 13, 18, 37]. However, for this reform to indeed meaningfully affect the position of consumers, further research in computer science is needed. Some limitations with the use of pretrained systems should be noted. Currently, ChatGPT free version limits the input text to approximately 3000 words. When arguing for more comprehensive privacy policies, the word limit could create problems. For example, during the experiments, we also tested the performance of the system by entering the privacy policy splitting it over two messages. The results were significantly worse compared to the single input \u2013 the answers were based on general information about privacy policies, not the specific mock policy we provided. Moreover, GPT architecture remains opaque and users are given limited control over the results they receive. In this paper, we suggested that making privacy policies fully comprehensive is a pro-consumer move, provided that automated means for question answering are available. We created a mock privacy policy, demonstrating what format we have in mind, and tested the performance of Chat GPT 3.5 and 4, using both short and long prompts. The novelty of this experiment lays in the creation and use of a synthetic privacy policy to test the abilities of the new LLMs. Even though the results where not perfect, the good performance of GPT 4 suggests that this is a promising path. As part of experimental work, we plan to expand the mock privacy policy and the number of documents to be assessed, test different LLMs, including open source LLMs; explore the use of API to create further controls for the quality of the answers, as well as, link the answers to the relevant portions of the text. We hereby report the complete list of the results obtained with the 10 different runs (5 for each of two accounts) across the four considered settings. Figure 1 presents a sample of the mock privacy policy. It includes two data types (i.e., email address, name and surname) their specified sources, purposes for processing, explanations of these purposes, legal basis and further explanations, and information about storage. THIS IS A MOCK POLICY OF \u201cORDEROO INC.\u201d A COMPANY SIMILAR TO DELIVEROO/ DOORDASH /JUST EAT/ UBER EATS etc. ORDEROO INC. PRIVACY POLICY We process your personal data in the following way: 1. Your email address. You provide us with your email address when registering for the service. We use your email address for the following purposes: unique identifier, it serves as a unique identifier allowing you to set up and log in to your account (contractual necessity); account access, to let you reset your password if you forget it (contractual necessity); transaction-related-communication, to send you receipts of your orders (legal obligation: to issue receipts, according to the Receipts Act); distribution of own advertising, to send you advertisements of our own services, new functionalities or new order options (legitimate interest: informing the consumers about the available offers and features, and promoting them); distribution of third-party marketing, to send you advertisements of vendors selling their products on our site (legitimate interest: to subsidize the price of the service with payments from the vendors we promote); tracking transaction history, we keep it as a part of your order history data in case it becomes necessary to reveal it to investigative authorities (legal obligation: Accounting Act and Code of Criminal Procedure); we use the domain name part of your email when profiling (see the separate section at the bottom of the document). We share your email address with Cloud711 (processor), for the purpose of data storage and backup, i.e., storing our IT operations on their servers (legitimate interest: lowering the cost of operation and keeping the data safe); Microsoft (processor) for the purpose of facilitating communication, i.e., sending our own emails (legitimate interest in outsourcing the operation of email servers and protocols); CoolAccountants (processor) for the purpose of accounting, i.e., reviewing our financial records and keeping them in order (legal obligation: Accounting Act); FraudDetectors (processor) for the purpose of fraud detection (legitimate interest: not becoming a victim of fraud). We do not share your email address with recipients choosing their own purposes of processing (controllers). We store your email for as long as you\u2019re using our services, i.e., until you delete your account, PLUS THREE MONTHS. For the purposes required by the Receipts Act, Accounting Act, and the Code of Criminal Procedure, we store your email address for five years after your last transaction."
    },
    {
        "id": 16,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6947-MXP 1732891273 2049273887 Varnish cache server"
    },
    {
        "id": 17,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6936-MXP 1732891276 3131796270 Varnish cache server"
    },
    {
        "id": 18,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6945-MXP 1732891278 303113787 Varnish cache server"
    },
    {
        "id": 19,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6961-MXP 1732891279 601538818 Varnish cache server"
    },
    {
        "id": 20,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6964-MXP 1732891281 3019863040 Varnish cache server"
    },
    {
        "id": 21,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6929-MXP 1732891283 1436757801 Varnish cache server"
    },
    {
        "id": 22,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6955-MXP 1732891285 3948698235 Varnish cache server"
    },
    {
        "id": 23,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6942-MXP 1732891287 2514391911 Varnish cache server"
    },
    {
        "id": 24,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6962-MXP 1732891289 2621907916 Varnish cache server"
    },
    {
        "id": 25,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6936-MXP 1732891291 3131872671 Varnish cache server"
    },
    {
        "id": 26,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6979-MXP 1732891292 3521721524 Varnish cache server"
    },
    {
        "id": 27,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6927-MXP 1732891294 1039036101 Varnish cache server"
    },
    {
        "id": 28,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6971-MXP 1732891297 1578712412 Varnish cache server"
    },
    {
        "id": 29,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6982-MXP 1732891298 2012373100 Varnish cache server"
    },
    {
        "id": 30,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6942-MXP 1732891300 2514455570 Varnish cache server"
    },
    {
        "id": 31,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6976-MXP 1732891302 2403517230 Varnish cache server"
    },
    {
        "id": 32,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6928-MXP 1732891303 3086828704 Varnish cache server"
    },
    {
        "id": 33,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6979-MXP 1732891305 3521787349 Varnish cache server"
    },
    {
        "id": 34,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6937-MXP 1732891307 2970045765 Varnish cache server"
    },
    {
        "id": 35,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6982-MXP 1732891309 2012425973 Varnish cache server"
    },
    {
        "id": 36,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6972-MXP 1732891311 3495546333 Varnish cache server"
    },
    {
        "id": 37,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6921-MXP 1732891314 487397893 Varnish cache server"
    },
    {
        "id": 38,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6964-MXP 1732891316 3020035032 Varnish cache server"
    },
    {
        "id": 39,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6932-MXP 1732891318 3596549782 Varnish cache server"
    },
    {
        "id": 40,
        "title": "Error 403 Forbidden",
        "corpus": "Forbidden Details: cache-mxp6974-MXP 1732891319 3791691817 Varnish cache server"
    }
]